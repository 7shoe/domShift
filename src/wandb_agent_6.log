nohup: ignoring input
wandb: Starting wandb agent üïµÔ∏è
2025-03-26 17:04:47,910 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 17:04:48,391 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:04:48,392 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 17:04:48,395 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.7000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:04:53,415 - wandb.wandb_agent - INFO - Running runs: ['1hbobatz']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170453-1hbobatz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1hbobatz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1hbobatz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4537 | Val Loss: 1.3229
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2301 | Val Loss: 1.0867
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0656 | Val Loss: 1.0146
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9984 | Val Loss: 0.9818
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9421 | Val Loss: 0.9082
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8605 | Val Loss: 0.8275
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8151 | Val Loss: 0.8207
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8192 | Val Loss: 0.8350
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8302 | Val Loss: 0.8318
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.82913
wandb:   val_loss 0.82863
wandb: 
wandb: üöÄ View run colorful-sweep-6 at: https://wandb.ai/7shoe/domShift-extensive/runs/1hbobatz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170453-1hbobatz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8291 | Val Loss: 0.8286
2025-03-26 17:05:23,896 - wandb.wandb_agent - INFO - Cleaning up finished run: 1hbobatz
2025-03-26 17:05:24,319 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:24,319 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:05:24,322 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimCLR --model_class=cnn --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.9
2025-03-26 17:05:29,335 - wandb.wandb_agent - INFO - Running runs: ['37xnxbe3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170530-37xnxbe3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/37xnxbe3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 37xnxbe3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 6.0098 | Val Loss: 5.7478
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 5.6763 | Val Loss: 5.4684
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 5.5579 | Val Loss: 5.3083
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 5.5032 | Val Loss: 5.2644
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 5.4570 | Val Loss: 5.2160
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 5.4169 | Val Loss: 5.2002
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 5.4019 | Val Loss: 5.1726
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 5.3888 | Val Loss: 5.1589
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 5.3744 | Val Loss: 5.1479
wandb: - 34.320 MB of 34.320 MB uploadedwandb: \ 34.320 MB of 34.339 MB uploadedwandb: | 34.320 MB of 34.339 MB uploadedwandb: / 34.339 MB of 34.339 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.349
wandb:   val_loss 5.13399
wandb: 
wandb: üöÄ View run woven-sweep-11 at: https://wandb.ai/7shoe/domShift-extensive/runs/37xnxbe3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170530-37xnxbe3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 5.3490 | Val Loss: 5.1340
2025-03-26 17:05:54,701 - wandb.wandb_agent - INFO - Cleaning up finished run: 37xnxbe3
2025-03-26 17:05:55,104 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:55,104 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:05:55,107 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:06:00,118 - wandb.wandb_agent - INFO - Running runs: ['yosd9wms']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170601-yosd9wms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yosd9wms
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: yosd9wms
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5956 | Val Loss: 1.4170
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3857 | Val Loss: 1.3440
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3018 | Val Loss: 1.2536
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2525 | Val Loss: 1.2228
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1387 | Val Loss: 1.0288
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9788 | Val Loss: 0.9559
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9705 | Val Loss: 0.9860
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9935 | Val Loss: 0.9944
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9923 | Val Loss: 0.9887
wandb: - 32.875 MB of 32.875 MB uploadedwandb: \ 32.875 MB of 32.875 MB uploadedwandb: | 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.98751
wandb:   val_loss 0.98612
wandb: 
wandb: üöÄ View run glamorous-sweep-15 at: https://wandb.ai/7shoe/domShift-extensive/runs/yosd9wms
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170601-yosd9wms/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9875 | Val Loss: 0.9861
2025-03-26 17:06:35,626 - wandb.wandb_agent - INFO - Cleaning up finished run: yosd9wms
2025-03-26 17:06:36,139 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:06:36,139 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:06:36,143 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.4
2025-03-26 17:06:41,156 - wandb.wandb_agent - INFO - Running runs: ['3d4pm86i']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170642-3d4pm86i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3d4pm86i
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3d4pm86i
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2899 | Val Loss: 3.0107
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.0224 | Val Loss: 3.0335
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.0415 | Val Loss: 3.0244
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.9867 | Val Loss: 2.9171
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8742 | Val Loss: 2.8306
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.8166 | Val Loss: 2.7907
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.7816 | Val Loss: 2.7640
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.7586 | Val Loss: 2.7463
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.7426 | Val Loss: 2.7329
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.987 MB uploadedwandb: | 32.968 MB of 32.987 MB uploadedwandb: / 32.987 MB of 32.987 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.73055
wandb:   val_loss 2.72318
wandb: 
wandb: üöÄ View run spring-sweep-22 at: https://wandb.ai/7shoe/domShift-extensive/runs/3d4pm86i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170642-3d4pm86i/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.7305 | Val Loss: 2.7232
2025-03-26 17:07:11,561 - wandb.wandb_agent - INFO - Cleaning up finished run: 3d4pm86i
2025-03-26 17:07:12,039 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:12,039 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:07:12,042 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:07:17,054 - wandb.wandb_agent - INFO - Running runs: ['btdcf11b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170718-btdcf11b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/btdcf11b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: btdcf11b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 2.6145 | Val Loss: 2.2311
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.2325 | Val Loss: 2.2794
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.3373 | Val Loss: 2.3672
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.3673 | Val Loss: 2.3537
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 2.3377 | Val Loss: 2.2997
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 2.2687 | Val Loss: 2.2461
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 2.2358 | Val Loss: 2.2262
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.2208 | Val Loss: 2.2151
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.2097 | Val Loss: 2.2030
wandb: - 32.964 MB of 32.964 MB uploadedwandb: \ 32.964 MB of 32.964 MB uploadedwandb: | 32.984 MB of 32.984 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.19834
wandb:   val_loss 2.19492
wandb: 
wandb: üöÄ View run cosmic-sweep-27 at: https://wandb.ai/7shoe/domShift-extensive/runs/btdcf11b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170718-btdcf11b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.1983 | Val Loss: 2.1949
2025-03-26 17:07:57,637 - wandb.wandb_agent - INFO - Cleaning up finished run: btdcf11b
2025-03-26 17:07:58,164 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:58,164 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:07:58,166 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:08:03,178 - wandb.wandb_agent - INFO - Running runs: ['52h7rfy8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170804-52h7rfy8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/52h7rfy8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 52h7rfy8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5175 | Val Loss: 1.1956
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1447 | Val Loss: 1.1238
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0661 | Val Loss: 1.0133
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0019 | Val Loss: 0.9944
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0152 | Val Loss: 1.0424
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0643 | Val Loss: 1.0837
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0925 | Val Loss: 1.1036
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1118 | Val Loss: 1.1158
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1075 | Val Loss: 1.0951
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08661
wandb:   val_loss 1.08016
wandb: 
wandb: üöÄ View run fine-sweep-34 at: https://wandb.ai/7shoe/domShift-extensive/runs/52h7rfy8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170804-52h7rfy8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0866 | Val Loss: 1.0802
2025-03-26 17:08:43,751 - wandb.wandb_agent - INFO - Cleaning up finished run: 52h7rfy8
2025-03-26 17:08:44,243 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:08:44,243 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:08:44,246 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:08:49,257 - wandb.wandb_agent - INFO - Running runs: ['ub59zkf7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170850-ub59zkf7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ub59zkf7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ub59zkf7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1788 | Val Loss: 2.9402
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6873 | Val Loss: 2.4679
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4529 | Val Loss: 2.4462
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4500 | Val Loss: 2.4512
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4616 | Val Loss: 2.4763
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4988 | Val Loss: 2.5203
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5379 | Val Loss: 2.5479
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5533 | Val Loss: 2.5546
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5530 | Val Loss: 2.5487
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.5434
wandb:   val_loss 2.53642
wandb: 
wandb: üöÄ View run gentle-sweep-39 at: https://wandb.ai/7shoe/domShift-extensive/runs/ub59zkf7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170850-ub59zkf7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5434 | Val Loss: 2.5364
2025-03-26 17:09:40,011 - wandb.wandb_agent - INFO - Cleaning up finished run: ub59zkf7
2025-03-26 17:09:41,291 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:41,291 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:09:41,293 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:09:46,305 - wandb.wandb_agent - INFO - Running runs: ['ex95o4lh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170947-ex95o4lh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ex95o4lh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ex95o4lh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7383 | Val Loss: 1.5510
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3811 | Val Loss: 1.4761
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4681 | Val Loss: 1.4546
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4334 | Val Loss: 1.4101
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4116 | Val Loss: 1.4193
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4178 | Val Loss: 1.4081
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3979 | Val Loss: 1.3925
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3952 | Val Loss: 1.3949
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3898 | Val Loss: 1.3843
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.38427
wandb:   val_loss 1.38435
wandb: 
wandb: üöÄ View run resilient-sweep-45 at: https://wandb.ai/7shoe/domShift-extensive/runs/ex95o4lh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170947-ex95o4lh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3843 | Val Loss: 1.3843
2025-03-26 17:10:32,217 - wandb.wandb_agent - INFO - Cleaning up finished run: ex95o4lh
2025-03-26 17:10:52,733 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:10:52,733 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:10:52,736 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:10:57,747 - wandb.wandb_agent - INFO - Running runs: ['nk6skt56']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171057-nk6skt56
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nk6skt56
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: nk6skt56
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4463 | Val Loss: 1.3999
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3512 | Val Loss: 1.3588
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3701 | Val Loss: 1.3786
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3721 | Val Loss: 1.3764
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3706 | Val Loss: 1.3738
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3684 | Val Loss: 1.3714
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3665 | Val Loss: 1.3691
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3660 | Val Loss: 1.3700
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3674 | Val Loss: 1.3712
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36885
wandb:   val_loss 1.37263
wandb: 
wandb: üöÄ View run glorious-sweep-54 at: https://wandb.ai/7shoe/domShift-extensive/runs/nk6skt56
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171057-nk6skt56/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3688 | Val Loss: 1.3726
2025-03-26 17:11:38,285 - wandb.wandb_agent - INFO - Cleaning up finished run: nk6skt56
2025-03-26 17:11:39,030 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:11:39,030 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:11:39,033 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:11:44,040 - wandb.wandb_agent - INFO - Running runs: ['6s2bbkaq']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171144-6s2bbkaq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6s2bbkaq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6s2bbkaq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.5468 | Val Loss: 2.9151
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7964 | Val Loss: 2.6916
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6445 | Val Loss: 2.5518
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5425 | Val Loss: 2.4563
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4680 | Val Loss: 2.4047
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3982 | Val Loss: 2.3635
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3542 | Val Loss: 2.3189
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3129 | Val Loss: 2.2666
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2886 | Val Loss: 2.2493
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.26998
wandb:   val_loss 2.23871
wandb: 
wandb: üöÄ View run golden-sweep-61 at: https://wandb.ai/7shoe/domShift-extensive/runs/6s2bbkaq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171144-6s2bbkaq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2700 | Val Loss: 2.2387
2025-03-26 17:12:19,533 - wandb.wandb_agent - INFO - Cleaning up finished run: 6s2bbkaq
2025-03-26 17:12:20,096 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:20,096 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:12:20,099 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:12:25,111 - wandb.wandb_agent - INFO - Running runs: ['y5bb9dlj']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171224-y5bb9dlj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-66
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y5bb9dlj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: y5bb9dlj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.0511 | Val Loss: 3.7543
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.5645 | Val Loss: 3.2950
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.1164 | Val Loss: 2.8896
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7858 | Val Loss: 2.6738
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.6123 | Val Loss: 2.5370
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.5038 | Val Loss: 2.4663
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.4567 | Val Loss: 2.4405
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.4411 | Val Loss: 2.4360
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.4413 | Val Loss: 2.4416
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 32.988 MB of 32.988 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.44834
wandb:   val_loss 2.4502
wandb: 
wandb: üöÄ View run valiant-sweep-66 at: https://wandb.ai/7shoe/domShift-extensive/runs/y5bb9dlj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171224-y5bb9dlj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.4483 | Val Loss: 2.4502
2025-03-26 17:12:55,524 - wandb.wandb_agent - INFO - Cleaning up finished run: y5bb9dlj
2025-03-26 17:12:56,121 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:56,121 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:12:56,124 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:13:01,136 - wandb.wandb_agent - INFO - Running runs: ['cmkctmju']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171302-cmkctmju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-71
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cmkctmju
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cmkctmju
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4327 | Val Loss: 1.1951
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1582 | Val Loss: 1.1190
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1079 | Val Loss: 1.0758
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0424 | Val Loss: 1.0435
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0768 | Val Loss: 1.1381
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1781 | Val Loss: 1.2079
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2252 | Val Loss: 1.2188
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1811 | Val Loss: 1.1522
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1605 | Val Loss: 1.1558
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13595
wandb:   val_loss 1.10727
wandb: 
wandb: üöÄ View run bumbling-sweep-71 at: https://wandb.ai/7shoe/domShift-extensive/runs/cmkctmju
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171302-cmkctmju/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1359 | Val Loss: 1.1073
2025-03-26 17:13:41,702 - wandb.wandb_agent - INFO - Cleaning up finished run: cmkctmju
2025-03-26 17:13:42,376 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:42,376 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:13:42,378 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:13:47,390 - wandb.wandb_agent - INFO - Running runs: ['5h243g92']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171347-5h243g92
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5h243g92
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 5h243g92
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.9413 | Val Loss: 3.6327
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.5855 | Val Loss: 3.4982
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.4535 | Val Loss: 3.3756
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.3292 | Val Loss: 3.2440
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.2035 | Val Loss: 3.1423
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.1053 | Val Loss: 3.0330
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.9714 | Val Loss: 2.8791
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.8390 | Val Loss: 2.7924
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.7774 | Val Loss: 2.7528
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.74047
wandb:   val_loss 2.71951
wandb: 
wandb: üöÄ View run different-sweep-78 at: https://wandb.ai/7shoe/domShift-extensive/runs/5h243g92
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171347-5h243g92/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7405 | Val Loss: 2.7195
2025-03-26 17:14:22,865 - wandb.wandb_agent - INFO - Cleaning up finished run: 5h243g92
2025-03-26 17:14:23,467 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:14:23,467 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:14:23,470 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:14:28,481 - wandb.wandb_agent - INFO - Running runs: ['5744mcni']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171429-5744mcni
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5744mcni
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 5744mcni
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.1076 | Val Loss: 2.0652
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0846 | Val Loss: 2.1605
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2658 | Val Loss: 2.3090
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2334 | Val Loss: 2.2679
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2625 | Val Loss: 2.2492
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2250 | Val Loss: 2.2022
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2056 | Val Loss: 2.2352
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2833 | Val Loss: 2.3557
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4407 | Val Loss: 2.5228
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.425 MB uploadedwandb: | 137.266 MB of 137.425 MB uploadedwandb: / 137.425 MB of 137.425 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.53639
wandb:   val_loss 2.54467
wandb: 
wandb: üöÄ View run colorful-sweep-81 at: https://wandb.ai/7shoe/domShift-extensive/runs/5744mcni
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171429-5744mcni/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5364 | Val Loss: 2.5447
2025-03-26 17:15:49,629 - wandb.wandb_agent - INFO - Cleaning up finished run: 5744mcni
2025-03-26 17:15:50,231 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:15:50,231 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:15:50,234 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:15:55,246 - wandb.wandb_agent - INFO - Running runs: ['irb8t5ew']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171555-irb8t5ew
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/irb8t5ew
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: irb8t5ew
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7036 | Val Loss: 1.4577
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4134 | Val Loss: 1.4764
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5423 | Val Loss: 1.5651
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5326 | Val Loss: 1.4764
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4603 | Val Loss: 1.4522
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4150 | Val Loss: 1.3489
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3096 | Val Loss: 1.2911
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2820 | Val Loss: 1.2714
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2542 | Val Loss: 1.2356
wandb: - 10.265 MB of 10.265 MB uploadedwandb: \ 10.265 MB of 10.265 MB uploadedwandb: | 10.285 MB of 10.285 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21156
wandb:   val_loss 1.18191
wandb: 
wandb: üöÄ View run volcanic-sweep-93 at: https://wandb.ai/7shoe/domShift-extensive/runs/irb8t5ew
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171555-irb8t5ew/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2116 | Val Loss: 1.1819
2025-03-26 17:16:25,669 - wandb.wandb_agent - INFO - Cleaning up finished run: irb8t5ew
2025-03-26 17:16:26,196 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:26,196 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:16:26,199 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:16:31,210 - wandb.wandb_agent - INFO - Running runs: ['60m7z89q']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171631-60m7z89q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/60m7z89q
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 60m7z89q
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.4920 | Val Loss: 3.2967
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.2416 | Val Loss: 3.2127
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.1992 | Val Loss: 3.2020
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.1729 | Val Loss: 3.0166
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8387 | Val Loss: 2.6750
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5836 | Val Loss: 2.5014
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4427 | Val Loss: 2.3940
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3900 | Val Loss: 2.3816
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3636 | Val Loss: 2.3425
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.3288
wandb:   val_loss 2.31067
wandb: 
wandb: üöÄ View run earnest-sweep-98 at: https://wandb.ai/7shoe/domShift-extensive/runs/60m7z89q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171631-60m7z89q/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3288 | Val Loss: 2.3107
2025-03-26 17:17:11,771 - wandb.wandb_agent - INFO - Cleaning up finished run: 60m7z89q
2025-03-26 17:17:12,319 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:17:12,319 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:17:12,322 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:17:17,334 - wandb.wandb_agent - INFO - Running runs: ['p3q0rfkm']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171717-p3q0rfkm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-104
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/p3q0rfkm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: p3q0rfkm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.2239 | Val Loss: 1.8623
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.9599 | Val Loss: 2.0290
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0822 | Val Loss: 2.1322
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1681 | Val Loss: 2.2006
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2237 | Val Loss: 2.2446
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2600 | Val Loss: 2.2744
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2852 | Val Loss: 2.2954
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3032 | Val Loss: 2.3107
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3164 | Val Loss: 2.3218
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.195 MB of 137.195 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.32618
wandb:   val_loss 2.33052
wandb: 
wandb: üöÄ View run misunderstood-sweep-104 at: https://wandb.ai/7shoe/domShift-extensive/runs/p3q0rfkm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171717-p3q0rfkm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3262 | Val Loss: 2.3305
2025-03-26 17:18:33,387 - wandb.wandb_agent - INFO - Cleaning up finished run: p3q0rfkm
2025-03-26 17:18:33,959 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:18:33,959 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:18:33,962 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:18:38,974 - wandb.wandb_agent - INFO - Running runs: ['ip39x7w1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171840-ip39x7w1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-113
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ip39x7w1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ip39x7w1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6211 | Val Loss: 1.4113
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3332 | Val Loss: 1.2395
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2011 | Val Loss: 1.1582
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1474 | Val Loss: 1.1315
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1284 | Val Loss: 1.1211
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1214 | Val Loss: 1.1174
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1188 | Val Loss: 1.1167
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1199 | Val Loss: 1.1214
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1262 | Val Loss: 1.1288
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.873 MB uploadedwandb: | 32.853 MB of 32.873 MB uploadedwandb: / 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13392
wandb:   val_loss 1.13776
wandb: 
wandb: üöÄ View run sleek-sweep-113 at: https://wandb.ai/7shoe/domShift-extensive/runs/ip39x7w1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171840-ip39x7w1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1339 | Val Loss: 1.1378
2025-03-26 17:19:09,393 - wandb.wandb_agent - INFO - Cleaning up finished run: ip39x7w1
2025-03-26 17:19:10,055 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:19:10,056 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:19:10,058 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:19:15,070 - wandb.wandb_agent - INFO - Running runs: ['slctcgp7']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171915-slctcgp7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-117
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/slctcgp7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: slctcgp7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.5693 | Val Loss: 3.2339
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.2427 | Val Loss: 3.1921
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.1215 | Val Loss: 3.0284
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.9908 | Val Loss: 2.9462
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.9803 | Val Loss: 3.0324
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.1026 | Val Loss: 3.1858
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.2435 | Val Loss: 3.2975
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.3223 | Val Loss: 3.3369
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.3251 | Val Loss: 3.2764
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ
wandb:   val_loss ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.192
wandb:   val_loss 3.03119
wandb: 
wandb: üöÄ View run effortless-sweep-117 at: https://wandb.ai/7shoe/domShift-extensive/runs/slctcgp7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171915-slctcgp7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.1920 | Val Loss: 3.0312
2025-03-26 17:19:50,571 - wandb.wandb_agent - INFO - Cleaning up finished run: slctcgp7
2025-03-26 17:19:51,429 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:19:51,430 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:19:51,432 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:19:56,440 - wandb.wandb_agent - INFO - Running runs: ['a98mtihn']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171956-a98mtihn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-122
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/a98mtihn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: a98mtihn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5219 | Val Loss: 1.9421
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.8188 | Val Loss: 1.6928
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6663 | Val Loss: 1.6764
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.7096 | Val Loss: 1.7422
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.7694 | Val Loss: 1.7947
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8172 | Val Loss: 1.8387
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8590 | Val Loss: 1.8787
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.8976 | Val Loss: 1.9161
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9340 | Val Loss: 1.9516
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.9685
wandb:   val_loss 1.98519
wandb: 
wandb: üöÄ View run soft-sweep-122 at: https://wandb.ai/7shoe/domShift-extensive/runs/a98mtihn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171956-a98mtihn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9685 | Val Loss: 1.9852
2025-03-26 17:20:52,233 - wandb.wandb_agent - INFO - Cleaning up finished run: a98mtihn
2025-03-26 17:20:52,993 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:52,993 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:20:52,996 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 17:20:58,009 - wandb.wandb_agent - INFO - Running runs: ['6747wpkx']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172059-6747wpkx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-128
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6747wpkx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 6747wpkx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1811 | Val Loss: 2.7088
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5646 | Val Loss: 2.3875
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.3183 | Val Loss: 2.2825
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2808 | Val Loss: 2.2811
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2851 | Val Loss: 2.2890
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2941 | Val Loss: 2.2984
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3027 | Val Loss: 2.3058
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3096 | Val Loss: 2.3133
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3181 | Val Loss: 2.3227
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.031 MB of 137.031 MB uploadedwandb: | 137.190 MB of 137.190 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.32821
wandb:   val_loss 2.33331
wandb: 
wandb: üöÄ View run worldly-sweep-128 at: https://wandb.ai/7shoe/domShift-extensive/runs/6747wpkx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172059-6747wpkx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3282 | Val Loss: 2.3333
2025-03-26 17:22:14,071 - wandb.wandb_agent - INFO - Cleaning up finished run: 6747wpkx
2025-03-26 17:22:14,712 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:22:14,712 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:22:14,715 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:22:19,726 - wandb.wandb_agent - INFO - Running runs: ['mqfouhfx']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172220-mqfouhfx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-138
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mqfouhfx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: mqfouhfx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7178 | Val Loss: 1.6346
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5184 | Val Loss: 1.4486
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4108 | Val Loss: 1.3994
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3586 | Val Loss: 1.3273
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3175 | Val Loss: 1.3136
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3238 | Val Loss: 1.3410
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3538 | Val Loss: 1.3668
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3962 | Val Loss: 1.4514
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4882 | Val Loss: 1.5143
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.095 MB uploadedwandb: | 137.075 MB of 137.095 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.53666
wandb:   val_loss 1.55736
wandb: 
wandb: üöÄ View run crimson-sweep-138 at: https://wandb.ai/7shoe/domShift-extensive/runs/mqfouhfx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172220-mqfouhfx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5367 | Val Loss: 1.5574
2025-03-26 17:22:55,213 - wandb.wandb_agent - INFO - Cleaning up finished run: mqfouhfx
2025-03-26 17:22:55,884 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:22:55,884 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:22:55,888 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:23:00,900 - wandb.wandb_agent - INFO - Running runs: ['s1fsta6u']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172300-s1fsta6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-143
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s1fsta6u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s1fsta6u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5506 | Val Loss: 2.5374
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5844 | Val Loss: 2.6015
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5353 | Val Loss: 2.4720
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4390 | Val Loss: 2.4218
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4369 | Val Loss: 2.4663
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4983 | Val Loss: 2.5283
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5544 | Val Loss: 2.5798
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5991 | Val Loss: 2.6121
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6143 | Val Loss: 2.6121
wandb: - 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá
wandb:   val_loss ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.58653
wandb:   val_loss 2.54351
wandb: 
wandb: üöÄ View run hardy-sweep-143 at: https://wandb.ai/7shoe/domShift-extensive/runs/s1fsta6u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172300-s1fsta6u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5865 | Val Loss: 2.5435
2025-03-26 17:24:01,757 - wandb.wandb_agent - INFO - Cleaning up finished run: s1fsta6u
2025-03-26 17:24:02,472 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:02,472 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:24:02,475 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:24:07,487 - wandb.wandb_agent - INFO - Running runs: ['5n2jiecv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172408-5n2jiecv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-151
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5n2jiecv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5n2jiecv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2463 | Val Loss: 2.7737
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.7037 | Val Loss: 2.6742
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.7311 | Val Loss: 2.7610
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7571 | Val Loss: 2.7268
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.6978 | Val Loss: 2.6438
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.6037 | Val Loss: 2.5435
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.5151 | Val Loss: 2.4720
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.4460 | Val Loss: 2.4085
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3677 | Val Loss: 2.2947
wandb: - 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.25345
wandb:   val_loss 2.20214
wandb: 
wandb: üöÄ View run expert-sweep-151 at: https://wandb.ai/7shoe/domShift-extensive/runs/5n2jiecv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172408-5n2jiecv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.2535 | Val Loss: 2.2021
2025-03-26 17:24:32,868 - wandb.wandb_agent - INFO - Cleaning up finished run: 5n2jiecv
2025-03-26 17:24:33,390 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:33,390 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:24:33,392 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:24:38,404 - wandb.wandb_agent - INFO - Running runs: ['ygki1o2n']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172438-ygki1o2n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-155
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ygki1o2n
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ygki1o2n
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2849 | Val Loss: 1.2095
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2773 | Val Loss: 1.3650
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2233 | Val Loss: 1.1731
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1637 | Val Loss: 1.1626
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1513 | Val Loss: 1.1413
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1395 | Val Loss: 1.1378
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1375 | Val Loss: 1.1367
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1364 | Val Loss: 1.1355
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1352 | Val Loss: 1.1346
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13438
wandb:   val_loss 1.13384
wandb: 
wandb: üöÄ View run grateful-sweep-155 at: https://wandb.ai/7shoe/domShift-extensive/runs/ygki1o2n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172438-ygki1o2n/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1344 | Val Loss: 1.1338
2025-03-26 17:25:23,939 - wandb.wandb_agent - INFO - Cleaning up finished run: ygki1o2n
2025-03-26 17:25:24,402 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:25:24,403 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:25:24,405 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:25:29,417 - wandb.wandb_agent - INFO - Running runs: ['uhjfalkm']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172529-uhjfalkm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-160
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uhjfalkm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: uhjfalkm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3541 | Val Loss: 1.2660
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2817 | Val Loss: 1.1630
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1009 | Val Loss: 1.0865
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1070 | Val Loss: 1.1274
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1305 | Val Loss: 1.1308
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1290 | Val Loss: 1.1278
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1266 | Val Loss: 1.1255
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1246 | Val Loss: 1.1238
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1231 | Val Loss: 1.1224
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12188
wandb:   val_loss 1.12143
wandb: 
wandb: üöÄ View run firm-sweep-160 at: https://wandb.ai/7shoe/domShift-extensive/runs/uhjfalkm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172529-uhjfalkm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1219 | Val Loss: 1.1214
2025-03-26 17:26:30,212 - wandb.wandb_agent - INFO - Cleaning up finished run: uhjfalkm
2025-03-26 17:26:30,909 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:26:30,909 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:26:30,912 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.4
2025-03-26 17:26:35,924 - wandb.wandb_agent - INFO - Running runs: ['g84g8tb6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172635-g84g8tb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-168
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/g84g8tb6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: g84g8tb6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2338 | Val Loss: 1.1336
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1246 | Val Loss: 1.1307
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1303 | Val Loss: 1.1416
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1283 | Val Loss: 1.0957
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0644 | Val Loss: 1.0591
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0761 | Val Loss: 1.1018
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1027 | Val Loss: 1.1001
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0584 | Val Loss: 1.0050
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9848 | Val Loss: 0.9835
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99212
wandb:   val_loss 1.02031
wandb: 
wandb: üöÄ View run easy-sweep-168 at: https://wandb.ai/7shoe/domShift-extensive/runs/g84g8tb6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172635-g84g8tb6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9921 | Val Loss: 1.0203
2025-03-26 17:27:06,246 - wandb.wandb_agent - INFO - Cleaning up finished run: g84g8tb6
2025-03-26 17:27:06,891 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:27:06,891 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:27:06,894 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:27:11,906 - wandb.wandb_agent - INFO - Running runs: ['3gsiytql']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172713-3gsiytql
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-172
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3gsiytql
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3gsiytql
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5973 | Val Loss: 1.3248
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3223 | Val Loss: 1.3372
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4091 | Val Loss: 1.7014
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5503 | Val Loss: 1.3791
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3535 | Val Loss: 1.3475
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3315 | Val Loss: 1.3047
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2842 | Val Loss: 1.2510
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2300 | Val Loss: 1.2101
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2030 | Val Loss: 1.1941
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.18918
wandb:   val_loss 1.18275
wandb: 
wandb: üöÄ View run helpful-sweep-172 at: https://wandb.ai/7shoe/domShift-extensive/runs/3gsiytql
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172713-3gsiytql/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1892 | Val Loss: 1.1827
2025-03-26 17:27:47,425 - wandb.wandb_agent - INFO - Cleaning up finished run: 3gsiytql
2025-03-26 17:27:47,958 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:27:47,958 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:27:47,961 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:27:52,973 - wandb.wandb_agent - INFO - Running runs: ['yzog3ntd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172754-yzog3ntd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-178
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yzog3ntd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yzog3ntd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0290 | Val Loss: 0.8807
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.8732 | Val Loss: 0.8705
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8656 | Val Loss: 0.8735
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8834 | Val Loss: 0.8828
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8815 | Val Loss: 0.8860
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8897 | Val Loss: 0.8905
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8896 | Val Loss: 0.8924
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8929 | Val Loss: 0.8925
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8926 | Val Loss: 0.8944
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.473 MB uploadedwandb: | 137.314 MB of 137.473 MB uploadedwandb: / 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.89311
wandb:   val_loss 0.89323
wandb: 
wandb: üöÄ View run helpful-sweep-178 at: https://wandb.ai/7shoe/domShift-extensive/runs/yzog3ntd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172754-yzog3ntd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8931 | Val Loss: 0.8932
2025-03-26 17:29:09,022 - wandb.wandb_agent - INFO - Cleaning up finished run: yzog3ntd
2025-03-26 17:29:09,594 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:29:09,594 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:29:09,596 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 17:29:14,608 - wandb.wandb_agent - INFO - Running runs: ['cbx6xfoz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172914-cbx6xfoz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-187
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cbx6xfoz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: cbx6xfoz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.3174 | Val Loss: 2.5969
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3785 | Val Loss: 2.5191
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6467 | Val Loss: 2.7776
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.8035 | Val Loss: 2.7778
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8224 | Val Loss: 2.8387
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8456 | Val Loss: 2.8502
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.8501 | Val Loss: 2.8476
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.7934 | Val Loss: 2.6393
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4877 | Val Loss: 2.3620
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.28438
wandb:   val_loss 2.2202
wandb: 
wandb: üöÄ View run dry-sweep-187 at: https://wandb.ai/7shoe/domShift-extensive/runs/cbx6xfoz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172914-cbx6xfoz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2844 | Val Loss: 2.2202
2025-03-26 17:30:05,324 - wandb.wandb_agent - INFO - Cleaning up finished run: cbx6xfoz
2025-03-26 17:30:05,938 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:30:05,938 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:30:05,941 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:30:10,953 - wandb.wandb_agent - INFO - Running runs: ['8q604kn2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173010-8q604kn2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-193
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8q604kn2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 8q604kn2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2620 | Val Loss: 1.1393
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1794 | Val Loss: 1.1946
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1594 | Val Loss: 1.1900
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2019 | Val Loss: 1.2170
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2038 | Val Loss: 1.1818
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1605 | Val Loss: 1.1430
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0929 | Val Loss: 1.0487
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0649 | Val Loss: 1.1320
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1614 | Val Loss: 1.1044
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÇ
wandb:   val_loss ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09094
wandb:   val_loss 1.17339
wandb: 
wandb: üöÄ View run woven-sweep-193 at: https://wandb.ai/7shoe/domShift-extensive/runs/8q604kn2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173010-8q604kn2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0909 | Val Loss: 1.1734
2025-03-26 17:30:56,579 - wandb.wandb_agent - INFO - Cleaning up finished run: 8q604kn2
2025-03-26 17:30:57,240 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:30:57,240 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:30:57,243 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:31:02,253 - wandb.wandb_agent - INFO - Running runs: ['skrr86lh']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173102-skrr86lh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-198
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/skrr86lh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: skrr86lh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.4227 | Val Loss: 1.4557
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3280 | Val Loss: 1.1663
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1445 | Val Loss: 1.0704
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0514 | Val Loss: 1.0089
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0033 | Val Loss: 0.9690
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9658 | Val Loss: 0.9428
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9373 | Val Loss: 0.9088
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9139 | Val Loss: 0.8981
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9029 | Val Loss: 0.8789
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.88571
wandb:   val_loss 0.8755
wandb: 
wandb: üöÄ View run good-sweep-198 at: https://wandb.ai/7shoe/domShift-extensive/runs/skrr86lh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173102-skrr86lh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8857 | Val Loss: 0.8755
2025-03-26 17:31:42,880 - wandb.wandb_agent - INFO - Cleaning up finished run: skrr86lh
2025-03-26 17:31:43,655 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:31:43,655 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:31:43,659 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:31:48,670 - wandb.wandb_agent - INFO - Running runs: ['m8g2q6ol']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173149-m8g2q6ol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-203
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m8g2q6ol
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: m8g2q6ol
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5352 | Val Loss: 1.2166
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1376 | Val Loss: 1.1234
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1245 | Val Loss: 1.1259
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1276 | Val Loss: 1.1291
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1334 | Val Loss: 1.1376
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1399 | Val Loss: 1.1415
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1431 | Val Loss: 1.1466
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1457 | Val Loss: 1.1453
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1481 | Val Loss: 1.1504
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15218
wandb:   val_loss 1.15344
wandb: 
wandb: üöÄ View run driven-sweep-203 at: https://wandb.ai/7shoe/domShift-extensive/runs/m8g2q6ol
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173149-m8g2q6ol/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1522 | Val Loss: 1.1534
2025-03-26 17:33:04,783 - wandb.wandb_agent - INFO - Cleaning up finished run: m8g2q6ol
2025-03-26 17:33:05,350 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:33:05,350 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:33:05,353 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:33:10,365 - wandb.wandb_agent - INFO - Running runs: ['nh8de57q']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173311-nh8de57q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-211
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nh8de57q
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: nh8de57q
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6258 | Val Loss: 1.5802
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5605 | Val Loss: 1.5587
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5925 | Val Loss: 1.6363
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6113 | Val Loss: 1.4782
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4179 | Val Loss: 1.3649
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3197 | Val Loss: 1.2575
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2646 | Val Loss: 1.2804
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2500 | Val Loss: 1.2247
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2282 | Val Loss: 1.2263
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñá‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñá‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21195
wandb:   val_loss 1.1995
wandb: 
wandb: üöÄ View run solar-sweep-211 at: https://wandb.ai/7shoe/domShift-extensive/runs/nh8de57q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173311-nh8de57q/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2120 | Val Loss: 1.1995
2025-03-26 17:33:40,776 - wandb.wandb_agent - INFO - Cleaning up finished run: nh8de57q
2025-03-26 17:33:41,343 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:33:41,343 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:33:41,346 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:33:46,358 - wandb.wandb_agent - INFO - Running runs: ['jtnih4o6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173347-jtnih4o6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-214
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jtnih4o6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jtnih4o6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4815 | Val Loss: 1.2295
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2279 | Val Loss: 1.2294
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2455 | Val Loss: 1.2368
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2147 | Val Loss: 1.1988
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2133 | Val Loss: 1.2374
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2550 | Val Loss: 1.2617
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2406 | Val Loss: 1.1975
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1475 | Val Loss: 1.0727
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0366 | Val Loss: 1.0140
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.015 MB uploadedwandb: | 32.995 MB of 33.015 MB uploadedwandb: / 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.01056
wandb:   val_loss 1.01473
wandb: 
wandb: üöÄ View run lyric-sweep-214 at: https://wandb.ai/7shoe/domShift-extensive/runs/jtnih4o6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173347-jtnih4o6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0106 | Val Loss: 1.0147
2025-03-26 17:34:16,780 - wandb.wandb_agent - INFO - Cleaning up finished run: jtnih4o6
2025-03-26 17:34:17,340 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:34:17,341 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:34:17,344 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:34:22,355 - wandb.wandb_agent - INFO - Running runs: ['xeq9c9o7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173423-xeq9c9o7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-219
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xeq9c9o7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: xeq9c9o7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4053 | Val Loss: 1.2120
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2197 | Val Loss: 1.2155
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1951 | Val Loss: 1.1894
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1988 | Val Loss: 1.2014
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2219 | Val Loss: 1.2415
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2589 | Val Loss: 1.2752
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2874 | Val Loss: 1.2984
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2878 | Val Loss: 1.2756
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2746 | Val Loss: 1.2570
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.095 MB uploadedwandb: | 137.075 MB of 137.095 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ
wandb:   val_loss ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñà‚ñá‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23641
wandb:   val_loss 1.21965
wandb: 
wandb: üöÄ View run volcanic-sweep-219 at: https://wandb.ai/7shoe/domShift-extensive/runs/xeq9c9o7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173423-xeq9c9o7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2364 | Val Loss: 1.2197
2025-03-26 17:35:07,997 - wandb.wandb_agent - INFO - Cleaning up finished run: xeq9c9o7
2025-03-26 17:35:08,796 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:08,796 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:35:08,799 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:35:13,808 - wandb.wandb_agent - INFO - Running runs: ['r0r871b3']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173513-r0r871b3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-223
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/r0r871b3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: r0r871b3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4189 | Val Loss: 1.2526
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2013 | Val Loss: 1.1592
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1479 | Val Loss: 1.1372
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1323 | Val Loss: 1.1266
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1272 | Val Loss: 1.1302
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1275 | Val Loss: 1.1363
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1305 | Val Loss: 1.1285
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1300 | Val Loss: 1.1303
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1320 | Val Loss: 1.1340
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13355
wandb:   val_loss 1.13135
wandb: 
wandb: üöÄ View run splendid-sweep-223 at: https://wandb.ai/7shoe/domShift-extensive/runs/r0r871b3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173513-r0r871b3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1336 | Val Loss: 1.1314
2025-03-26 17:35:54,349 - wandb.wandb_agent - INFO - Cleaning up finished run: r0r871b3
2025-03-26 17:35:55,152 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:55,152 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:35:55,155 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:36:00,168 - wandb.wandb_agent - INFO - Running runs: ['pakju82g']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173601-pakju82g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-229
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pakju82g
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pakju82g
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4528 | Val Loss: 1.4147
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4593 | Val Loss: 1.4931
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3931 | Val Loss: 1.2365
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1916 | Val Loss: 1.1720
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1835 | Val Loss: 1.1852
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1863 | Val Loss: 1.1889
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1885 | Val Loss: 1.1870
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1858 | Val Loss: 1.1836
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1823 | Val Loss: 1.1807
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17977
wandb:   val_loss 1.17855
wandb: 
wandb: üöÄ View run resilient-sweep-229 at: https://wandb.ai/7shoe/domShift-extensive/runs/pakju82g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173601-pakju82g/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1798 | Val Loss: 1.1785
2025-03-26 17:37:16,293 - wandb.wandb_agent - INFO - Cleaning up finished run: pakju82g
2025-03-26 17:37:17,173 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:37:17,173 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:37:17,176 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:37:22,186 - wandb.wandb_agent - INFO - Running runs: ['ays6l8tg']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173722-ays6l8tg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-238
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ays6l8tg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ays6l8tg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5852 | Val Loss: 1.3294
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3580 | Val Loss: 1.3745
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3819 | Val Loss: 1.3464
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2415 | Val Loss: 1.1868
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1888 | Val Loss: 1.1787
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1753 | Val Loss: 1.1734
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1738 | Val Loss: 1.1745
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1760 | Val Loss: 1.1766
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1781 | Val Loss: 1.1796
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.18153
wandb:   val_loss 1.18314
wandb: 
wandb: üöÄ View run breezy-sweep-238 at: https://wandb.ai/7shoe/domShift-extensive/runs/ays6l8tg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173722-ays6l8tg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1815 | Val Loss: 1.1831
2025-03-26 17:38:02,746 - wandb.wandb_agent - INFO - Cleaning up finished run: ays6l8tg
2025-03-26 17:38:03,596 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:38:03,596 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:38:03,599 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:38:08,610 - wandb.wandb_agent - INFO - Running runs: ['5oijobah']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173808-5oijobah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-244
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5oijobah
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5oijobah
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.9287 | Val Loss: 3.2617
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.1311 | Val Loss: 2.8834
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.7997 | Val Loss: 2.6295
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5675 | Val Loss: 2.4259
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3738 | Val Loss: 2.2607
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2192 | Val Loss: 2.1128
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0632 | Val Loss: 1.9645
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.9331 | Val Loss: 1.8714
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.8513 | Val Loss: 1.8049
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.056 MB uploadedwandb: | 137.037 MB of 137.056 MB uploadedwandb: / 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.79009
wandb:   val_loss 1.75809
wandb: 
wandb: üöÄ View run brisk-sweep-244 at: https://wandb.ai/7shoe/domShift-extensive/runs/5oijobah
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173808-5oijobah/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7901 | Val Loss: 1.7581
2025-03-26 17:38:39,029 - wandb.wandb_agent - INFO - Cleaning up finished run: 5oijobah
2025-03-26 17:38:39,621 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:38:39,621 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:38:39,624 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:38:44,636 - wandb.wandb_agent - INFO - Running runs: ['b4gwg9xg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173846-b4gwg9xg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-248
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b4gwg9xg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: b4gwg9xg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8285 | Val Loss: 1.4974
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5066 | Val Loss: 1.4761
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4186 | Val Loss: 1.3751
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3895 | Val Loss: 1.4046
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4096 | Val Loss: 1.4109
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4110 | Val Loss: 1.4122
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4159 | Val Loss: 1.4146
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4256 | Val Loss: 1.4436
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4603 | Val Loss: 1.4704
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.234 MB uploadedwandb: | 137.075 MB of 137.234 MB uploadedwandb: / 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.47592
wandb:   val_loss 1.46493
wandb: 
wandb: üöÄ View run golden-sweep-248 at: https://wandb.ai/7shoe/domShift-extensive/runs/b4gwg9xg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173846-b4gwg9xg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4759 | Val Loss: 1.4649
2025-03-26 17:39:50,641 - wandb.wandb_agent - INFO - Cleaning up finished run: b4gwg9xg
2025-03-26 17:39:51,348 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:51,348 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:39:51,351 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:39:56,364 - wandb.wandb_agent - INFO - Running runs: ['29kfat8a']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173956-29kfat8a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-257
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/29kfat8a
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 29kfat8a
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3245 | Val Loss: 1.1487
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1568 | Val Loss: 1.1784
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1917 | Val Loss: 1.2069
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2112 | Val Loss: 1.2083
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2013 | Val Loss: 1.1989
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2014 | Val Loss: 1.2039
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2041 | Val Loss: 1.2043
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2035 | Val Loss: 1.2029
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2033 | Val Loss: 1.2042
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20392
wandb:   val_loss 1.20439
wandb: 
wandb: üöÄ View run mild-sweep-257 at: https://wandb.ai/7shoe/domShift-extensive/runs/29kfat8a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173956-29kfat8a/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2039 | Val Loss: 1.2044
2025-03-26 17:40:26,782 - wandb.wandb_agent - INFO - Cleaning up finished run: 29kfat8a
2025-03-26 17:40:27,685 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:40:27,685 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:40:27,688 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:40:32,699 - wandb.wandb_agent - INFO - Running runs: ['wyyv1kgo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174033-wyyv1kgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-261
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wyyv1kgo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wyyv1kgo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.6604 | Val Loss: 2.4617
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5729 | Val Loss: 2.6773
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6742 | Val Loss: 2.6900
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7073 | Val Loss: 2.7206
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7663 | Val Loss: 2.8285
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8875 | Val Loss: 2.9475
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.9989 | Val Loss: 3.0466
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.0844 | Val Loss: 3.1217
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.1547 | Val Loss: 3.1851
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.425 MB of 137.425 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.20543
wandb:   val_loss 3.22449
wandb: 
wandb: üöÄ View run frosty-sweep-261 at: https://wandb.ai/7shoe/domShift-extensive/runs/wyyv1kgo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174033-wyyv1kgo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.2054 | Val Loss: 3.2245
2025-03-26 17:41:53,912 - wandb.wandb_agent - INFO - Cleaning up finished run: wyyv1kgo
2025-03-26 17:41:54,595 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:41:54,595 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimCLR
	model_class: cnn
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:41:54,598 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimCLR --model_class=cnn --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.9
2025-03-26 17:41:59,610 - wandb.wandb_agent - INFO - Running runs: ['r76oicqf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174200-r76oicqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-271
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/r76oicqf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: r76oicqf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 5.4566 | Val Loss: 5.2705
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 5.2576 | Val Loss: 5.2333
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 5.2376 | Val Loss: 5.2238
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 5.2266 | Val Loss: 5.2145
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 5.2205 | Val Loss: 5.2075
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 5.2137 | Val Loss: 5.1987
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 5.2057 | Val Loss: 5.1984
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 5.2058 | Val Loss: 5.1911
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 5.2020 | Val Loss: 5.1922
wandb: - 10.265 MB of 10.265 MB uploadedwandb: \ 10.265 MB of 10.265 MB uploadedwandb: | 10.285 MB of 10.285 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.19594
wandb:   val_loss 5.18781
wandb: 
wandb: üöÄ View run eager-sweep-271 at: https://wandb.ai/7shoe/domShift-extensive/runs/r76oicqf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174200-r76oicqf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 5.1959 | Val Loss: 5.1878
2025-03-26 17:42:35,109 - wandb.wandb_agent - INFO - Cleaning up finished run: r76oicqf
2025-03-26 17:42:35,714 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:42:35,714 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:42:35,716 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:42:40,728 - wandb.wandb_agent - INFO - Running runs: ['wtvyg930']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174240-wtvyg930
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-275
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wtvyg930
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wtvyg930
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2373 | Val Loss: 1.0625
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0689 | Val Loss: 1.0836
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1185 | Val Loss: 1.1023
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1139 | Val Loss: 1.1128
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1198 | Val Loss: 1.1212
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1201 | Val Loss: 1.0981
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0657 | Val Loss: 1.0183
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0026 | Val Loss: 0.9967
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9886 | Val Loss: 0.9831
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.97839
wandb:   val_loss 0.97425
wandb: 
wandb: üöÄ View run silvery-sweep-275 at: https://wandb.ai/7shoe/domShift-extensive/runs/wtvyg930
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174240-wtvyg930/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9784 | Val Loss: 0.9742
2025-03-26 17:43:51,703 - wandb.wandb_agent - INFO - Cleaning up finished run: wtvyg930
2025-03-26 17:43:52,306 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:43:52,307 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:43:52,310 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:43:57,323 - wandb.wandb_agent - INFO - Running runs: ['h9wglh4u']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174357-h9wglh4u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-284
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/h9wglh4u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: h9wglh4u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.5480 | Val Loss: 3.1501
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.0448 | Val Loss: 2.9278
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8498 | Val Loss: 2.7596
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.6723 | Val Loss: 2.6116
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.5915 | Val Loss: 2.5794
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.5788 | Val Loss: 2.5799
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.5837 | Val Loss: 2.5868
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.5908 | Val Loss: 2.5943
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.5972 | Val Loss: 2.5997
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.60343
wandb:   val_loss 2.60769
wandb: 
wandb: üöÄ View run wandering-sweep-284 at: https://wandb.ai/7shoe/domShift-extensive/runs/h9wglh4u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174357-h9wglh4u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.6034 | Val Loss: 2.6077
2025-03-26 17:44:42,964 - wandb.wandb_agent - INFO - Cleaning up finished run: h9wglh4u
2025-03-26 17:44:44,006 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:44:44,006 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:44:44,008 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:44:49,019 - wandb.wandb_agent - INFO - Running runs: ['tf0nfylj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174449-tf0nfylj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-290
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tf0nfylj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: tf0nfylj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.8430 | Val Loss: 1.2699
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1827 | Val Loss: 1.1200
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1117 | Val Loss: 1.1110
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1175 | Val Loss: 1.1251
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1328 | Val Loss: 1.1407
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1483 | Val Loss: 1.1561
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1633 | Val Loss: 1.1707
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1774 | Val Loss: 1.1843
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1902 | Val Loss: 1.1962
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20132
wandb:   val_loss 1.20653
wandb: 
wandb: üöÄ View run earthy-sweep-290 at: https://wandb.ai/7shoe/domShift-extensive/runs/tf0nfylj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174449-tf0nfylj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2013 | Val Loss: 1.2065
2025-03-26 17:45:34,700 - wandb.wandb_agent - INFO - Cleaning up finished run: tf0nfylj
2025-03-26 17:45:35,486 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:45:35,486 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:45:35,489 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:45:40,501 - wandb.wandb_agent - INFO - Running runs: ['zfw0awqj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174540-zfw0awqj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-296
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zfw0awqj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zfw0awqj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4876 | Val Loss: 1.2237
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3239 | Val Loss: 1.3755
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3867 | Val Loss: 1.4151
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4183 | Val Loss: 1.4209
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4038 | Val Loss: 1.3788
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3742 | Val Loss: 1.3697
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3660 | Val Loss: 1.3605
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3229 | Val Loss: 1.2868
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2064 | Val Loss: 1.0990
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10121
wandb:   val_loss 1.09911
wandb: 
wandb: üöÄ View run lucky-sweep-296 at: https://wandb.ai/7shoe/domShift-extensive/runs/zfw0awqj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174540-zfw0awqj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1012 | Val Loss: 1.0991
2025-03-26 17:46:21,053 - wandb.wandb_agent - INFO - Cleaning up finished run: zfw0awqj
2025-03-26 17:46:21,594 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:46:21,594 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:46:21,597 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:46:26,609 - wandb.wandb_agent - INFO - Running runs: ['2bt9nncm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174626-2bt9nncm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-301
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2bt9nncm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 2bt9nncm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6122 | Val Loss: 1.4633
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3668 | Val Loss: 1.3121
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2767 | Val Loss: 1.2365
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2244 | Val Loss: 1.2231
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2277 | Val Loss: 1.2285
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2312 | Val Loss: 1.2329
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2291 | Val Loss: 1.2235
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2213 | Val Loss: 1.2210
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2213 | Val Loss: 1.2201
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20021
wandb:   val_loss 1.08831
wandb: 
wandb: üöÄ View run dainty-sweep-301 at: https://wandb.ai/7shoe/domShift-extensive/runs/2bt9nncm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174626-2bt9nncm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2002 | Val Loss: 1.0883
2025-03-26 17:47:07,071 - wandb.wandb_agent - INFO - Cleaning up finished run: 2bt9nncm
2025-03-26 17:47:07,711 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:47:07,711 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:47:07,714 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:47:12,725 - wandb.wandb_agent - INFO - Running runs: ['654ykynb']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174712-654ykynb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-305
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/654ykynb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 654ykynb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.5974 | Val Loss: 3.3714
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.1422 | Val Loss: 2.8757
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6822 | Val Loss: 2.5057
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3629 | Val Loss: 2.2179
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1526 | Val Loss: 2.1209
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1257 | Val Loss: 2.1373
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1520 | Val Loss: 2.1679
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1783 | Val Loss: 2.1860
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1897 | Val Loss: 2.1938
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.19922
wandb:   val_loss 2.20567
wandb: 
wandb: üöÄ View run wild-sweep-305 at: https://wandb.ai/7shoe/domShift-extensive/runs/654ykynb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174712-654ykynb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1992 | Val Loss: 2.2057
2025-03-26 17:48:08,451 - wandb.wandb_agent - INFO - Cleaning up finished run: 654ykynb
2025-03-26 17:48:09,070 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:48:09,070 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:48:09,073 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:48:14,085 - wandb.wandb_agent - INFO - Running runs: ['o3y5eoov']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174815-o3y5eoov
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-311
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o3y5eoov
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o3y5eoov
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6492 | Val Loss: 1.4790
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4539 | Val Loss: 1.4170
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2651 | Val Loss: 1.1191
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1723 | Val Loss: 1.2030
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1686 | Val Loss: 1.1281
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1035 | Val Loss: 1.0861
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0808 | Val Loss: 1.0770
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0758 | Val Loss: 1.0738
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0732 | Val Loss: 1.0722
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07291
wandb:   val_loss 1.07341
wandb: 
wandb: üöÄ View run whole-sweep-311 at: https://wandb.ai/7shoe/domShift-extensive/runs/o3y5eoov
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174815-o3y5eoov/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0729 | Val Loss: 1.0734
2025-03-26 17:48:54,681 - wandb.wandb_agent - INFO - Cleaning up finished run: o3y5eoov
2025-03-26 17:48:55,286 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:48:55,286 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:48:55,289 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:49:00,301 - wandb.wandb_agent - INFO - Running runs: ['re81s2be']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174901-re81s2be
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-317
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/re81s2be
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: re81s2be
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.3705 | Val Loss: 2.0618
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0338 | Val Loss: 2.0068
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.9957 | Val Loss: 1.9897
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.9954 | Val Loss: 2.0031
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0078 | Val Loss: 2.0113
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0118 | Val Loss: 2.0108
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0095 | Val Loss: 2.0076
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0057 | Val Loss: 2.0038
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0021 | Val Loss: 2.0005
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.99992
wandb:   val_loss 1.99961
wandb: 
wandb: üöÄ View run fancy-sweep-317 at: https://wandb.ai/7shoe/domShift-extensive/runs/re81s2be
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174901-re81s2be/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9999 | Val Loss: 1.9996
2025-03-26 17:49:45,973 - wandb.wandb_agent - INFO - Cleaning up finished run: re81s2be
2025-03-26 17:49:46,601 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:46,601 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:49:46,603 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:49:51,615 - wandb.wandb_agent - INFO - Running runs: ['0hvadff2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174952-0hvadff2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-323
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0hvadff2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 0hvadff2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.2152 | Val Loss: 2.2305
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1366 | Val Loss: 2.1630
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1711 | Val Loss: 2.1653
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1744 | Val Loss: 2.1862
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1957 | Val Loss: 2.2061
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2159 | Val Loss: 2.2248
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2314 | Val Loss: 2.2396
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2486 | Val Loss: 2.2578
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2662 | Val Loss: 2.2752
wandb: - 137.051 MB of 137.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb:   val_loss ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.28334
wandb:   val_loss 2.29184
wandb: 
wandb: üöÄ View run peachy-sweep-323 at: https://wandb.ai/7shoe/domShift-extensive/runs/0hvadff2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174952-0hvadff2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2833 | Val Loss: 2.2918
2025-03-26 17:50:42,309 - wandb.wandb_agent - INFO - Cleaning up finished run: 0hvadff2
2025-03-26 17:50:43,213 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:50:43,213 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:50:43,216 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:50:48,228 - wandb.wandb_agent - INFO - Running runs: ['ku4ilb50']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175049-ku4ilb50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-331
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ku4ilb50
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ku4ilb50
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5053 | Val Loss: 1.3114
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1127 | Val Loss: 1.0649
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0755 | Val Loss: 1.0536
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0553 | Val Loss: 1.0624
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0614 | Val Loss: 1.0687
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0733 | Val Loss: 1.0746
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0710 | Val Loss: 1.0696
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0617 | Val Loss: 1.0571
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0626 | Val Loss: 1.0727
wandb: - 32.947 MB of 32.947 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06887
wandb:   val_loss 1.06205
wandb: 
wandb: üöÄ View run fallen-sweep-331 at: https://wandb.ai/7shoe/domShift-extensive/runs/ku4ilb50
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175049-ku4ilb50/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0689 | Val Loss: 1.0620
2025-03-26 17:51:33,858 - wandb.wandb_agent - INFO - Cleaning up finished run: ku4ilb50
2025-03-26 17:51:34,644 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:51:34,644 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:51:34,646 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:51:39,659 - wandb.wandb_agent - INFO - Running runs: ['mavomk9i']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175139-mavomk9i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-335
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mavomk9i
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mavomk9i
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4876 | Val Loss: 0.9203
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9298 | Val Loss: 0.9373
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9488 | Val Loss: 0.9548
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9546 | Val Loss: 0.9549
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9547 | Val Loss: 0.9550
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9548 | Val Loss: 0.9551
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9549 | Val Loss: 0.9552
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9550 | Val Loss: 0.9552
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9551 | Val Loss: 0.9553
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95513
wandb:   val_loss 0.95539
wandb: 
wandb: üöÄ View run toasty-sweep-335 at: https://wandb.ai/7shoe/domShift-extensive/runs/mavomk9i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175139-mavomk9i/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9551 | Val Loss: 0.9554
2025-03-26 17:52:50,699 - wandb.wandb_agent - INFO - Cleaning up finished run: mavomk9i
2025-03-26 17:52:51,276 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:52:51,276 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:52:51,279 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 17:52:56,291 - wandb.wandb_agent - INFO - Running runs: ['nv3ahmrx']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175257-nv3ahmrx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-344
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nv3ahmrx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: nv3ahmrx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4827 | Val Loss: 1.1044
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0565 | Val Loss: 1.0231
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0396 | Val Loss: 1.0761
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0991 | Val Loss: 1.0977
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1217 | Val Loss: 1.1513
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1630 | Val Loss: 1.1733
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1722 | Val Loss: 1.1666
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1622 | Val Loss: 1.1623
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1787 | Val Loss: 1.1971
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20643
wandb:   val_loss 1.21052
wandb: 
wandb: üöÄ View run leafy-sweep-344 at: https://wandb.ai/7shoe/domShift-extensive/runs/nv3ahmrx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175257-nv3ahmrx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2064 | Val Loss: 1.2105
2025-03-26 17:53:47,011 - wandb.wandb_agent - INFO - Cleaning up finished run: nv3ahmrx
2025-03-26 17:53:47,836 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:53:47,836 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:53:47,839 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:53:52,850 - wandb.wandb_agent - INFO - Running runs: ['pqoccmgx']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175352-pqoccmgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-350
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pqoccmgx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pqoccmgx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1557 | Val Loss: 0.9982
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0199 | Val Loss: 1.0266
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0365 | Val Loss: 1.0439
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0518 | Val Loss: 1.0592
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0651 | Val Loss: 1.0699
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0746 | Val Loss: 1.0787
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0829 | Val Loss: 1.0866
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0904 | Val Loss: 1.0939
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0974 | Val Loss: 1.1005
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10369
wandb:   val_loss 1.10646
wandb: 
wandb: üöÄ View run royal-sweep-350 at: https://wandb.ai/7shoe/domShift-extensive/runs/pqoccmgx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175352-pqoccmgx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1037 | Val Loss: 1.1065
2025-03-26 17:54:43,615 - wandb.wandb_agent - INFO - Cleaning up finished run: pqoccmgx
2025-03-26 17:54:49,452 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:54:49,453 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:54:49,456 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 17:54:54,470 - wandb.wandb_agent - INFO - Running runs: ['qlbq4azh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175454-qlbq4azh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-355
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qlbq4azh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qlbq4azh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4860 | Val Loss: 1.2637
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2255 | Val Loss: 1.2061
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2013 | Val Loss: 1.1966
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1951 | Val Loss: 1.1922
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1898 | Val Loss: 1.1887
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1877 | Val Loss: 1.1771
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1539 | Val Loss: 1.1558
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1601 | Val Loss: 1.1627
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1619 | Val Loss: 1.1624
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16152
wandb:   val_loss 1.16222
wandb: 
wandb: üöÄ View run sunny-sweep-355 at: https://wandb.ai/7shoe/domShift-extensive/runs/qlbq4azh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175454-qlbq4azh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1615 | Val Loss: 1.1622
2025-03-26 17:56:05,439 - wandb.wandb_agent - INFO - Cleaning up finished run: qlbq4azh
2025-03-26 17:56:06,378 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:56:06,378 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:56:06,381 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:56:11,392 - wandb.wandb_agent - INFO - Running runs: ['tkdzl4tz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175612-tkdzl4tz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-365
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tkdzl4tz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: tkdzl4tz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3319 | Val Loss: 1.1467
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0141 | Val Loss: 0.9245
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9282 | Val Loss: 0.9348
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9330 | Val Loss: 0.9255
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9256 | Val Loss: 0.9292
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9308 | Val Loss: 0.9348
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9404 | Val Loss: 0.9446
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9435 | Val Loss: 0.9419
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9416 | Val Loss: 0.9426
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.94309
wandb:   val_loss 0.94345
wandb: 
wandb: üöÄ View run astral-sweep-365 at: https://wandb.ai/7shoe/domShift-extensive/runs/tkdzl4tz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175612-tkdzl4tz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9431 | Val Loss: 0.9435
2025-03-26 17:56:41,820 - wandb.wandb_agent - INFO - Cleaning up finished run: tkdzl4tz
2025-03-26 17:56:42,371 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:56:42,371 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:56:42,373 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:56:47,385 - wandb.wandb_agent - INFO - Running runs: ['o0o3ccr8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175647-o0o3ccr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-369
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o0o3ccr8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o0o3ccr8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3810 | Val Loss: 1.2979
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2178 | Val Loss: 1.1172
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1229 | Val Loss: 1.1548
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1375 | Val Loss: 1.1161
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1205 | Val Loss: 1.1300
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1227 | Val Loss: 1.1237
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1245 | Val Loss: 1.1226
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1236 | Val Loss: 1.1242
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1234 | Val Loss: 1.1248
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1235
wandb:   val_loss 1.12455
wandb: 
wandb: üöÄ View run classic-sweep-369 at: https://wandb.ai/7shoe/domShift-extensive/runs/o0o3ccr8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175647-o0o3ccr8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1235 | Val Loss: 1.1246
2025-03-26 17:57:38,077 - wandb.wandb_agent - INFO - Cleaning up finished run: o0o3ccr8
2025-03-26 17:57:38,632 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:57:38,632 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:57:38,635 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 17:57:43,647 - wandb.wandb_agent - INFO - Running runs: ['meeinra1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175744-meeinra1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-374
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/meeinra1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: meeinra1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.8311 | Val Loss: 3.4477
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.2490 | Val Loss: 2.9797
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.7910 | Val Loss: 2.6041
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.5247 | Val Loss: 2.4447
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.4172 | Val Loss: 2.3793
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.3481 | Val Loss: 2.3001
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.2529 | Val Loss: 2.1935
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.1536 | Val Loss: 2.1075
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.0803 | Val Loss: 2.0474
wandb: - 32.851 MB of 32.851 MB uploadedwandb: \ 32.851 MB of 32.851 MB uploadedwandb: | 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.02913
wandb:   val_loss 2.00613
wandb: 
wandb: üöÄ View run usual-sweep-374 at: https://wandb.ai/7shoe/domShift-extensive/runs/meeinra1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175744-meeinra1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.0291 | Val Loss: 2.0061
2025-03-26 17:58:14,093 - wandb.wandb_agent - INFO - Cleaning up finished run: meeinra1
2025-03-26 17:58:14,625 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:58:14,625 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:58:14,627 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:58:19,639 - wandb.wandb_agent - INFO - Running runs: ['tkcefbup']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175820-tkcefbup
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-380
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tkcefbup
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tkcefbup
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6478 | Val Loss: 1.2983
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5149 | Val Loss: 1.6597
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5414 | Val Loss: 1.4727
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4591 | Val Loss: 1.5285
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6015 | Val Loss: 1.5900
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5269 | Val Loss: 1.4884
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4892 | Val Loss: 1.4959
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4346 | Val Loss: 1.3634
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2609 | Val Loss: 1.0812
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05753
wandb:   val_loss 1.03079
wandb: 
wandb: üöÄ View run stellar-sweep-380 at: https://wandb.ai/7shoe/domShift-extensive/runs/tkcefbup
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175820-tkcefbup/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0575 | Val Loss: 1.0308
2025-03-26 17:59:15,397 - wandb.wandb_agent - INFO - Cleaning up finished run: tkcefbup
2025-03-26 17:59:19,711 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:59:19,711 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:59:19,714 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:59:24,726 - wandb.wandb_agent - INFO - Running runs: ['ihelt05z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175925-ihelt05z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-384
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ihelt05z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ihelt05z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7374 | Val Loss: 1.6040
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5733 | Val Loss: 1.5573
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5415 | Val Loss: 1.5233
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5103 | Val Loss: 1.4991
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5004 | Val Loss: 1.5016
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4966 | Val Loss: 1.4784
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4616 | Val Loss: 1.4462
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4418 | Val Loss: 1.4390
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4346 | Val Loss: 1.4228
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33226
wandb:   val_loss 1.21271
wandb: 
wandb: üöÄ View run vivid-sweep-384 at: https://wandb.ai/7shoe/domShift-extensive/runs/ihelt05z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175925-ihelt05z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3323 | Val Loss: 1.2127
2025-03-26 18:00:05,292 - wandb.wandb_agent - INFO - Cleaning up finished run: ihelt05z
2025-03-26 18:00:05,924 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:05,924 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:00:05,927 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:00:10,939 - wandb.wandb_agent - INFO - Running runs: ['jytuhe91']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180012-jytuhe91
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-391
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jytuhe91
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jytuhe91
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.7039 | Val Loss: 3.3556
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.3095 | Val Loss: 3.2681
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.2446 | Val Loss: 3.2129
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.1833 | Val Loss: 3.1410
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.1050 | Val Loss: 3.0565
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.0188 | Val Loss: 2.9645
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.9173 | Val Loss: 2.8541
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.7999 | Val Loss: 2.7266
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6786 | Val Loss: 2.6193
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.59692
wandb:   val_loss 2.56992
wandb: 
wandb: üöÄ View run glamorous-sweep-391 at: https://wandb.ai/7shoe/domShift-extensive/runs/jytuhe91
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180012-jytuhe91/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5969 | Val Loss: 2.5699
2025-03-26 18:00:46,459 - wandb.wandb_agent - INFO - Cleaning up finished run: jytuhe91
2025-03-26 18:00:47,855 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:47,855 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:00:47,858 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 18:00:52,870 - wandb.wandb_agent - INFO - Running runs: ['7k1y45hm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180054-7k1y45hm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-396
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7k1y45hm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7k1y45hm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6782 | Val Loss: 1.4173
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3229 | Val Loss: 1.2617
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2416 | Val Loss: 1.2352
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2413 | Val Loss: 1.2423
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2390 | Val Loss: 1.2318
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2323 | Val Loss: 1.2330
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2350 | Val Loss: 1.2345
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2343 | Val Loss: 1.2328
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2331 | Val Loss: 1.2326
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23349
wandb:   val_loss 1.23335
wandb: 
wandb: üöÄ View run restful-sweep-396 at: https://wandb.ai/7shoe/domShift-extensive/runs/7k1y45hm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180054-7k1y45hm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2335 | Val Loss: 1.2334
2025-03-26 18:01:48,663 - wandb.wandb_agent - INFO - Cleaning up finished run: 7k1y45hm
2025-03-26 18:01:49,328 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:01:49,329 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:01:49,331 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:01:54,340 - wandb.wandb_agent - INFO - Running runs: ['yh8mawng']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180154-yh8mawng
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-401
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yh8mawng
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yh8mawng
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5283 | Val Loss: 1.3107
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2733 | Val Loss: 1.2191
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1745 | Val Loss: 1.1189
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1098 | Val Loss: 1.1108
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1345 | Val Loss: 1.1659
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1981 | Val Loss: 1.2371
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2755 | Val Loss: 1.3060
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3190 | Val Loss: 1.3283
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3360 | Val Loss: 1.3395
wandb: - 29.698 MB of 32.995 MB uploadedwandb: \ 29.718 MB of 33.014 MB uploadedwandb: | 29.718 MB of 33.014 MB uploadedwandb: / 29.718 MB of 33.014 MB uploadedwandb: - 29.718 MB of 33.014 MB uploadedwandb: \ 29.718 MB of 33.014 MB uploadedwandb: | 33.014 MB of 33.014 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33967
wandb:   val_loss 1.33705
wandb: 
wandb: üöÄ View run misty-sweep-401 at: https://wandb.ai/7shoe/domShift-extensive/runs/yh8mawng
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180154-yh8mawng/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3397 | Val Loss: 1.3370
2025-03-26 18:02:29,799 - wandb.wandb_agent - INFO - Cleaning up finished run: yh8mawng
2025-03-26 18:02:30,395 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:02:30,395 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:02:30,398 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:02:35,409 - wandb.wandb_agent - INFO - Running runs: ['fedxcww6']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180235-fedxcww6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-406
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fedxcww6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fedxcww6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4738 | Val Loss: 1.2807
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1982 | Val Loss: 1.1407
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0681 | Val Loss: 1.0534
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1070 | Val Loss: 1.0895
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0440 | Val Loss: 1.0196
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0018 | Val Loss: 0.9729
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9546 | Val Loss: 0.9486
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9567 | Val Loss: 0.9620
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9559 | Val Loss: 0.9448
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93925
wandb:   val_loss 0.93742
wandb: 
wandb: üöÄ View run ethereal-sweep-406 at: https://wandb.ai/7shoe/domShift-extensive/runs/fedxcww6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180235-fedxcww6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9393 | Val Loss: 0.9374
2025-03-26 18:03:46,414 - wandb.wandb_agent - INFO - Cleaning up finished run: fedxcww6
2025-03-26 18:03:47,351 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:03:47,351 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:03:47,354 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:03:52,367 - wandb.wandb_agent - INFO - Running runs: ['if5o4gba']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180353-if5o4gba
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-414
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/if5o4gba
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: if5o4gba
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3045 | Val Loss: 1.1424
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1716 | Val Loss: 1.1961
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2393 | Val Loss: 1.3421
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3036 | Val Loss: 1.3231
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3677 | Val Loss: 1.4303
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4858 | Val Loss: 1.5525
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4624 | Val Loss: 1.3100
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3320 | Val Loss: 1.3347
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3385 | Val Loss: 1.3391
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.234 MB uploadedwandb: | 137.075 MB of 137.234 MB uploadedwandb: / 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34036
wandb:   val_loss 1.34044
wandb: 
wandb: üöÄ View run clean-sweep-414 at: https://wandb.ai/7shoe/domShift-extensive/runs/if5o4gba
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180353-if5o4gba/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3404 | Val Loss: 1.3404
2025-03-26 18:04:53,261 - wandb.wandb_agent - INFO - Cleaning up finished run: if5o4gba
2025-03-26 18:04:53,774 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:04:53,774 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:04:53,776 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:04:58,788 - wandb.wandb_agent - INFO - Running runs: ['oy76snbt']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180459-oy76snbt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-422
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/oy76snbt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: oy76snbt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2376 | Val Loss: 0.9158
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9776 | Val Loss: 1.0203
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0193 | Val Loss: 1.0105
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0143 | Val Loss: 1.0249
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0235 | Val Loss: 1.0291
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0371 | Val Loss: 1.0362
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0786 | Val Loss: 1.1555
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3339 | Val Loss: 1.3926
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3907 | Val Loss: 1.4181
wandb: - 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.41632
wandb:   val_loss 1.41414
wandb: 
wandb: üöÄ View run worldly-sweep-422 at: https://wandb.ai/7shoe/domShift-extensive/runs/oy76snbt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180459-oy76snbt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4163 | Val Loss: 1.4141
2025-03-26 18:05:44,426 - wandb.wandb_agent - INFO - Cleaning up finished run: oy76snbt
2025-03-26 18:05:44,951 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:05:44,952 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:05:44,954 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:05:49,965 - wandb.wandb_agent - INFO - Running runs: ['aszqplk8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180551-aszqplk8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-428
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/aszqplk8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: aszqplk8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5732 | Val Loss: 1.4534
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5528 | Val Loss: 1.6112
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6590 | Val Loss: 1.7006
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5932 | Val Loss: 1.5122
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5187 | Val Loss: 1.5345
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5568 | Val Loss: 1.5804
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5961 | Val Loss: 1.6108
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.6551 | Val Loss: 2.0573
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.5462 | Val Loss: 1.3444
wandb: - 32.967 MB of 32.967 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37041
wandb:   val_loss 1.39941
wandb: 
wandb: üöÄ View run easy-sweep-428 at: https://wandb.ai/7shoe/domShift-extensive/runs/aszqplk8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180551-aszqplk8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3704 | Val Loss: 1.3994
2025-03-26 18:06:35,628 - wandb.wandb_agent - INFO - Cleaning up finished run: aszqplk8
2025-03-26 18:06:36,203 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:36,203 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:06:36,206 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 18:06:41,218 - wandb.wandb_agent - INFO - Running runs: ['n2ew22kk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180641-n2ew22kk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-434
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/n2ew22kk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: n2ew22kk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9709 | Val Loss: 2.3826
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.2927 | Val Loss: 2.2476
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.2425 | Val Loss: 2.2417
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.2688 | Val Loss: 2.2971
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.3312 | Val Loss: 2.3604
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.3830 | Val Loss: 2.3982
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.3906 | Val Loss: 2.3821
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.3799 | Val Loss: 2.3746
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3718 | Val Loss: 2.3674
wandb: - 32.872 MB of 32.872 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñá‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.36634
wandb:   val_loss 2.36463
wandb: 
wandb: üöÄ View run revived-sweep-434 at: https://wandb.ai/7shoe/domShift-extensive/runs/n2ew22kk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180641-n2ew22kk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.3663 | Val Loss: 2.3646
2025-03-26 18:07:16,729 - wandb.wandb_agent - INFO - Cleaning up finished run: n2ew22kk
2025-03-26 18:07:17,500 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:07:17,500 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:07:17,504 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:07:22,515 - wandb.wandb_agent - INFO - Running runs: ['02zxmxad']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180723-02zxmxad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-439
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/02zxmxad
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 02zxmxad
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2524 | Val Loss: 3.2211
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.1391 | Val Loss: 3.1576
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.2438 | Val Loss: 3.3337
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.4614 | Val Loss: 3.5609
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.6133 | Val Loss: 3.6524
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.6731 | Val Loss: 3.6881
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.6122 | Val Loss: 3.5104
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.3771 | Val Loss: 3.2808
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.1704 | Val Loss: 3.0063
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.286 MB uploadedwandb: | 137.266 MB of 137.286 MB uploadedwandb: / 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.8643
wandb:   val_loss 2.74854
wandb: 
wandb: üöÄ View run pretty-sweep-439 at: https://wandb.ai/7shoe/domShift-extensive/runs/02zxmxad
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180723-02zxmxad/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.8643 | Val Loss: 2.7485
2025-03-26 18:08:23,380 - wandb.wandb_agent - INFO - Cleaning up finished run: 02zxmxad
2025-03-26 18:08:24,187 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:08:24,187 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:08:24,190 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:08:29,202 - wandb.wandb_agent - INFO - Running runs: ['lfyaplvt']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180830-lfyaplvt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-445
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lfyaplvt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lfyaplvt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5686 | Val Loss: 1.4041
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3446 | Val Loss: 1.3590
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3022 | Val Loss: 1.3317
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3297 | Val Loss: 1.3212
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3238 | Val Loss: 1.3297
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3285 | Val Loss: 1.3181
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3152 | Val Loss: 1.3182
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3287 | Val Loss: 1.3397
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3405 | Val Loss: 1.3344
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33044
wandb:   val_loss 1.32918
wandb: 
wandb: üöÄ View run dulcet-sweep-445 at: https://wandb.ai/7shoe/domShift-extensive/runs/lfyaplvt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180830-lfyaplvt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3304 | Val Loss: 1.3292
2025-03-26 18:09:40,222 - wandb.wandb_agent - INFO - Cleaning up finished run: lfyaplvt
2025-03-26 18:09:40,837 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:09:40,837 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:09:40,840 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 18:09:45,853 - wandb.wandb_agent - INFO - Running runs: ['oqgpculz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180947-oqgpculz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-455
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/oqgpculz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: oqgpculz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3473 | Val Loss: 0.8749
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0382 | Val Loss: 1.2117
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1748 | Val Loss: 1.1875
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2507 | Val Loss: 1.2753
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2758 | Val Loss: 1.2945
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3085 | Val Loss: 1.2982
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3303 | Val Loss: 1.3543
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3503 | Val Loss: 1.3468
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3465 | Val Loss: 1.3418
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33645
wandb:   val_loss 1.32926
wandb: 
wandb: üöÄ View run worldly-sweep-455 at: https://wandb.ai/7shoe/domShift-extensive/runs/oqgpculz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180947-oqgpculz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3364 | Val Loss: 1.3293
2025-03-26 18:10:51,806 - wandb.wandb_agent - INFO - Cleaning up finished run: oqgpculz
2025-03-26 18:10:52,709 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:10:52,709 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:10:52,712 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:10:57,724 - wandb.wandb_agent - INFO - Running runs: ['hn5fa4gu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181058-hn5fa4gu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-460
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hn5fa4gu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hn5fa4gu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.8708 | Val Loss: 2.0578
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.1241 | Val Loss: 2.1690
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.1378 | Val Loss: 2.0664
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.0461 | Val Loss: 2.0047
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.9974 | Val Loss: 1.9378
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.9131 | Val Loss: 1.8441
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.8251 | Val Loss: 1.7678
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.7549 | Val Loss: 1.7012
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.6916 | Val Loss: 1.6491
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.64138
wandb:   val_loss 1.60533
wandb: 
wandb: üöÄ View run drawn-sweep-460 at: https://wandb.ai/7shoe/domShift-extensive/runs/hn5fa4gu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181058-hn5fa4gu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.6414 | Val Loss: 1.6053
2025-03-26 18:11:28,182 - wandb.wandb_agent - INFO - Cleaning up finished run: hn5fa4gu
2025-03-26 18:11:28,800 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:11:28,800 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:11:28,803 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:11:33,815 - wandb.wandb_agent - INFO - Running runs: ['lzkb6ag4']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181133-lzkb6ag4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-466
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lzkb6ag4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lzkb6ag4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.1831 | Val Loss: 1.9777
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.8983 | Val Loss: 1.9085
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.9607 | Val Loss: 1.9397
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.9267 | Val Loss: 1.8965
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.8808 | Val Loss: 1.8451
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.8248 | Val Loss: 1.7861
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.7609 | Val Loss: 1.7223
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.7012 | Val Loss: 1.6689
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.6510 | Val Loss: 1.6231
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.60801
wandb:   val_loss 1.58265
wandb: 
wandb: üöÄ View run wise-sweep-466 at: https://wandb.ai/7shoe/domShift-extensive/runs/lzkb6ag4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181133-lzkb6ag4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.6080 | Val Loss: 1.5826
2025-03-26 18:12:04,230 - wandb.wandb_agent - INFO - Cleaning up finished run: lzkb6ag4
2025-03-26 18:12:04,841 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:12:04,841 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:12:04,843 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:12:09,856 - wandb.wandb_agent - INFO - Running runs: ['yj34qygz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181210-yj34qygz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-470
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yj34qygz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yj34qygz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.2638 | Val Loss: 1.0153
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 0.9557 | Val Loss: 1.0385
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 0.9489 | Val Loss: 0.9496
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 0.9742 | Val Loss: 0.9725
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 0.9703 | Val Loss: 0.9744
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 0.9684 | Val Loss: 0.9603
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 0.9647 | Val Loss: 0.9581
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 0.9579 | Val Loss: 0.9641
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 0.9523 | Val Loss: 0.9478
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95842
wandb:   val_loss 0.99496
wandb: 
wandb: üöÄ View run rural-sweep-470 at: https://wandb.ai/7shoe/domShift-extensive/runs/yj34qygz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181210-yj34qygz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 0.9584 | Val Loss: 0.9950
2025-03-26 18:13:20,840 - wandb.wandb_agent - INFO - Cleaning up finished run: yj34qygz
2025-03-26 18:13:21,724 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:13:21,724 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:13:21,727 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:13:26,740 - wandb.wandb_agent - INFO - Running runs: ['7xqlaomi']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181326-7xqlaomi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-478
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7xqlaomi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7xqlaomi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4028 | Val Loss: 1.2885
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2646 | Val Loss: 1.2699
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2823 | Val Loss: 1.2912
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2928 | Val Loss: 1.2967
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3042 | Val Loss: 1.3125
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3199 | Val Loss: 1.3276
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3305 | Val Loss: 1.3320
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3347 | Val Loss: 1.3386
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3415 | Val Loss: 1.3433
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34421
wandb:   val_loss 1.34513
wandb: 
wandb: üöÄ View run swept-sweep-478 at: https://wandb.ai/7shoe/domShift-extensive/runs/7xqlaomi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181326-7xqlaomi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3442 | Val Loss: 1.3451
2025-03-26 18:14:17,418 - wandb.wandb_agent - INFO - Cleaning up finished run: 7xqlaomi
2025-03-26 18:14:18,462 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:14:18,462 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:14:18,466 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:14:23,479 - wandb.wandb_agent - INFO - Running runs: ['ddu8iq79']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181423-ddu8iq79
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-482
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ddu8iq79
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ddu8iq79
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.6073 | Val Loss: 2.9239
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.0058 | Val Loss: 2.9916
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9559 | Val Loss: 2.9000
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.8485 | Val Loss: 2.7967
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7228 | Val Loss: 2.6789
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.6821 | Val Loss: 2.6778
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6250 | Val Loss: 2.5729
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5258 | Val Loss: 2.4769
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4562 | Val Loss: 2.4391
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.4219
wandb:   val_loss 2.40638
wandb: 
wandb: üöÄ View run polished-sweep-482 at: https://wandb.ai/7shoe/domShift-extensive/runs/ddu8iq79
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181423-ddu8iq79/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4219 | Val Loss: 2.4064
2025-03-26 18:15:04,052 - wandb.wandb_agent - INFO - Cleaning up finished run: ddu8iq79
2025-03-26 18:15:04,951 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:15:04,951 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:15:04,953 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:15:09,965 - wandb.wandb_agent - INFO - Running runs: ['mb74go4j']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181510-mb74go4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-488
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mb74go4j
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: mb74go4j
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6834 | Val Loss: 1.5943
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4778 | Val Loss: 1.3175
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3334 | Val Loss: 1.3548
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3578 | Val Loss: 1.3549
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3627 | Val Loss: 1.3600
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3615 | Val Loss: 1.3604
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3608 | Val Loss: 1.3614
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3611 | Val Loss: 1.3607
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3612 | Val Loss: 1.3607
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.241 MB uploadedwandb: | 137.082 MB of 137.241 MB uploadedwandb: / 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36129
wandb:   val_loss 1.3608
wandb: 
wandb: üöÄ View run ethereal-sweep-488 at: https://wandb.ai/7shoe/domShift-extensive/runs/mb74go4j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181510-mb74go4j/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3613 | Val Loss: 1.3608
2025-03-26 18:16:51,811 - wandb.wandb_agent - INFO - Cleaning up finished run: mb74go4j
2025-03-26 18:16:52,305 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:16:52,305 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:16:52,308 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:16:57,321 - wandb.wandb_agent - INFO - Running runs: ['fktlap4h']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181657-fktlap4h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-496
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fktlap4h
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fktlap4h
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.6205 | Val Loss: 2.2703
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5951 | Val Loss: 2.9087
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9867 | Val Loss: 3.0155
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.0001 | Val Loss: 2.9689
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.9347 | Val Loss: 2.8864
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8511 | Val Loss: 2.8004
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.7543 | Val Loss: 2.6960
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6687 | Val Loss: 2.6390
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6274 | Val Loss: 2.6102
wandb: - 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÅ‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.60255
wandb:   val_loss 2.59145
wandb: 
wandb: üöÄ View run worthy-sweep-496 at: https://wandb.ai/7shoe/domShift-extensive/runs/fktlap4h
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181657-fktlap4h/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.6026 | Val Loss: 2.5914
2025-03-26 18:17:42,942 - wandb.wandb_agent - INFO - Cleaning up finished run: fktlap4h
2025-03-26 18:17:43,674 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:17:43,674 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:17:43,677 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:17:48,688 - wandb.wandb_agent - INFO - Running runs: ['hvtmw6xd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181748-hvtmw6xd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-501
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hvtmw6xd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: hvtmw6xd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7558 | Val Loss: 2.0006
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7142 | Val Loss: 1.5574
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5163 | Val Loss: 1.4974
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5080 | Val Loss: 1.5241
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5467 | Val Loss: 1.5699
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5931 | Val Loss: 1.6162
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6378 | Val Loss: 1.6589
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6783 | Val Loss: 1.6977
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.7153 | Val Loss: 1.7326
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.196 MB of 137.196 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.74986
wandb:   val_loss 1.76795
wandb: 
wandb: üöÄ View run vibrant-sweep-501 at: https://wandb.ai/7shoe/domShift-extensive/runs/hvtmw6xd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181748-hvtmw6xd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7499 | Val Loss: 1.7679
2025-03-26 18:19:04,761 - wandb.wandb_agent - INFO - Cleaning up finished run: hvtmw6xd
2025-03-26 18:19:05,405 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:19:05,405 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:19:05,408 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:19:10,420 - wandb.wandb_agent - INFO - Running runs: ['3voa03ou']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181911-3voa03ou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-508
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3voa03ou
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3voa03ou
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 2.7870 | Val Loss: 1.9068
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.8986 | Val Loss: 1.9636
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.0378 | Val Loss: 2.0932
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.1199 | Val Loss: 2.1361
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.1409 | Val Loss: 2.1418
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.1414 | Val Loss: 2.1406
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.1418 | Val Loss: 2.1441
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.1363 | Val Loss: 2.0536
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.9595 | Val Loss: 1.8948
wandb: - 137.025 MB of 137.025 MB uploadedwandb: \ 137.025 MB of 137.025 MB uploadedwandb: | 137.183 MB of 137.183 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.86239
wandb:   val_loss 1.83958
wandb: 
wandb: üöÄ View run youthful-sweep-508 at: https://wandb.ai/7shoe/domShift-extensive/runs/3voa03ou
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181911-3voa03ou/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.8624 | Val Loss: 1.8396
2025-03-26 18:20:26,520 - wandb.wandb_agent - INFO - Cleaning up finished run: 3voa03ou
2025-03-26 18:20:27,025 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:20:27,025 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:20:27,029 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:20:32,040 - wandb.wandb_agent - INFO - Running runs: ['8xs5ne0j']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182033-8xs5ne0j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-516
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8xs5ne0j
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 8xs5ne0j
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.1638 | Val Loss: 1.7452
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6328 | Val Loss: 1.5446
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5369 | Val Loss: 1.5351
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5484 | Val Loss: 1.5507
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5456 | Val Loss: 1.5417
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5381 | Val Loss: 1.5263
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5101 | Val Loss: 1.4801
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4652 | Val Loss: 1.4521
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4294 | Val Loss: 1.3976
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.38432
wandb:   val_loss 1.38112
wandb: 
wandb: üöÄ View run misty-sweep-516 at: https://wandb.ai/7shoe/domShift-extensive/runs/8xs5ne0j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182033-8xs5ne0j/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3843 | Val Loss: 1.3811
2025-03-26 18:21:07,503 - wandb.wandb_agent - INFO - Cleaning up finished run: 8xs5ne0j
2025-03-26 18:21:09,527 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:21:09,528 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:21:09,530 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:21:14,540 - wandb.wandb_agent - INFO - Running runs: ['nmbc5wnz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182115-nmbc5wnz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-520
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nmbc5wnz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: nmbc5wnz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2557 | Val Loss: 1.1208
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0665 | Val Loss: 1.0426
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0387 | Val Loss: 1.0341
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0479 | Val Loss: 1.0540
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0732 | Val Loss: 1.0888
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0915 | Val Loss: 1.0935
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0961 | Val Loss: 1.0985
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1002 | Val Loss: 1.1021
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1037 | Val Loss: 1.1055
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10698
wandb:   val_loss 1.10866
wandb: 
wandb: üöÄ View run whole-sweep-520 at: https://wandb.ai/7shoe/domShift-extensive/runs/nmbc5wnz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182115-nmbc5wnz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1070 | Val Loss: 1.1087
2025-03-26 18:22:20,487 - wandb.wandb_agent - INFO - Cleaning up finished run: nmbc5wnz
2025-03-26 18:22:21,126 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:22:21,126 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:22:21,129 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:22:26,140 - wandb.wandb_agent - INFO - Running runs: ['fg88wgh5']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182225-fg88wgh5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-527
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fg88wgh5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fg88wgh5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1725 | Val Loss: 2.9953
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7998 | Val Loss: 2.7597
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9272 | Val Loss: 3.0916
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.1702 | Val Loss: 3.2316
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.2494 | Val Loss: 3.2488
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.1398 | Val Loss: 3.0152
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.9225 | Val Loss: 2.8318
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.7857 | Val Loss: 2.7529
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.7487 | Val Loss: 2.7501
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.330 MB of 137.330 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÇ‚ñÉ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.76259
wandb:   val_loss 2.77691
wandb: 
wandb: üöÄ View run restful-sweep-527 at: https://wandb.ai/7shoe/domShift-extensive/runs/fg88wgh5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182225-fg88wgh5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7626 | Val Loss: 2.7769
2025-03-26 18:23:26,971 - wandb.wandb_agent - INFO - Cleaning up finished run: fg88wgh5
2025-03-26 18:23:27,605 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:23:27,606 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:23:27,609 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 18:23:32,621 - wandb.wandb_agent - INFO - Running runs: ['qnj0a3mq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182333-qnj0a3mq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-532
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qnj0a3mq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qnj0a3mq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6246 | Val Loss: 1.3864
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3604 | Val Loss: 1.3369
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2800 | Val Loss: 1.2058
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2093 | Val Loss: 1.2546
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1536 | Val Loss: 0.8945
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9484 | Val Loss: 0.9682
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9538 | Val Loss: 0.9714
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9760 | Val Loss: 0.9776
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9779 | Val Loss: 0.9796
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.97749
wandb:   val_loss 0.97589
wandb: 
wandb: üöÄ View run eternal-sweep-532 at: https://wandb.ai/7shoe/domShift-extensive/runs/qnj0a3mq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182333-qnj0a3mq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9775 | Val Loss: 0.9759
2025-03-26 18:24:28,364 - wandb.wandb_agent - INFO - Cleaning up finished run: qnj0a3mq
2025-03-26 18:24:28,962 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:24:28,963 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:24:28,965 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:24:33,978 - wandb.wandb_agent - INFO - Running runs: ['4wttnvxy']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182435-4wttnvxy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-538
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4wttnvxy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4wttnvxy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5127 | Val Loss: 1.3571
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3768 | Val Loss: 1.3893
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3785 | Val Loss: 1.3778
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3781 | Val Loss: 1.3774
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3777 | Val Loss: 1.3772
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3776 | Val Loss: 1.3773
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3776 | Val Loss: 1.3772
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3776 | Val Loss: 1.3772
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3776 | Val Loss: 1.3772
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37756
wandb:   val_loss 1.37716
wandb: 
wandb: üöÄ View run fearless-sweep-538 at: https://wandb.ai/7shoe/domShift-extensive/runs/4wttnvxy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182435-4wttnvxy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3776 | Val Loss: 1.3772
2025-03-26 18:25:29,751 - wandb.wandb_agent - INFO - Cleaning up finished run: 4wttnvxy
2025-03-26 18:25:30,364 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:25:30,364 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:25:30,367 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:25:35,380 - wandb.wandb_agent - INFO - Running runs: ['mplb1tgq']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182535-mplb1tgq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-545
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mplb1tgq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mplb1tgq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6511 | Val Loss: 1.4370
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3509 | Val Loss: 1.2989
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2723 | Val Loss: 1.2566
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2719 | Val Loss: 1.2823
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2923 | Val Loss: 1.3002
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3082 | Val Loss: 1.3148
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3210 | Val Loss: 1.3248
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3288 | Val Loss: 1.3308
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3316 | Val Loss: 1.3324
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33292
wandb:   val_loss 1.33247
wandb: 
wandb: üöÄ View run fresh-sweep-545 at: https://wandb.ai/7shoe/domShift-extensive/runs/mplb1tgq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182535-mplb1tgq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3329 | Val Loss: 1.3325
2025-03-26 18:26:26,079 - wandb.wandb_agent - INFO - Cleaning up finished run: mplb1tgq
2025-03-26 18:26:26,607 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:26:26,607 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:26:26,610 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:26:31,621 - wandb.wandb_agent - INFO - Running runs: ['ceywohoz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182632-ceywohoz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-550
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ceywohoz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ceywohoz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6532 | Val Loss: 1.4779
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3289 | Val Loss: 1.1959
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1680 | Val Loss: 1.1396
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1433 | Val Loss: 1.1481
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1378 | Val Loss: 1.1359
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1404 | Val Loss: 1.1287
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1356 | Val Loss: 1.1005
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0950 | Val Loss: 1.1000
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0884 | Val Loss: 1.0837
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09017
wandb:   val_loss 1.09194
wandb: 
wandb: üöÄ View run hopeful-sweep-550 at: https://wandb.ai/7shoe/domShift-extensive/runs/ceywohoz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182632-ceywohoz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0902 | Val Loss: 1.0919
2025-03-26 18:27:12,159 - wandb.wandb_agent - INFO - Cleaning up finished run: ceywohoz
2025-03-26 18:27:12,620 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:27:12,620 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:27:12,623 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:27:17,635 - wandb.wandb_agent - INFO - Running runs: ['fonwoejb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182718-fonwoejb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-556
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fonwoejb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: fonwoejb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8035 | Val Loss: 1.9042
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0426 | Val Loss: 1.9038
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7036 | Val Loss: 1.6911
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5719 | Val Loss: 1.4206
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4846 | Val Loss: 1.6331
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4580 | Val Loss: 1.4281
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5341 | Val Loss: 1.6164
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5913 | Val Loss: 1.3812
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4198 | Val Loss: 1.5958
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.233 MB of 137.233 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÑ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.66544
wandb:   val_loss 1.69658
wandb: 
wandb: üöÄ View run earthy-sweep-556 at: https://wandb.ai/7shoe/domShift-extensive/runs/fonwoejb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182718-fonwoejb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.6654 | Val Loss: 1.6966
2025-03-26 18:28:23,593 - wandb.wandb_agent - INFO - Cleaning up finished run: fonwoejb
2025-03-26 18:28:24,254 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:28:24,254 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:28:24,257 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 18:28:29,269 - wandb.wandb_agent - INFO - Running runs: ['cdpuvpdy']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182830-cdpuvpdy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-561
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cdpuvpdy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cdpuvpdy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8076 | Val Loss: 2.4740
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.4911 | Val Loss: 2.5708
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6452 | Val Loss: 2.6868
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.6531 | Val Loss: 2.5904
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5178 | Val Loss: 2.4365
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3676 | Val Loss: 2.2962
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2334 | Val Loss: 2.1690
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1345 | Val Loss: 2.1072
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0964 | Val Loss: 2.0886
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.08823
wandb:   val_loss 2.08915
wandb: 
wandb: üöÄ View run northern-sweep-561 at: https://wandb.ai/7shoe/domShift-extensive/runs/cdpuvpdy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182830-cdpuvpdy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.0882 | Val Loss: 2.0891
2025-03-26 18:29:30,123 - wandb.wandb_agent - INFO - Cleaning up finished run: cdpuvpdy
2025-03-26 18:29:30,938 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:29:30,938 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:29:30,941 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:29:35,952 - wandb.wandb_agent - INFO - Running runs: ['v6g9s5ll']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182936-v6g9s5ll
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-569
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/v6g9s5ll
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: v6g9s5ll
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3480 | Val Loss: 0.9926
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0894 | Val Loss: 1.1407
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1108 | Val Loss: 1.0653
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9678 | Val Loss: 0.8881
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.8370 | Val Loss: 0.8056
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.8047 | Val Loss: 0.8094
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.8155 | Val Loss: 0.8214
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8222 | Val Loss: 0.8217
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8241 | Val Loss: 0.8272
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.83134
wandb:   val_loss 0.83566
wandb: 
wandb: üöÄ View run silvery-sweep-569 at: https://wandb.ai/7shoe/domShift-extensive/runs/v6g9s5ll
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182936-v6g9s5ll/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8313 | Val Loss: 0.8357
2025-03-26 18:30:21,573 - wandb.wandb_agent - INFO - Cleaning up finished run: v6g9s5ll
2025-03-26 18:30:23,037 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:23,037 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:30:23,040 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:30:28,052 - wandb.wandb_agent - INFO - Running runs: ['fmjxz9au']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183027-fmjxz9au
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-572
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fmjxz9au
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fmjxz9au
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8143 | Val Loss: 1.5543
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4194 | Val Loss: 1.3211
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2858 | Val Loss: 1.2368
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2023 | Val Loss: 1.1447
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1289 | Val Loss: 1.1353
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1573 | Val Loss: 1.1852
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1861 | Val Loss: 1.1832
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1733 | Val Loss: 1.1671
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1700 | Val Loss: 1.1767
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17281
wandb:   val_loss 1.1705
wandb: 
wandb: üöÄ View run dulcet-sweep-572 at: https://wandb.ai/7shoe/domShift-extensive/runs/fmjxz9au
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183027-fmjxz9au/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1728 | Val Loss: 1.1705
2025-03-26 18:30:58,485 - wandb.wandb_agent - INFO - Cleaning up finished run: fmjxz9au
2025-03-26 18:30:58,997 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:58,997 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:30:59,000 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:31:04,012 - wandb.wandb_agent - INFO - Running runs: ['1rrgg9ea']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183103-1rrgg9ea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-577
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1rrgg9ea
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1rrgg9ea
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2622 | Val Loss: 1.1647
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1951 | Val Loss: 1.0775
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0064 | Val Loss: 1.0040
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9951 | Val Loss: 0.9861
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9884 | Val Loss: 1.0015
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9615 | Val Loss: 0.9408
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9399 | Val Loss: 0.9396
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9399 | Val Loss: 0.9399
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9399 | Val Loss: 0.9400
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93989
wandb:   val_loss 0.94
wandb: 
wandb: üöÄ View run glad-sweep-577 at: https://wandb.ai/7shoe/domShift-extensive/runs/1rrgg9ea
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183103-1rrgg9ea/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9399 | Val Loss: 0.9400
2025-03-26 18:32:20,076 - wandb.wandb_agent - INFO - Cleaning up finished run: 1rrgg9ea
2025-03-26 18:32:20,916 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:32:20,916 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:32:20,919 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:32:25,931 - wandb.wandb_agent - INFO - Running runs: ['9qypjgc0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183227-9qypjgc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-583
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9qypjgc0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9qypjgc0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6019 | Val Loss: 1.5618
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5026 | Val Loss: 1.2998
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2677 | Val Loss: 1.3308
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3986 | Val Loss: 1.4380
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4387 | Val Loss: 1.4328
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4324 | Val Loss: 1.4277
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4215 | Val Loss: 1.4079
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3997 | Val Loss: 1.3943
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3902 | Val Loss: 1.3812
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.015 MB uploadedwandb: | 32.995 MB of 33.015 MB uploadedwandb: / 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36174
wandb:   val_loss 1.33181
wandb: 
wandb: üöÄ View run mild-sweep-583 at: https://wandb.ai/7shoe/domShift-extensive/runs/9qypjgc0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183227-9qypjgc0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3617 | Val Loss: 1.3318
2025-03-26 18:33:06,494 - wandb.wandb_agent - INFO - Cleaning up finished run: 9qypjgc0
2025-03-26 18:33:06,973 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:33:06,973 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:33:06,976 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:33:11,988 - wandb.wandb_agent - INFO - Running runs: ['x0szc9nv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183312-x0szc9nv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-587
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/x0szc9nv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: x0szc9nv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3159 | Val Loss: 1.0564
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0439 | Val Loss: 1.0679
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0686 | Val Loss: 1.0691
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0767 | Val Loss: 1.0914
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0853 | Val Loss: 1.0862
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0854 | Val Loss: 1.0866
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0832 | Val Loss: 1.0850
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0826 | Val Loss: 1.0776
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0829 | Val Loss: 1.0789
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10545
wandb:   val_loss 1.10674
wandb: 
wandb: üöÄ View run dandy-sweep-587 at: https://wandb.ai/7shoe/domShift-extensive/runs/x0szc9nv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183312-x0szc9nv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1055 | Val Loss: 1.1067
2025-03-26 18:34:07,763 - wandb.wandb_agent - INFO - Cleaning up finished run: x0szc9nv
2025-03-26 18:34:08,295 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:34:08,295 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:34:08,298 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 18:34:13,311 - wandb.wandb_agent - INFO - Running runs: ['hi92f4g9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183414-hi92f4g9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-593
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hi92f4g9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: hi92f4g9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2364 | Val Loss: 1.1732
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2320 | Val Loss: 1.2363
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2324 | Val Loss: 1.3092
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2564 | Val Loss: 1.1644
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1622 | Val Loss: 1.1813
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1964 | Val Loss: 1.2020
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1924 | Val Loss: 1.1814
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1784 | Val Loss: 1.1712
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1776 | Val Loss: 1.1848
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.234 MB uploadedwandb: | 137.075 MB of 137.234 MB uploadedwandb: / 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21158
wandb:   val_loss 1.23224
wandb: 
wandb: üöÄ View run legendary-sweep-593 at: https://wandb.ai/7shoe/domShift-extensive/runs/hi92f4g9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183414-hi92f4g9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2116 | Val Loss: 1.2322
2025-03-26 18:35:14,151 - wandb.wandb_agent - INFO - Cleaning up finished run: hi92f4g9
2025-03-26 18:35:15,178 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:35:15,178 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:35:15,180 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:35:20,192 - wandb.wandb_agent - INFO - Running runs: ['ntksx0u9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183521-ntksx0u9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-603
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ntksx0u9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ntksx0u9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7806 | Val Loss: 1.6503
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6343 | Val Loss: 1.6238
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6212 | Val Loss: 1.6175
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5909 | Val Loss: 1.5432
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5279 | Val Loss: 1.5113
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5126 | Val Loss: 1.5116
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4832 | Val Loss: 1.4355
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3667 | Val Loss: 1.1250
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1156 | Val Loss: 1.1269
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12448
wandb:   val_loss 1.12946
wandb: 
wandb: üöÄ View run hopeful-sweep-603 at: https://wandb.ai/7shoe/domShift-extensive/runs/ntksx0u9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183521-ntksx0u9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1245 | Val Loss: 1.1295
2025-03-26 18:36:00,766 - wandb.wandb_agent - INFO - Cleaning up finished run: ntksx0u9
2025-03-26 18:36:01,368 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:36:01,368 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:36:01,372 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:36:06,384 - wandb.wandb_agent - INFO - Running runs: ['xrtg35ph']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183607-xrtg35ph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-607
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xrtg35ph
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xrtg35ph
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6640 | Val Loss: 1.4804
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3390 | Val Loss: 1.3426
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2105 | Val Loss: 1.2032
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2736 | Val Loss: 1.4054
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3131 | Val Loss: 1.0675
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1221 | Val Loss: 1.1733
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1814 | Val Loss: 1.1883
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1767 | Val Loss: 1.1672
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1689 | Val Loss: 1.1735
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17718
wandb:   val_loss 1.17966
wandb: 
wandb: üöÄ View run lilac-sweep-607 at: https://wandb.ai/7shoe/domShift-extensive/runs/xrtg35ph
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183607-xrtg35ph/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1772 | Val Loss: 1.1797
2025-03-26 18:37:17,350 - wandb.wandb_agent - INFO - Cleaning up finished run: xrtg35ph
2025-03-26 18:37:18,191 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:37:18,191 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:37:18,194 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:37:23,206 - wandb.wandb_agent - INFO - Running runs: ['aqk0sy50']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183724-aqk0sy50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-615
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/aqk0sy50
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: aqk0sy50
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3179 | Val Loss: 1.0000
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0860 | Val Loss: 1.2013
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1281 | Val Loss: 1.1194
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0965 | Val Loss: 1.0992
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0954 | Val Loss: 1.0553
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1006 | Val Loss: 1.1179
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1189 | Val Loss: 1.1190
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1092 | Val Loss: 1.1165
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1433 | Val Loss: 1.1901
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14233
wandb:   val_loss 1.0862
wandb: 
wandb: üöÄ View run eager-sweep-615 at: https://wandb.ai/7shoe/domShift-extensive/runs/aqk0sy50
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183724-aqk0sy50/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1423 | Val Loss: 1.0862
2025-03-26 18:38:34,181 - wandb.wandb_agent - INFO - Cleaning up finished run: aqk0sy50
2025-03-26 18:38:34,845 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:38:34,845 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:38:34,848 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 18:38:39,860 - wandb.wandb_agent - INFO - Running runs: ['bc6gq348']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183839-bc6gq348
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-624
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bc6gq348
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: bc6gq348
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6119 | Val Loss: 1.3724
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3418 | Val Loss: 1.2922
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2374 | Val Loss: 1.1924
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1821 | Val Loss: 1.1733
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1669 | Val Loss: 1.1600
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1592 | Val Loss: 1.1578
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1579 | Val Loss: 1.1568
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1570 | Val Loss: 1.1564
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1568 | Val Loss: 1.1563
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1567
wandb:   val_loss 1.15618
wandb: 
wandb: üöÄ View run lively-sweep-624 at: https://wandb.ai/7shoe/domShift-extensive/runs/bc6gq348
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183839-bc6gq348/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1567 | Val Loss: 1.1562
2025-03-26 18:39:15,349 - wandb.wandb_agent - INFO - Cleaning up finished run: bc6gq348
2025-03-26 18:39:15,906 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:39:15,907 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:39:15,909 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:39:20,921 - wandb.wandb_agent - INFO - Running runs: ['1wcx1xd1']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183920-1wcx1xd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-626
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1wcx1xd1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1wcx1xd1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2177 | Val Loss: 1.0405
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0686 | Val Loss: 1.0806
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3844 | Val Loss: 1.7616
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4703 | Val Loss: 0.9595
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8149 | Val Loss: 0.7342
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.7456 | Val Loss: 0.7481
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.7500 | Val Loss: 0.7494
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.7498 | Val Loss: 0.7499
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.7507 | Val Loss: 0.7509
wandb: - 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.75165
wandb:   val_loss 0.7518
wandb: 
wandb: üöÄ View run confused-sweep-626 at: https://wandb.ai/7shoe/domShift-extensive/runs/1wcx1xd1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183920-1wcx1xd1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.7517 | Val Loss: 0.7518
2025-03-26 18:40:26,882 - wandb.wandb_agent - INFO - Cleaning up finished run: 1wcx1xd1
2025-03-26 18:40:27,368 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:40:27,368 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:40:27,372 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 18:40:32,384 - wandb.wandb_agent - INFO - Running runs: ['4qedwhr3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184033-4qedwhr3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-634
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4qedwhr3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4qedwhr3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7002 | Val Loss: 1.4498
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4120 | Val Loss: 1.4252
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4283 | Val Loss: 1.4294
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4295 | Val Loss: 1.4242
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4146 | Val Loss: 1.4047
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4012 | Val Loss: 1.3975
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3934 | Val Loss: 1.3883
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3846 | Val Loss: 1.3810
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3779 | Val Loss: 1.3743
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37137
wandb:   val_loss 1.36813
wandb: 
wandb: üöÄ View run colorful-sweep-634 at: https://wandb.ai/7shoe/domShift-extensive/runs/4qedwhr3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184033-4qedwhr3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3714 | Val Loss: 1.3681
2025-03-26 18:41:18,001 - wandb.wandb_agent - INFO - Cleaning up finished run: 4qedwhr3
2025-03-26 18:41:18,656 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:41:18,656 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:41:18,659 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:41:23,671 - wandb.wandb_agent - INFO - Running runs: ['2kr8ohrg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184124-2kr8ohrg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-639
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2kr8ohrg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2kr8ohrg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3276 | Val Loss: 1.1806
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2027 | Val Loss: 1.2464
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2388 | Val Loss: 1.2306
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2442 | Val Loss: 1.2481
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2495 | Val Loss: 1.2504
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2507 | Val Loss: 1.2510
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2510 | Val Loss: 1.2511
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2511 | Val Loss: 1.2512
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2512 | Val Loss: 1.2514
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25127
wandb:   val_loss 1.25137
wandb: 
wandb: üöÄ View run azure-sweep-639 at: https://wandb.ai/7shoe/domShift-extensive/runs/2kr8ohrg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184124-2kr8ohrg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2513 | Val Loss: 1.2514
2025-03-26 18:42:39,766 - wandb.wandb_agent - INFO - Cleaning up finished run: 2kr8ohrg
2025-03-26 18:42:59,840 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 18:43:00,336 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:43:00,336 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:43:00,339 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:43:05,351 - wandb.wandb_agent - INFO - Running runs: ['s38qsj0s']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184305-s38qsj0s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-650
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s38qsj0s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s38qsj0s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3592 | Val Loss: 1.0891
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1508 | Val Loss: 1.1339
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3619 | Val Loss: 1.6258
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6558 | Val Loss: 1.4241
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1956 | Val Loss: 1.1816
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1361 | Val Loss: 1.1361
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1455 | Val Loss: 1.1432
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1452 | Val Loss: 1.1448
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1448 | Val Loss: 1.1453
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14504
wandb:   val_loss 1.14523
wandb: 
wandb: üöÄ View run snowy-sweep-650 at: https://wandb.ai/7shoe/domShift-extensive/runs/s38qsj0s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184305-s38qsj0s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1450 | Val Loss: 1.1452
2025-03-26 18:43:56,000 - wandb.wandb_agent - INFO - Cleaning up finished run: s38qsj0s
2025-03-26 18:43:56,501 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:43:56,501 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:43:56,504 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:44:01,516 - wandb.wandb_agent - INFO - Running runs: ['g1fd72ig']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184401-g1fd72ig
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-656
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/g1fd72ig
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: g1fd72ig
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.6559 | Val Loss: 3.8903
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.6570 | Val Loss: 3.4217
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.2477 | Val Loss: 3.0640
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.0057 | Val Loss: 2.9225
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8651 | Val Loss: 2.7523
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.7595 | Val Loss: 2.6774
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6614 | Val Loss: 2.5850
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5851 | Val Loss: 2.5113
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5129 | Val Loss: 2.4808
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.332 MB uploadedwandb: | 137.312 MB of 137.332 MB uploadedwandb: / 137.332 MB of 137.332 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.47867
wandb:   val_loss 2.41728
wandb: 
wandb: üöÄ View run scarlet-sweep-656 at: https://wandb.ai/7shoe/domShift-extensive/runs/g1fd72ig
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184401-g1fd72ig/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4787 | Val Loss: 2.4173
2025-03-26 18:44:42,060 - wandb.wandb_agent - INFO - Cleaning up finished run: g1fd72ig
2025-03-26 18:44:42,683 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:44:42,684 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:44:42,688 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:44:47,699 - wandb.wandb_agent - INFO - Running runs: ['q3jpzip2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184448-q3jpzip2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-661
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/q3jpzip2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: q3jpzip2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.6378 | Val Loss: 2.3437
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.4870 | Val Loss: 2.6760
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.7956 | Val Loss: 2.8745
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.9091 | Val Loss: 2.9324
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8534 | Val Loss: 2.5953
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4643 | Val Loss: 2.3766
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3487 | Val Loss: 2.3345
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3322 | Val Loss: 2.3295
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3221 | Val Loss: 2.3082
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÉ‚ñá‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÖ‚ñá‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.28603
wandb:   val_loss 2.25934
wandb: 
wandb: üöÄ View run ruby-sweep-661 at: https://wandb.ai/7shoe/domShift-extensive/runs/q3jpzip2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184448-q3jpzip2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2860 | Val Loss: 2.2593
2025-03-26 18:46:13,899 - wandb.wandb_agent - INFO - Cleaning up finished run: q3jpzip2
2025-03-26 18:46:14,632 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:46:14,633 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:46:14,635 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:46:19,647 - wandb.wandb_agent - INFO - Running runs: ['lfhituup']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184619-lfhituup
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-669
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lfhituup
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lfhituup
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4477 | Val Loss: 1.3112
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3363 | Val Loss: 1.3419
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3399 | Val Loss: 1.3396
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3399 | Val Loss: 1.3394
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3400 | Val Loss: 1.3397
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3401 | Val Loss: 1.3401
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3401 | Val Loss: 1.3399
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3402 | Val Loss: 1.3402
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3403 | Val Loss: 1.3402
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34035
wandb:   val_loss 1.34029
wandb: 
wandb: üöÄ View run major-sweep-669 at: https://wandb.ai/7shoe/domShift-extensive/runs/lfhituup
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184619-lfhituup/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3404 | Val Loss: 1.3403
2025-03-26 18:47:25,580 - wandb.wandb_agent - INFO - Cleaning up finished run: lfhituup
2025-03-26 18:47:26,315 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:47:26,315 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:47:26,318 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:47:31,330 - wandb.wandb_agent - INFO - Running runs: ['cco51rlj']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184731-cco51rlj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-676
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cco51rlj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cco51rlj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2030 | Val Loss: 0.9826
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0337 | Val Loss: 1.0489
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0311 | Val Loss: 1.0251
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0183 | Val Loss: 1.0144
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0128 | Val Loss: 1.0113
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0102 | Val Loss: 1.0093
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0086 | Val Loss: 1.0082
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0077 | Val Loss: 1.0077
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0074 | Val Loss: 1.0075
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.00732
wandb:   val_loss 1.00749
wandb: 
wandb: üöÄ View run good-sweep-676 at: https://wandb.ai/7shoe/domShift-extensive/runs/cco51rlj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184731-cco51rlj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0073 | Val Loss: 1.0075
2025-03-26 18:48:37,295 - wandb.wandb_agent - INFO - Cleaning up finished run: cco51rlj
2025-03-26 18:48:37,876 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:48:37,877 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:48:37,879 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:48:42,891 - wandb.wandb_agent - INFO - Running runs: ['pgwj6oii']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184842-pgwj6oii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-684
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pgwj6oii
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pgwj6oii
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2906 | Val Loss: 3.0583
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.0395 | Val Loss: 3.0335
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.0416 | Val Loss: 3.0319
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.0332 | Val Loss: 3.0187
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.0368 | Val Loss: 3.0458
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.0602 | Val Loss: 3.0741
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.0663 | Val Loss: 3.0156
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.9739 | Val Loss: 2.9010
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.8593 | Val Loss: 2.7826
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.74678
wandb:   val_loss 2.68284
wandb: 
wandb: üöÄ View run firm-sweep-684 at: https://wandb.ai/7shoe/domShift-extensive/runs/pgwj6oii
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184842-pgwj6oii/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7468 | Val Loss: 2.6828
2025-03-26 18:49:13,335 - wandb.wandb_agent - INFO - Cleaning up finished run: pgwj6oii
2025-03-26 18:49:13,805 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:49:13,805 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:49:13,810 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:49:18,822 - wandb.wandb_agent - INFO - Running runs: ['94rzgto0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184918-94rzgto0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-689
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/94rzgto0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 94rzgto0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5371 | Val Loss: 1.2984
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2320 | Val Loss: 1.1870
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1831 | Val Loss: 1.1805
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1874 | Val Loss: 1.1933
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2002 | Val Loss: 1.2076
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2132 | Val Loss: 1.2185
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2247 | Val Loss: 1.2303
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2339 | Val Loss: 1.2372
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2398 | Val Loss: 1.2420
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.24295
wandb:   val_loss 1.24407
wandb: 
wandb: üöÄ View run electric-sweep-689 at: https://wandb.ai/7shoe/domShift-extensive/runs/94rzgto0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184918-94rzgto0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2430 | Val Loss: 1.2441
2025-03-26 18:50:24,792 - wandb.wandb_agent - INFO - Cleaning up finished run: 94rzgto0
2025-03-26 18:50:25,508 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:50:25,509 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:50:25,511 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:50:30,522 - wandb.wandb_agent - INFO - Running runs: ['zoobz08j']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185031-zoobz08j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-696
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zoobz08j
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zoobz08j
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4539 | Val Loss: 1.3117
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3136 | Val Loss: 1.2862
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2252 | Val Loss: 1.2220
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2357 | Val Loss: 1.2195
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2124 | Val Loss: 1.2048
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1966 | Val Loss: 1.1884
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1836 | Val Loss: 1.1789
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1752 | Val Loss: 1.1713
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1682 | Val Loss: 1.1650
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16245
wandb:   val_loss 1.15979
wandb: 
wandb: üöÄ View run astral-sweep-696 at: https://wandb.ai/7shoe/domShift-extensive/runs/zoobz08j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185031-zoobz08j/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1625 | Val Loss: 1.1598
2025-03-26 18:51:41,545 - wandb.wandb_agent - INFO - Cleaning up finished run: zoobz08j
2025-03-26 18:51:42,442 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:51:42,442 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:51:42,445 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 18:51:47,457 - wandb.wandb_agent - INFO - Running runs: ['ccoaki09']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185148-ccoaki09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-704
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ccoaki09
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ccoaki09
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7920 | Val Loss: 1.6060
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5249 | Val Loss: 1.3819
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3098 | Val Loss: 1.2310
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1753 | Val Loss: 1.1218
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1204 | Val Loss: 1.1300
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1448 | Val Loss: 1.1600
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1729 | Val Loss: 1.1872
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1952 | Val Loss: 1.2038
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2087 | Val Loss: 1.2126
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21393
wandb:   val_loss 1.21322
wandb: 
wandb: üöÄ View run sleek-sweep-704 at: https://wandb.ai/7shoe/domShift-extensive/runs/ccoaki09
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185148-ccoaki09/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2139 | Val Loss: 1.2132
2025-03-26 18:52:22,916 - wandb.wandb_agent - INFO - Cleaning up finished run: ccoaki09
2025-03-26 18:52:23,464 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:52:23,464 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:52:23,467 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:52:28,479 - wandb.wandb_agent - INFO - Running runs: ['lrg0199d']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185228-lrg0199d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-707
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lrg0199d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lrg0199d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.2787 | Val Loss: 1.9924
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.9870 | Val Loss: 2.0236
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0532 | Val Loss: 2.0738
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0767 | Val Loss: 2.0783
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0734 | Val Loss: 2.0647
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0692 | Val Loss: 2.0752
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0780 | Val Loss: 2.0795
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0797 | Val Loss: 2.0804
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0799 | Val Loss: 2.0791
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.330 MB uploadedwandb: | 137.311 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.07739
wandb:   val_loss 2.0756
wandb: 
wandb: üöÄ View run youthful-sweep-707 at: https://wandb.ai/7shoe/domShift-extensive/runs/lrg0199d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185228-lrg0199d/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.0774 | Val Loss: 2.0756
2025-03-26 18:53:14,107 - wandb.wandb_agent - INFO - Cleaning up finished run: lrg0199d
2025-03-26 18:53:14,738 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:53:14,738 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:53:14,740 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:53:19,752 - wandb.wandb_agent - INFO - Running runs: ['rov4f3yw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185320-rov4f3yw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-712
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rov4f3yw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: rov4f3yw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8297 | Val Loss: 1.5570
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4052 | Val Loss: 1.3300
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3323 | Val Loss: 1.3383
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3521 | Val Loss: 1.3661
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3779 | Val Loss: 1.3880
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3992 | Val Loss: 1.4128
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4267 | Val Loss: 1.4388
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4479 | Val Loss: 1.4565
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4645 | Val Loss: 1.4723
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4798
wandb:   val_loss 1.4873
wandb: 
wandb: üöÄ View run flowing-sweep-712 at: https://wandb.ai/7shoe/domShift-extensive/runs/rov4f3yw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185320-rov4f3yw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4798 | Val Loss: 1.4873
2025-03-26 18:54:15,549 - wandb.wandb_agent - INFO - Cleaning up finished run: rov4f3yw
2025-03-26 18:54:16,073 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:54:16,074 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:54:16,076 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:54:21,088 - wandb.wandb_agent - INFO - Running runs: ['osc1ij1w']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185422-osc1ij1w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-720
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/osc1ij1w
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: osc1ij1w
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4280 | Val Loss: 1.4009
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5167 | Val Loss: 1.6457
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5946 | Val Loss: 1.5590
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5918 | Val Loss: 1.6639
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6770 | Val Loss: 1.6452
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5055 | Val Loss: 1.4602
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5267 | Val Loss: 1.5049
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4735 | Val Loss: 1.4238
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3749 | Val Loss: 1.3805
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñÇ‚ñà‚ñÖ‚ñà‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4306
wandb:   val_loss 1.5163
wandb: 
wandb: üöÄ View run peach-sweep-720 at: https://wandb.ai/7shoe/domShift-extensive/runs/osc1ij1w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185422-osc1ij1w/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4306 | Val Loss: 1.5163
2025-03-26 18:55:01,659 - wandb.wandb_agent - INFO - Cleaning up finished run: osc1ij1w
2025-03-26 18:55:02,305 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:55:02,305 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:55:02,308 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 18:55:07,320 - wandb.wandb_agent - INFO - Running runs: ['mfnihd7b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185507-mfnihd7b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-724
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mfnihd7b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: mfnihd7b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5672 | Val Loss: 1.6214
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6281 | Val Loss: 1.6310
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6019 | Val Loss: 1.4976
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5052 | Val Loss: 1.4910
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4502 | Val Loss: 1.4262
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4542 | Val Loss: 1.4938
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4765 | Val Loss: 1.4675
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4758 | Val Loss: 1.4482
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4079 | Val Loss: 1.3812
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33107
wandb:   val_loss 1.31275
wandb: 
wandb: üöÄ View run stoic-sweep-724 at: https://wandb.ai/7shoe/domShift-extensive/runs/mfnihd7b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185507-mfnihd7b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3311 | Val Loss: 1.3128
2025-03-26 18:55:52,926 - wandb.wandb_agent - INFO - Cleaning up finished run: mfnihd7b
2025-03-26 18:56:12,995 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 18:56:17,392 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:56:17,392 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.8
2025-03-26 18:56:17,395 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.8
2025-03-26 18:56:22,407 - wandb.wandb_agent - INFO - Running runs: ['gh2002jo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185623-gh2002jo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-729
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gh2002jo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gh2002jo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3790 | Val Loss: 1.2125
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2930 | Val Loss: 1.2325
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2198 | Val Loss: 1.2482
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2962 | Val Loss: 1.3635
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3389 | Val Loss: 1.3187
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3516 | Val Loss: 1.3519
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3648 | Val Loss: 1.3841
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3897 | Val Loss: 1.3767
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3100 | Val Loss: 1.2692
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27348
wandb:   val_loss 1.26138
wandb: 
wandb: üöÄ View run swept-sweep-729 at: https://wandb.ai/7shoe/domShift-extensive/runs/gh2002jo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185623-gh2002jo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2735 | Val Loss: 1.2614
2025-03-26 18:57:08,120 - wandb.wandb_agent - INFO - Cleaning up finished run: gh2002jo
2025-03-26 18:57:08,716 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:08,716 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:57:08,719 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:57:13,731 - wandb.wandb_agent - INFO - Running runs: ['ppumydir']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185713-ppumydir
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-737
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ppumydir
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ppumydir
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.3915 | Val Loss: 2.7106
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6364 | Val Loss: 2.4549
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4703 | Val Loss: 2.3517
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3842 | Val Loss: 2.3055
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3145 | Val Loss: 2.2494
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2819 | Val Loss: 2.2039
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2505 | Val Loss: 2.1863
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2224 | Val Loss: 2.1563
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1980 | Val Loss: 2.1349
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.18332
wandb:   val_loss 2.12292
wandb: 
wandb: üöÄ View run glad-sweep-737 at: https://wandb.ai/7shoe/domShift-extensive/runs/ppumydir
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185713-ppumydir/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1833 | Val Loss: 2.1229
2025-03-26 18:57:59,369 - wandb.wandb_agent - INFO - Cleaning up finished run: ppumydir
2025-03-26 18:57:59,916 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:59,916 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:57:59,919 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:58:04,931 - wandb.wandb_agent - INFO - Running runs: ['3pn2i0gd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185806-3pn2i0gd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-743
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3pn2i0gd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3pn2i0gd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2945 | Val Loss: 1.0037
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9431 | Val Loss: 0.8827
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9092 | Val Loss: 0.9233
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9236 | Val Loss: 0.9238
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9253 | Val Loss: 0.9263
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9266 | Val Loss: 0.9271
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9273 | Val Loss: 0.9276
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9276 | Val Loss: 0.9278
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9277 | Val Loss: 0.9279
wandb: - 137.083 MB of 137.242 MB uploadedwandb: \ 137.083 MB of 137.242 MB uploadedwandb: | 137.242 MB of 137.242 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92783
wandb:   val_loss 0.92794
wandb: 
wandb: üöÄ View run efficient-sweep-743 at: https://wandb.ai/7shoe/domShift-extensive/runs/3pn2i0gd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185806-3pn2i0gd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9278 | Val Loss: 0.9279
2025-03-26 18:59:05,780 - wandb.wandb_agent - INFO - Cleaning up finished run: 3pn2i0gd
2025-03-26 18:59:06,318 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:59:06,318 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:59:06,321 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:59:11,333 - wandb.wandb_agent - INFO - Running runs: ['7hitzvuw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185912-7hitzvuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-750
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7hitzvuw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7hitzvuw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.4607 | Val Loss: 2.0814
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.0594 | Val Loss: 2.1488
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.3601 | Val Loss: 2.5670
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7198 | Val Loss: 2.8399
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.9593 | Val Loss: 3.0776
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.1626 | Val Loss: 3.2321
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 3.2748 | Val Loss: 3.3105
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 3.3374 | Val Loss: 3.3597
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 3.3686 | Val Loss: 3.3645
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 33.060 MB uploadedwandb: | 32.968 MB of 33.060 MB uploadedwandb: / 33.060 MB of 33.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.3407
wandb:   val_loss 3.30746
wandb: 
wandb: üöÄ View run deep-sweep-750 at: https://wandb.ai/7shoe/domShift-extensive/runs/7hitzvuw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185912-7hitzvuw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.3407 | Val Loss: 3.3075
2025-03-26 19:00:02,031 - wandb.wandb_agent - INFO - Cleaning up finished run: 7hitzvuw
2025-03-26 19:00:02,544 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:02,544 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:00:02,547 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 19:00:07,559 - wandb.wandb_agent - INFO - Running runs: ['lzmjgw21']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190008-lzmjgw21
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-755
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lzmjgw21
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: lzmjgw21
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.3146 | Val Loss: 2.5373
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.5388 | Val Loss: 2.6527
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.7543 | Val Loss: 2.8243
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.8807 | Val Loss: 2.9275
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.9663 | Val Loss: 3.0209
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 3.1094 | Val Loss: 3.2074
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 3.2930 | Val Loss: 3.3757
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 3.4437 | Val Loss: 3.5072
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 3.5563 | Val Loss: 3.6009
wandb: - 137.025 MB of 137.025 MB uploadedwandb: \ 137.025 MB of 137.025 MB uploadedwandb: | 137.045 MB of 137.045 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.62343
wandb:   val_loss 3.63183
wandb: 
wandb: üöÄ View run lively-sweep-755 at: https://wandb.ai/7shoe/domShift-extensive/runs/lzmjgw21
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190008-lzmjgw21/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 3.6234 | Val Loss: 3.6318
2025-03-26 19:00:58,272 - wandb.wandb_agent - INFO - Cleaning up finished run: lzmjgw21
2025-03-26 19:00:59,065 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:59,065 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:00:59,068 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:01:04,080 - wandb.wandb_agent - INFO - Running runs: ['xn2lko1i']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190105-xn2lko1i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-761
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xn2lko1i
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xn2lko1i
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.4601 | Val Loss: 2.2680
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0502 | Val Loss: 2.0638
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2463 | Val Loss: 2.3952
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4660 | Val Loss: 2.5299
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5590 | Val Loss: 2.5776
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5651 | Val Loss: 2.5579
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5461 | Val Loss: 2.5412
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5318 | Val Loss: 2.5301
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5232 | Val Loss: 2.5228
wandb: - 43.717 MB of 43.717 MB uploadedwandb: \ 43.717 MB of 43.717 MB uploadedwandb: | 43.736 MB of 43.736 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.51569
wandb:   val_loss 2.51395
wandb: 
wandb: üöÄ View run dandy-sweep-761 at: https://wandb.ai/7shoe/domShift-extensive/runs/xn2lko1i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190105-xn2lko1i/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5157 | Val Loss: 2.5139
2025-03-26 19:01:44,639 - wandb.wandb_agent - INFO - Cleaning up finished run: xn2lko1i
2025-03-26 19:01:45,250 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:01:45,250 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:01:45,253 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 19:01:50,265 - wandb.wandb_agent - INFO - Running runs: ['f4wi0inj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190150-f4wi0inj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-767
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f4wi0inj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: f4wi0inj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 5.4779 | Val Loss: 5.0346
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 5.0647 | Val Loss: 4.7637
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 4.8431 | Val Loss: 4.7109
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 4.6968 | Val Loss: 4.4285
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 4.5994 | Val Loss: 4.3727
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 4.5228 | Val Loss: 4.3450
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 4.4980 | Val Loss: 4.3269
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 4.4850 | Val Loss: 4.3017
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 4.4824 | Val Loss: 4.3078
wandb: - 32.854 MB of 32.854 MB uploadedwandb: \ 32.854 MB of 32.854 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.43448
wandb:   val_loss 4.33484
wandb: 
wandb: üöÄ View run exalted-sweep-767 at: https://wandb.ai/7shoe/domShift-extensive/runs/f4wi0inj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190150-f4wi0inj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 4.4345 | Val Loss: 4.3348
2025-03-26 19:02:25,727 - wandb.wandb_agent - INFO - Cleaning up finished run: f4wi0inj
2025-03-26 19:02:26,538 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:02:26,538 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:02:26,541 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:02:31,553 - wandb.wandb_agent - INFO - Running runs: ['ysl5454i']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190231-ysl5454i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-773
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ysl5454i
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ysl5454i
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6434 | Val Loss: 1.4005
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3564 | Val Loss: 1.2348
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2364 | Val Loss: 1.2581
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2709 | Val Loss: 1.2891
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3259 | Val Loss: 1.3519
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3678 | Val Loss: 1.3778
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3870 | Val Loss: 1.3972
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4068 | Val Loss: 1.4170
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4263 | Val Loss: 1.4366
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.44739
wandb:   val_loss 1.46054
wandb: 
wandb: üöÄ View run twilight-sweep-773 at: https://wandb.ai/7shoe/domShift-extensive/runs/ysl5454i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190231-ysl5454i/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4474 | Val Loss: 1.4605
2025-03-26 19:03:22,272 - wandb.wandb_agent - INFO - Cleaning up finished run: ysl5454i
2025-03-26 19:03:23,184 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:03:23,184 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:03:23,187 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:03:28,200 - wandb.wandb_agent - INFO - Running runs: ['aw75kuc1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190328-aw75kuc1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-778
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/aw75kuc1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: aw75kuc1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8345 | Val Loss: 2.4083
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3155 | Val Loss: 2.2305
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1962 | Val Loss: 2.1765
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1611 | Val Loss: 2.1383
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1142 | Val Loss: 2.0351
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.9646 | Val Loss: 1.9198
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8614 | Val Loss: 1.7382
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6782 | Val Loss: 1.6309
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6019 | Val Loss: 1.5781
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.196 MB of 137.196 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.56655
wandb:   val_loss 1.55822
wandb: 
wandb: üöÄ View run helpful-sweep-778 at: https://wandb.ai/7shoe/domShift-extensive/runs/aw75kuc1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190328-aw75kuc1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5666 | Val Loss: 1.5582
2025-03-26 19:04:39,167 - wandb.wandb_agent - INFO - Cleaning up finished run: aw75kuc1
2025-03-26 19:04:39,844 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:04:39,844 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:04:39,847 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:04:44,859 - wandb.wandb_agent - INFO - Running runs: ['g1ysgkym']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190444-g1ysgkym
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-786
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/g1ysgkym
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: g1ysgkym
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2444 | Val Loss: 1.0435
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0612 | Val Loss: 1.1252
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1585 | Val Loss: 1.1844
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2019 | Val Loss: 1.2180
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2301 | Val Loss: 1.2422
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2524 | Val Loss: 1.2629
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2724 | Val Loss: 1.2830
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2976 | Val Loss: 1.3116
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3228 | Val Loss: 1.3367
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3591
wandb:   val_loss 1.39083
wandb: 
wandb: üöÄ View run legendary-sweep-786 at: https://wandb.ai/7shoe/domShift-extensive/runs/g1ysgkym
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190444-g1ysgkym/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3591 | Val Loss: 1.3908
2025-03-26 19:05:55,863 - wandb.wandb_agent - INFO - Cleaning up finished run: g1ysgkym
2025-03-26 19:05:56,846 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:05:56,847 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 19:05:56,849 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 19:06:01,862 - wandb.wandb_agent - INFO - Running runs: ['9xbtuzd4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190603-9xbtuzd4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-795
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9xbtuzd4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9xbtuzd4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4451 | Val Loss: 1.2993
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0486 | Val Loss: 0.9837
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9893 | Val Loss: 0.9949
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0083 | Val Loss: 1.0139
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0156 | Val Loss: 1.0147
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0153 | Val Loss: 1.0143
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0152 | Val Loss: 1.0142
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0151 | Val Loss: 1.0141
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0151 | Val Loss: 1.0141
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.473 MB uploadedwandb: | 137.314 MB of 137.473 MB uploadedwandb: / 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.01508
wandb:   val_loss 1.01406
wandb: 
wandb: üöÄ View run decent-sweep-795 at: https://wandb.ai/7shoe/domShift-extensive/runs/9xbtuzd4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190603-9xbtuzd4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0151 | Val Loss: 1.0141
2025-03-26 19:07:12,861 - wandb.wandb_agent - INFO - Cleaning up finished run: 9xbtuzd4
2025-03-26 19:07:13,411 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:07:13,411 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.9
2025-03-26 19:07:13,414 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.9
2025-03-26 19:07:18,426 - wandb.wandb_agent - INFO - Running runs: ['siimpefl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190719-siimpefl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-802
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/siimpefl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: siimpefl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.4782 | Val Loss: 2.0820
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3279 | Val Loss: 2.5126
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5102 | Val Loss: 2.4976
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4996 | Val Loss: 2.4665
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4346 | Val Loss: 2.4127
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4148 | Val Loss: 2.4253
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4378 | Val Loss: 2.4468
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4523 | Val Loss: 2.4585
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4646 | Val Loss: 2.4709
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.178 MB of 137.190 MB uploadedwandb: | 137.178 MB of 137.190 MB uploadedwandb: / 137.178 MB of 137.190 MB uploadedwandb: - 137.178 MB of 137.190 MB uploadedwandb: \ 137.178 MB of 137.190 MB uploadedwandb: | 137.190 MB of 137.190 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÅ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá
wandb:   val_loss ‚ñÅ‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.47512
wandb:   val_loss 2.47774
wandb: 
wandb: üöÄ View run serene-sweep-802 at: https://wandb.ai/7shoe/domShift-extensive/runs/siimpefl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190719-siimpefl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4751 | Val Loss: 2.4777
2025-03-26 19:08:34,498 - wandb.wandb_agent - INFO - Cleaning up finished run: siimpefl
2025-03-26 19:08:35,230 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:08:35,230 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 19:08:35,233 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 19:08:40,245 - wandb.wandb_agent - INFO - Running runs: ['ubkncanz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190841-ubkncanz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-810
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ubkncanz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ubkncanz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.0093 | Val Loss: 1.5134
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5923 | Val Loss: 1.6457
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6496 | Val Loss: 1.6407
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6238 | Val Loss: 1.6062
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5936 | Val Loss: 1.5824
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5772 | Val Loss: 1.5744
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5755 | Val Loss: 1.5775
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5813 | Val Loss: 1.5856
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.5912 | Val Loss: 1.5977
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 33.061 MB of 33.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.60466
wandb:   val_loss 1.6118
wandb: 
wandb: üöÄ View run comfy-sweep-810 at: https://wandb.ai/7shoe/domShift-extensive/runs/ubkncanz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190841-ubkncanz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.6047 | Val Loss: 1.6118
2025-03-26 19:09:30,919 - wandb.wandb_agent - INFO - Cleaning up finished run: ubkncanz
2025-03-26 19:09:31,531 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:09:31,532 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.9
2025-03-26 19:09:31,535 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.9
2025-03-26 19:09:36,547 - wandb.wandb_agent - INFO - Running runs: ['08y3vr97']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190937-08y3vr97
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-815
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/08y3vr97
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 08y3vr97
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.5917 | Val Loss: 3.3557
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.3401 | Val Loss: 3.2260
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.0974 | Val Loss: 2.9421
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.8783 | Val Loss: 2.8182
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.7970 | Val Loss: 2.7697
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.7583 | Val Loss: 2.7405
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.7345 | Val Loss: 2.7244
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.7228 | Val Loss: 2.7181
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.7191 | Val Loss: 2.7174
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 32.987 MB of 32.987 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.72008
wandb:   val_loss 2.72089
wandb: 
wandb: üöÄ View run amber-sweep-815 at: https://wandb.ai/7shoe/domShift-extensive/runs/08y3vr97
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190937-08y3vr97/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.7201 | Val Loss: 2.7209
2025-03-26 19:10:12,010 - wandb.wandb_agent - INFO - Cleaning up finished run: 08y3vr97
2025-03-26 19:10:12,456 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:12,456 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:10:12,458 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:10:17,470 - wandb.wandb_agent - INFO - Running runs: ['vghy8wrp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191017-vghy8wrp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-820
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vghy8wrp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vghy8wrp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1626 | Val Loss: 0.9152
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.7910 | Val Loss: 0.7452
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.7431 | Val Loss: 0.7474
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.7423 | Val Loss: 0.7419
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.7379 | Val Loss: 0.7377
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.7365 | Val Loss: 0.7377
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.7355 | Val Loss: 0.7369
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.7351 | Val Loss: 0.7370
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.7351 | Val Loss: 0.7369
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.73507
wandb:   val_loss 0.73686
wandb: 
wandb: üöÄ View run copper-sweep-820 at: https://wandb.ai/7shoe/domShift-extensive/runs/vghy8wrp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191017-vghy8wrp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.7351 | Val Loss: 0.7369
2025-03-26 19:10:53,007 - wandb.wandb_agent - INFO - Cleaning up finished run: vghy8wrp
2025-03-26 19:10:53,850 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:53,850 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:10:53,857 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:10:58,869 - wandb.wandb_agent - INFO - Running runs: ['0eaba0xd']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191058-0eaba0xd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-825
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0eaba0xd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0eaba0xd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7587 | Val Loss: 1.5488
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4111 | Val Loss: 1.3442
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2697 | Val Loss: 1.2216
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1944 | Val Loss: 1.1777
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1675 | Val Loss: 1.1555
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1509 | Val Loss: 1.1469
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1494 | Val Loss: 1.1667
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1680 | Val Loss: 1.1648
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1602 | Val Loss: 1.1585
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15848
wandb:   val_loss 1.15646
wandb: 
wandb: üöÄ View run fallen-sweep-825 at: https://wandb.ai/7shoe/domShift-extensive/runs/0eaba0xd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191058-0eaba0xd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1585 | Val Loss: 1.1565
2025-03-26 19:11:49,629 - wandb.wandb_agent - INFO - Cleaning up finished run: 0eaba0xd
2025-03-26 19:11:50,223 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:11:50,223 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.5
2025-03-26 19:11:50,225 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.5
2025-03-26 19:11:55,237 - wandb.wandb_agent - INFO - Running runs: ['vxzas3g9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191156-vxzas3g9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-830
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vxzas3g9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vxzas3g9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.6508 | Val Loss: 2.3626
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.2510 | Val Loss: 2.1079
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.0338 | Val Loss: 1.9757
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.9378 | Val Loss: 1.9004
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.8755 | Val Loss: 1.8498
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.7124 | Val Loss: 1.5733
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5741 | Val Loss: 1.5795
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5759 | Val Loss: 1.5690
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.5613 | Val Loss: 1.5506
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 32.987 MB of 32.987 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.54397
wandb:   val_loss 1.53659
wandb: 
wandb: üöÄ View run bumbling-sweep-830 at: https://wandb.ai/7shoe/domShift-extensive/runs/vxzas3g9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191156-vxzas3g9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.5440 | Val Loss: 1.5366
2025-03-26 19:12:30,697 - wandb.wandb_agent - INFO - Cleaning up finished run: vxzas3g9
2025-03-26 19:12:31,488 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:12:31,488 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:12:31,492 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:12:36,505 - wandb.wandb_agent - INFO - Running runs: ['3p6kxd9e']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191236-3p6kxd9e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-836
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3p6kxd9e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3p6kxd9e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4816 | Val Loss: 1.4371
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4705 | Val Loss: 1.6407
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6958 | Val Loss: 1.5066
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4326 | Val Loss: 1.4682
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4767 | Val Loss: 1.4757
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4729 | Val Loss: 1.4722
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4769 | Val Loss: 1.4824
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4879 | Val Loss: 1.4934
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4973 | Val Loss: 1.5014
wandb: - 32.995 MB of 33.088 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.50534
wandb:   val_loss 1.50897
wandb: 
wandb: üöÄ View run absurd-sweep-836 at: https://wandb.ai/7shoe/domShift-extensive/runs/3p6kxd9e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191236-3p6kxd9e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.5053 | Val Loss: 1.5090
2025-03-26 19:13:22,136 - wandb.wandb_agent - INFO - Cleaning up finished run: 3p6kxd9e
2025-03-26 19:13:22,775 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:13:22,775 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:13:22,778 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 19:13:27,791 - wandb.wandb_agent - INFO - Running runs: ['jpumur8e']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191328-jpumur8e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-841
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jpumur8e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jpumur8e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2870 | Val Loss: 2.9398
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.9413 | Val Loss: 2.9405
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8668 | Val Loss: 2.8085
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7858 | Val Loss: 2.7556
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.6540 | Val Loss: 2.5264
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.4442 | Val Loss: 2.3788
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.3478 | Val Loss: 2.3228
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.3105 | Val Loss: 2.3020
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3036 | Val Loss: 2.3107
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.32685
wandb:   val_loss 2.34641
wandb: 
wandb: üöÄ View run wandering-sweep-841 at: https://wandb.ai/7shoe/domShift-extensive/runs/jpumur8e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191328-jpumur8e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.3268 | Val Loss: 2.3464
2025-03-26 19:14:08,419 - wandb.wandb_agent - INFO - Cleaning up finished run: jpumur8e
2025-03-26 19:14:09,893 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:14:09,893 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:14:09,896 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.4
2025-03-26 19:14:14,908 - wandb.wandb_agent - INFO - Running runs: ['oatlrbwc']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191415-oatlrbwc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-846
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/oatlrbwc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: oatlrbwc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0601 | Val Loss: 1.9545
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7672 | Val Loss: 1.8014
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7462 | Val Loss: 1.7809
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.7658 | Val Loss: 1.7953
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.7888 | Val Loss: 1.8095
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8092 | Val Loss: 1.8333
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8356 | Val Loss: 1.8559
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.8592 | Val Loss: 1.8824
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.8813 | Val Loss: 1.8894
wandb: - 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.88414
wandb:   val_loss 1.89154
wandb: 
wandb: üöÄ View run silvery-sweep-846 at: https://wandb.ai/7shoe/domShift-extensive/runs/oatlrbwc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191415-oatlrbwc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.8841 | Val Loss: 1.8915
2025-03-26 19:14:40,241 - wandb.wandb_agent - INFO - Cleaning up finished run: oatlrbwc
2025-03-26 19:14:40,880 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:14:40,881 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:14:40,884 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:14:45,896 - wandb.wandb_agent - INFO - Running runs: ['198ph2ij']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191447-198ph2ij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-848
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/198ph2ij
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 198ph2ij
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.6557 | Val Loss: 2.1922
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.8480 | Val Loss: 1.5745
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4730 | Val Loss: 1.3911
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3670 | Val Loss: 1.3537
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3579 | Val Loss: 1.3657
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3854 | Val Loss: 1.4108
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4438 | Val Loss: 1.4826
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5252 | Val Loss: 1.5717
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.6168 | Val Loss: 1.6639
wandb: - 32.851 MB of 32.851 MB uploadedwandb: \ 32.851 MB of 32.851 MB uploadedwandb: | 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.70723
wandb:   val_loss 1.7518
wandb: 
wandb: üöÄ View run vocal-sweep-848 at: https://wandb.ai/7shoe/domShift-extensive/runs/198ph2ij
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191447-198ph2ij/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.7072 | Val Loss: 1.7518
2025-03-26 19:15:21,466 - wandb.wandb_agent - INFO - Cleaning up finished run: 198ph2ij
2025-03-26 19:15:22,377 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:15:22,377 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:15:22,380 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 19:15:27,392 - wandb.wandb_agent - INFO - Running runs: ['8wkqrxkb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191528-8wkqrxkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-853
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8wkqrxkb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 8wkqrxkb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.1762 | Val Loss: 1.2326
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0837 | Val Loss: 0.9943
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9328 | Val Loss: 0.9052
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9016 | Val Loss: 0.9007
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9024 | Val Loss: 0.9029
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9025 | Val Loss: 0.9022
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9025 | Val Loss: 0.9025
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9025 | Val Loss: 0.9025
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9025 | Val Loss: 0.9024
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.90253
wandb:   val_loss 0.90252
wandb: 
wandb: üöÄ View run worldly-sweep-853 at: https://wandb.ai/7shoe/domShift-extensive/runs/8wkqrxkb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191528-8wkqrxkb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9025 | Val Loss: 0.9025
2025-03-26 19:16:18,116 - wandb.wandb_agent - INFO - Cleaning up finished run: 8wkqrxkb
2025-03-26 19:16:18,882 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:16:18,883 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:16:18,885 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:16:23,899 - wandb.wandb_agent - INFO - Running runs: ['gq3zz268']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191624-gq3zz268
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-858
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gq3zz268
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gq3zz268
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.4619 | Val Loss: 1.3516
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.4014 | Val Loss: 1.4326
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.4384 | Val Loss: 1.4450
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.4494 | Val Loss: 1.4544
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.4548 | Val Loss: 1.4550
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.4544 | Val Loss: 1.4538
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.4529 | Val Loss: 1.4523
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.4516 | Val Loss: 1.4513
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.4509 | Val Loss: 1.4509
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.45113
wandb:   val_loss 1.4515
wandb: 
wandb: üöÄ View run serene-sweep-858 at: https://wandb.ai/7shoe/domShift-extensive/runs/gq3zz268
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191624-gq3zz268/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.4511 | Val Loss: 1.4515
2025-03-26 19:17:39,971 - wandb.wandb_agent - INFO - Cleaning up finished run: gq3zz268
2025-03-26 19:17:41,106 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:17:41,106 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:17:41,109 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
2025-03-26 19:17:46,121 - wandb.wandb_agent - INFO - Running runs: ['w8nz0t41']
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191746-w8nz0t41
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-867
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/w8nz0t41
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: w8nz0t41
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5081 | Val Loss: 1.3006
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2613 | Val Loss: 1.2114
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2041 | Val Loss: 1.2026
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2048 | Val Loss: 1.2099
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2206 | Val Loss: 1.2324
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2446 | Val Loss: 1.2559
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2691 | Val Loss: 1.2941
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2190 | Val Loss: 1.0938
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1061 | Val Loss: 1.1431
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.18433
wandb:   val_loss 1.22289
wandb: 
wandb: üöÄ View run vocal-sweep-867 at: https://wandb.ai/7shoe/domShift-extensive/runs/w8nz0t41
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191746-w8nz0t41/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1843 | Val Loss: 1.2229
2025-03-26 19:18:31,766 - wandb.wandb_agent - INFO - Cleaning up finished run: w8nz0t41
2025-03-26 19:18:32,353 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:18:32,354 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:18:32,358 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 19:18:37,371 - wandb.wandb_agent - INFO - Running runs: ['14v47hm4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191837-14v47hm4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-872
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/14v47hm4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 14v47hm4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7578 | Val Loss: 1.6256
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5290 | Val Loss: 1.3429
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2839 | Val Loss: 1.2621
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2814 | Val Loss: 1.3252
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3685 | Val Loss: 1.4200
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4369 | Val Loss: 1.4339
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4429 | Val Loss: 1.4782
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4518 | Val Loss: 1.3602
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2724 | Val Loss: 1.1911
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17921
wandb:   val_loss 1.17244
wandb: 
wandb: üöÄ View run avid-sweep-872 at: https://wandb.ai/7shoe/domShift-extensive/runs/14v47hm4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191837-14v47hm4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1792 | Val Loss: 1.1724
2025-03-26 19:19:12,871 - wandb.wandb_agent - INFO - Cleaning up finished run: 14v47hm4
2025-03-26 19:19:13,429 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:13,429 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:19:13,432 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:19:18,440 - wandb.wandb_agent - INFO - Running runs: ['tggtyeee']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191918-tggtyeee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-878
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tggtyeee
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tggtyeee
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1557 | Val Loss: 1.1297
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1533 | Val Loss: 1.1254
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1315 | Val Loss: 1.1566
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1869 | Val Loss: 1.2018
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1907 | Val Loss: 1.1834
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1931 | Val Loss: 1.2089
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2159 | Val Loss: 1.2187
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2342 | Val Loss: 1.2662
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3462 | Val Loss: 1.4810
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñà
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.56109
wandb:   val_loss 1.57001
wandb: 
wandb: üöÄ View run polar-sweep-878 at: https://wandb.ai/7shoe/domShift-extensive/runs/tggtyeee
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191918-tggtyeee/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5611 | Val Loss: 1.5700
2025-03-26 19:20:34,596 - wandb.wandb_agent - INFO - Cleaning up finished run: tggtyeee
2025-03-26 19:20:35,638 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:20:35,638 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:20:35,641 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:20:40,653 - wandb.wandb_agent - INFO - Running runs: ['pbw2jg2h']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_192040-pbw2jg2h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-885
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pbw2jg2h
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pbw2jg2h
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5739 | Val Loss: 1.4772
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4400 | Val Loss: 1.3577
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2691 | Val Loss: 1.2176
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1632 | Val Loss: 1.1568
