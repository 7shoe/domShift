nohup: ignoring input
wandb: Starting wandb agent üïµÔ∏è
2025-03-26 17:04:15,825 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 17:04:16,023 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:04:16,024 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:04:16,027 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 17:04:21,039 - wandb.wandb_agent - INFO - Running runs: ['y8h00s74']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170422-y8h00s74
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y8h00s74
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: y8h00s74
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 5.4777 | Val Loss: 4.9255
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 4.9293 | Val Loss: 4.5700
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 4.7655 | Val Loss: 4.4821
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 4.6687 | Val Loss: 4.3932
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 4.6042 | Val Loss: 4.3320
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 4.5326 | Val Loss: 4.2750
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 4.5017 | Val Loss: 4.2221
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 4.4591 | Val Loss: 4.2007
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 4.4432 | Val Loss: 4.1914
wandb: - 32.849 MB of 32.849 MB uploadedwandb: \ 32.849 MB of 32.849 MB uploadedwandb: | 32.942 MB of 32.942 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.41962
wandb:   val_loss 4.16122
wandb: 
wandb: üöÄ View run different-sweep-1 at: https://wandb.ai/7shoe/domShift-extensive/runs/y8h00s74
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170422-y8h00s74/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 4.4196 | Val Loss: 4.1612
2025-03-26 17:05:16,829 - wandb.wandb_agent - INFO - Cleaning up finished run: y8h00s74
2025-03-26 17:05:17,318 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:17,319 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:05:17,321 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.4
2025-03-26 17:05:22,334 - wandb.wandb_agent - INFO - Running runs: ['4puo2apr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170523-4puo2apr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4puo2apr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4puo2apr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7895 | Val Loss: 1.7487
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.7134 | Val Loss: 1.6336
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6026 | Val Loss: 1.5575
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5450 | Val Loss: 1.5308
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5231 | Val Loss: 1.5085
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5050 | Val Loss: 1.4841
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4711 | Val Loss: 1.4419
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4253 | Val Loss: 1.3934
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3772 | Val Loss: 1.3473
wandb: - 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33651
wandb:   val_loss 1.31931
wandb: 
wandb: üöÄ View run magic-sweep-9 at: https://wandb.ai/7shoe/domShift-extensive/runs/4puo2apr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170523-4puo2apr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3365 | Val Loss: 1.3193
2025-03-26 17:05:47,681 - wandb.wandb_agent - INFO - Cleaning up finished run: 4puo2apr
2025-03-26 17:05:48,212 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:48,212 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:05:48,215 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:05:53,227 - wandb.wandb_agent - INFO - Running runs: ['tfjtoynt']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170553-tfjtoynt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tfjtoynt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tfjtoynt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.8505 | Val Loss: 4.4214
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 4.2235 | Val Loss: 3.9918
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.9509 | Val Loss: 3.8301
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 3.7927 | Val Loss: 3.6928
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 3.7029 | Val Loss: 3.6159
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.6183 | Val Loss: 3.5499
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 3.5679 | Val Loss: 3.5154
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 3.5284 | Val Loss: 3.4786
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 3.4990 | Val Loss: 3.4615
wandb: - 32.989 MB of 32.989 MB uploadedwandb: \ 32.989 MB of 32.989 MB uploadedwandb: | 33.009 MB of 33.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.47653
wandb:   val_loss 3.43627
wandb: 
wandb: üöÄ View run soft-sweep-14 at: https://wandb.ai/7shoe/domShift-extensive/runs/tfjtoynt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170553-tfjtoynt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.4765 | Val Loss: 3.4363
2025-03-26 17:06:23,640 - wandb.wandb_agent - INFO - Cleaning up finished run: tfjtoynt
2025-03-26 17:06:24,018 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:06:24,018 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:06:24,021 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:06:29,033 - wandb.wandb_agent - INFO - Running runs: ['ern3tjmt']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170629-ern3tjmt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ern3tjmt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ern3tjmt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.8573 | Val Loss: 3.8278
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.6665 | Val Loss: 3.2290
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.2932 | Val Loss: 2.9244
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 3.0682 | Val Loss: 2.6939
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8591 | Val Loss: 2.5480
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.7378 | Val Loss: 2.4476
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.6339 | Val Loss: 2.3403
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.5185 | Val Loss: 2.2074
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3964 | Val Loss: 2.1690
wandb: - 32.854 MB of 32.854 MB uploadedwandb: \ 32.854 MB of 32.854 MB uploadedwandb: | 32.947 MB of 32.947 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.29594
wandb:   val_loss 2.04788
wandb: 
wandb: üöÄ View run denim-sweep-19 at: https://wandb.ai/7shoe/domShift-extensive/runs/ern3tjmt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170629-ern3tjmt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.2959 | Val Loss: 2.0479
2025-03-26 17:07:14,717 - wandb.wandb_agent - INFO - Cleaning up finished run: ern3tjmt
2025-03-26 17:07:15,098 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:15,098 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:07:15,101 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:07:20,114 - wandb.wandb_agent - INFO - Running runs: ['0e0sxkak']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170721-0e0sxkak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0e0sxkak
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0e0sxkak
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5814 | Val Loss: 1.3987
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3976 | Val Loss: 1.4071
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4163 | Val Loss: 1.4050
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3926 | Val Loss: 1.3695
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3519 | Val Loss: 1.3189
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2945 | Val Loss: 1.2549
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2178 | Val Loss: 1.1622
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1310 | Val Loss: 1.0954
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0775 | Val Loss: 1.0494
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03447
wandb:   val_loss 1.01491
wandb: 
wandb: üöÄ View run distinctive-sweep-28 at: https://wandb.ai/7shoe/domShift-extensive/runs/0e0sxkak
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170721-0e0sxkak/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0345 | Val Loss: 1.0149
2025-03-26 17:07:55,603 - wandb.wandb_agent - INFO - Cleaning up finished run: 0e0sxkak
2025-03-26 17:07:56,238 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:56,238 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:07:56,240 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:08:01,253 - wandb.wandb_agent - INFO - Running runs: ['bq00bfqg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170802-bq00bfqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bq00bfqg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: bq00bfqg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7516 | Val Loss: 1.4789
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3861 | Val Loss: 1.2988
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2942 | Val Loss: 1.3038
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3165 | Val Loss: 1.3254
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3477 | Val Loss: 1.3875
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4324 | Val Loss: 1.4934
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5377 | Val Loss: 1.5937
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.6257 | Val Loss: 1.6569
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.6733 | Val Loss: 1.7263
wandb: - 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.65611
wandb:   val_loss 1.52787
wandb: 
wandb: üöÄ View run effortless-sweep-32 at: https://wandb.ai/7shoe/domShift-extensive/runs/bq00bfqg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170802-bq00bfqg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.6561 | Val Loss: 1.5279
2025-03-26 17:08:26,648 - wandb.wandb_agent - INFO - Cleaning up finished run: bq00bfqg
2025-03-26 17:08:27,453 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:08:27,453 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:08:27,455 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:08:32,467 - wandb.wandb_agent - INFO - Running runs: ['73md87qo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170833-73md87qo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/73md87qo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 73md87qo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2754 | Val Loss: 2.6835
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5808 | Val Loss: 2.4646
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4040 | Val Loss: 2.3221
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2948 | Val Loss: 2.2630
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2676 | Val Loss: 2.2676
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2754 | Val Loss: 2.2754
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2802 | Val Loss: 2.2791
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2857 | Val Loss: 2.2886
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2947 | Val Loss: 2.2961
wandb: - 137.051 MB of 137.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.30016
wandb:   val_loss 2.30106
wandb: 
wandb: üöÄ View run stellar-sweep-37 at: https://wandb.ai/7shoe/domShift-extensive/runs/73md87qo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170833-73md87qo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3002 | Val Loss: 2.3011
2025-03-26 17:09:02,890 - wandb.wandb_agent - INFO - Cleaning up finished run: 73md87qo
2025-03-26 17:09:09,048 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:09,048 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:09:09,051 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:09:14,063 - wandb.wandb_agent - INFO - Running runs: ['2h5hv615']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170914-2h5hv615
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2h5hv615
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2h5hv615
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0912 | Val Loss: 1.9271
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6873 | Val Loss: 1.4407
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3790 | Val Loss: 1.2688
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2238 | Val Loss: 1.1357
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1509 | Val Loss: 1.0963
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1126 | Val Loss: 1.0647
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0656 | Val Loss: 1.0378
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0282 | Val Loss: 0.9910
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0021 | Val Loss: 0.9788
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.98145
wandb:   val_loss 0.95954
wandb: 
wandb: üöÄ View run iconic-sweep-42 at: https://wandb.ai/7shoe/domShift-extensive/runs/2h5hv615
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170914-2h5hv615/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9814 | Val Loss: 0.9595
2025-03-26 17:09:44,466 - wandb.wandb_agent - INFO - Cleaning up finished run: 2h5hv615
2025-03-26 17:09:45,416 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:45,416 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:09:45,419 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:09:50,431 - wandb.wandb_agent - INFO - Running runs: ['y7hdzky1']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170950-y7hdzky1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y7hdzky1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: y7hdzky1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.6563 | Val Loss: 2.4913
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.7051 | Val Loss: 2.8566
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8654 | Val Loss: 2.8497
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.8592 | Val Loss: 2.9222
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 3.0072 | Val Loss: 3.0593
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.0342 | Val Loss: 2.9422
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.5263 | Val Loss: 2.2298
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.1669 | Val Loss: 2.1193
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.0800 | Val Loss: 2.0525
wandb: - 10.261 MB of 10.261 MB uploadedwandb: \ 10.261 MB of 10.261 MB uploadedwandb: | 10.281 MB of 10.281 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.03641
wandb:   val_loss 2.02448
wandb: 
wandb: üöÄ View run solar-sweep-46 at: https://wandb.ai/7shoe/domShift-extensive/runs/y7hdzky1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170950-y7hdzky1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.0364 | Val Loss: 2.0245
2025-03-26 17:10:25,921 - wandb.wandb_agent - INFO - Cleaning up finished run: y7hdzky1
2025-03-26 17:10:29,185 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:10:29,186 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:10:29,188 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:10:34,201 - wandb.wandb_agent - INFO - Running runs: ['ie3dusei']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171035-ie3dusei
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-53
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ie3dusei
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ie3dusei
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.5600 | Val Loss: 3.2757
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7554 | Val Loss: 2.2241
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1330 | Val Loss: 2.1014
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1327 | Val Loss: 2.1514
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1506 | Val Loss: 2.1390
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1209 | Val Loss: 2.0930
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0678 | Val Loss: 2.0400
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0049 | Val Loss: 1.9731
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9573 | Val Loss: 1.9426
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.93552
wandb:   val_loss 1.92833
wandb: 
wandb: üöÄ View run floral-sweep-53 at: https://wandb.ai/7shoe/domShift-extensive/runs/ie3dusei
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171035-ie3dusei/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9355 | Val Loss: 1.9283
2025-03-26 17:11:35,084 - wandb.wandb_agent - INFO - Cleaning up finished run: ie3dusei
2025-03-26 17:11:36,815 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:11:36,815 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:11:36,818 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:11:41,831 - wandb.wandb_agent - INFO - Running runs: ['ny3vy612']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171143-ny3vy612
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ny3vy612
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ny3vy612
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4084 | Val Loss: 1.2905
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2124 | Val Loss: 1.0745
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9901 | Val Loss: 0.9466
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9221 | Val Loss: 0.8906
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8815 | Val Loss: 0.8711
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8698 | Val Loss: 0.8674
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8641 | Val Loss: 0.8455
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8724 | Val Loss: 0.9066
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9241 | Val Loss: 0.9435
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92229
wandb:   val_loss 0.90256
wandb: 
wandb: üöÄ View run jolly-sweep-60 at: https://wandb.ai/7shoe/domShift-extensive/runs/ny3vy612
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171143-ny3vy612/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9223 | Val Loss: 0.9026
2025-03-26 17:12:27,451 - wandb.wandb_agent - INFO - Cleaning up finished run: ny3vy612
2025-03-26 17:12:27,988 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:27,989 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:12:27,992 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:12:33,004 - wandb.wandb_agent - INFO - Running runs: ['dev0n0zp']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171232-dev0n0zp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dev0n0zp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dev0n0zp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2228 | Val Loss: 2.8871
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7435 | Val Loss: 2.6152
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5441 | Val Loss: 2.4864
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4487 | Val Loss: 2.4084
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3832 | Val Loss: 2.3519
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3208 | Val Loss: 2.2876
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2603 | Val Loss: 2.2317
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2109 | Val Loss: 2.1925
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1804 | Val Loss: 2.1678
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.15898
wandb:   val_loss 2.15005
wandb: 
wandb: üöÄ View run blooming-sweep-67 at: https://wandb.ai/7shoe/domShift-extensive/runs/dev0n0zp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171232-dev0n0zp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1590 | Val Loss: 2.1501
2025-03-26 17:13:33,915 - wandb.wandb_agent - INFO - Cleaning up finished run: dev0n0zp
2025-03-26 17:13:34,613 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:34,613 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:13:34,615 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:13:39,628 - wandb.wandb_agent - INFO - Running runs: ['dfxbqfva']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171339-dfxbqfva
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dfxbqfva
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dfxbqfva
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6597 | Val Loss: 1.5578
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4265 | Val Loss: 1.3053
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3262 | Val Loss: 1.3256
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2771 | Val Loss: 1.2411
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2107 | Val Loss: 1.1865
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1844 | Val Loss: 1.1844
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1725 | Val Loss: 1.1595
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1491 | Val Loss: 1.1374
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1331 | Val Loss: 1.1293
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12447
wandb:   val_loss 1.12194
wandb: 
wandb: üöÄ View run fiery-sweep-75 at: https://wandb.ai/7shoe/domShift-extensive/runs/dfxbqfva
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171339-dfxbqfva/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1245 | Val Loss: 1.1219
2025-03-26 17:14:55,696 - wandb.wandb_agent - INFO - Cleaning up finished run: dfxbqfva
2025-03-26 17:14:56,290 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:14:56,291 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:14:56,293 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:15:01,306 - wandb.wandb_agent - INFO - Running runs: ['tu7rz4ki']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171501-tu7rz4ki
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-86
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tu7rz4ki
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tu7rz4ki
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2587 | Val Loss: 1.2137
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1945 | Val Loss: 1.2232
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2195 | Val Loss: 1.2122
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2009 | Val Loss: 1.2098
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2153 | Val Loss: 1.2258
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2258 | Val Loss: 1.2306
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2288 | Val Loss: 1.2321
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2316 | Val Loss: 1.2374
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2377 | Val Loss: 1.2406
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb:   val_loss ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.24297
wandb:   val_loss 1.25219
wandb: 
wandb: üöÄ View run robust-sweep-86 at: https://wandb.ai/7shoe/domShift-extensive/runs/tu7rz4ki
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171501-tu7rz4ki/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2430 | Val Loss: 1.2522
2025-03-26 17:15:41,860 - wandb.wandb_agent - INFO - Cleaning up finished run: tu7rz4ki
2025-03-26 17:15:42,543 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:15:42,543 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:15:42,546 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:15:47,558 - wandb.wandb_agent - INFO - Running runs: ['bssqtus0']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171547-bssqtus0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bssqtus0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: bssqtus0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6448 | Val Loss: 1.3539
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4040 | Val Loss: 1.4658
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4146 | Val Loss: 1.3456
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3186 | Val Loss: 1.2974
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2765 | Val Loss: 1.2587
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2484 | Val Loss: 1.2321
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2305 | Val Loss: 1.2302
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2017 | Val Loss: 1.1766
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1637 | Val Loss: 1.1561
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15693
wandb:   val_loss 1.15486
wandb: 
wandb: üöÄ View run vocal-sweep-91 at: https://wandb.ai/7shoe/domShift-extensive/runs/bssqtus0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171547-bssqtus0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1569 | Val Loss: 1.1549
2025-03-26 17:16:23,064 - wandb.wandb_agent - INFO - Cleaning up finished run: bssqtus0
2025-03-26 17:16:23,628 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:23,628 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:16:23,631 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:16:28,640 - wandb.wandb_agent - INFO - Running runs: ['ddkl4a2z']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171628-ddkl4a2z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-97
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ddkl4a2z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ddkl4a2z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7128 | Val Loss: 1.6607
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6238 | Val Loss: 1.5482
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4635 | Val Loss: 1.3757
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2246 | Val Loss: 1.1282
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1135 | Val Loss: 1.1125
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1183 | Val Loss: 1.1205
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1231 | Val Loss: 1.1254
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1340 | Val Loss: 1.1380
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1376 | Val Loss: 1.1362
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13695
wandb:   val_loss 1.1372
wandb: 
wandb: üöÄ View run sweet-sweep-97 at: https://wandb.ai/7shoe/domShift-extensive/runs/ddkl4a2z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171628-ddkl4a2z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1369 | Val Loss: 1.1372
2025-03-26 17:17:19,336 - wandb.wandb_agent - INFO - Cleaning up finished run: ddkl4a2z
2025-03-26 17:17:19,835 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:17:19,835 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:17:19,838 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:17:24,851 - wandb.wandb_agent - INFO - Running runs: ['07g76lf4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171724-07g76lf4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-105
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/07g76lf4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 07g76lf4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.6395 | Val Loss: 2.2451
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.1226 | Val Loss: 1.8966
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.7641 | Val Loss: 1.6157
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5859 | Val Loss: 1.5767
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5881 | Val Loss: 1.6045
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.6191 | Val Loss: 1.6338
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.6446 | Val Loss: 1.6544
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.6615 | Val Loss: 1.6691
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.6770 | Val Loss: 1.6847
wandb: - 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.69072
wandb:   val_loss 1.69737
wandb: 
wandb: üöÄ View run azure-sweep-105 at: https://wandb.ai/7shoe/domShift-extensive/runs/07g76lf4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171724-07g76lf4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.6907 | Val Loss: 1.6974
2025-03-26 17:18:00,335 - wandb.wandb_agent - INFO - Cleaning up finished run: 07g76lf4
2025-03-26 17:18:02,216 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:18:02,216 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:18:02,219 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:18:07,231 - wandb.wandb_agent - INFO - Running runs: ['61kxgppa']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171807-61kxgppa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-109
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/61kxgppa
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 61kxgppa
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5429 | Val Loss: 1.2576
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1673 | Val Loss: 1.0539
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9697 | Val Loss: 0.9226
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9195 | Val Loss: 0.9195
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9205 | Val Loss: 0.9215
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9206 | Val Loss: 0.9188
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9186 | Val Loss: 0.9187
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9190 | Val Loss: 0.9197
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9199 | Val Loss: 0.9196
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.91934
wandb:   val_loss 0.91923
wandb: 
wandb: üöÄ View run misty-sweep-109 at: https://wandb.ai/7shoe/domShift-extensive/runs/61kxgppa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171807-61kxgppa/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9193 | Val Loss: 0.9192
2025-03-26 17:18:47,832 - wandb.wandb_agent - INFO - Cleaning up finished run: 61kxgppa
2025-03-26 17:18:48,926 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:18:48,926 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:18:48,929 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:18:53,940 - wandb.wandb_agent - INFO - Running runs: ['0tmk3m2m']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171855-0tmk3m2m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-114
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0tmk3m2m
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0tmk3m2m
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6599 | Val Loss: 1.2179
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1080 | Val Loss: 1.0653
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0562 | Val Loss: 1.0486
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0551 | Val Loss: 1.0480
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0406 | Val Loss: 1.0370
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0524 | Val Loss: 1.0798
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1020 | Val Loss: 1.1210
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1332 | Val Loss: 1.1458
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1553 | Val Loss: 1.1636
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17022
wandb:   val_loss 1.17709
wandb: 
wandb: üöÄ View run giddy-sweep-114 at: https://wandb.ai/7shoe/domShift-extensive/runs/0tmk3m2m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171855-0tmk3m2m/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1702 | Val Loss: 1.1771
2025-03-26 17:19:44,805 - wandb.wandb_agent - INFO - Cleaning up finished run: 0tmk3m2m
2025-03-26 17:19:45,548 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:19:45,549 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:19:45,553 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:19:50,565 - wandb.wandb_agent - INFO - Running runs: ['fcq7b55f']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171950-fcq7b55f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-121
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fcq7b55f
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fcq7b55f
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1488 | Val Loss: 2.8428
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7723 | Val Loss: 2.7235
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.7393 | Val Loss: 2.7661
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7804 | Val Loss: 2.7843
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7782 | Val Loss: 2.7697
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.7614 | Val Loss: 2.7520
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.7440 | Val Loss: 2.7355
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.7287 | Val Loss: 2.7217
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.7162 | Val Loss: 2.7105
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.70598
wandb:   val_loss 2.70144
wandb: 
wandb: üöÄ View run hearty-sweep-121 at: https://wandb.ai/7shoe/domShift-extensive/runs/fcq7b55f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171950-fcq7b55f/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7060 | Val Loss: 2.7014
2025-03-26 17:20:51,428 - wandb.wandb_agent - INFO - Cleaning up finished run: fcq7b55f
2025-03-26 17:20:52,044 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:52,044 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:20:52,046 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:20:57,059 - wandb.wandb_agent - INFO - Running runs: ['zom91s54']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172057-zom91s54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-127
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zom91s54
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: zom91s54
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9439 | Val Loss: 2.3740
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2421 | Val Loss: 2.1526
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1228 | Val Loss: 2.1038
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0997 | Val Loss: 2.0991
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1018 | Val Loss: 2.1032
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1063 | Val Loss: 2.1093
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1126 | Val Loss: 2.1153
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1181 | Val Loss: 2.1210
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1236 | Val Loss: 2.1260
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.12773
wandb:   val_loss 2.12917
wandb: 
wandb: üöÄ View run peachy-sweep-127 at: https://wandb.ai/7shoe/domShift-extensive/runs/zom91s54
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172057-zom91s54/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1277 | Val Loss: 2.1292
2025-03-26 17:21:47,789 - wandb.wandb_agent - INFO - Cleaning up finished run: zom91s54
2025-03-26 17:21:48,793 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:21:48,793 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:21:48,796 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:21:53,808 - wandb.wandb_agent - INFO - Running runs: ['trknkekh']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172153-trknkekh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-135
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/trknkekh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: trknkekh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7827 | Val Loss: 1.6057
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4769 | Val Loss: 1.3895
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3242 | Val Loss: 1.2536
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1950 | Val Loss: 1.1993
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1854 | Val Loss: 1.1197
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1048 | Val Loss: 1.1063
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1183 | Val Loss: 1.1238
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1304 | Val Loss: 1.1358
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1373 | Val Loss: 1.1354
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13582
wandb:   val_loss 1.13527
wandb: 
wandb: üöÄ View run silver-sweep-135 at: https://wandb.ai/7shoe/domShift-extensive/runs/trknkekh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172153-trknkekh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1358 | Val Loss: 1.1353
2025-03-26 17:22:29,241 - wandb.wandb_agent - INFO - Cleaning up finished run: trknkekh
2025-03-26 17:22:29,990 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:22:29,991 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:22:29,995 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:22:35,007 - wandb.wandb_agent - INFO - Running runs: ['fkwzmqn9']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172234-fkwzmqn9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-140
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fkwzmqn9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fkwzmqn9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5256 | Val Loss: 1.3798
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2253 | Val Loss: 1.1437
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1258 | Val Loss: 1.1143
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1108 | Val Loss: 1.1063
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1043 | Val Loss: 1.1016
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1005 | Val Loss: 1.0989
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0976 | Val Loss: 1.0958
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0946 | Val Loss: 1.0931
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0923 | Val Loss: 1.0911
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09057
wandb:   val_loss 1.08967
wandb: 
wandb: üöÄ View run graceful-sweep-140 at: https://wandb.ai/7shoe/domShift-extensive/runs/fkwzmqn9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172234-fkwzmqn9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0906 | Val Loss: 1.0897
2025-03-26 17:23:25,741 - wandb.wandb_agent - INFO - Cleaning up finished run: fkwzmqn9
2025-03-26 17:23:26,184 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:23:26,184 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:23:26,187 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:23:31,200 - wandb.wandb_agent - INFO - Running runs: ['2vutpdi2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172331-2vutpdi2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-147
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2vutpdi2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2vutpdi2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.3873 | Val Loss: 2.1528
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1796 | Val Loss: 2.1958
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2088 | Val Loss: 2.2364
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2664 | Val Loss: 2.2935
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3113 | Val Loss: 2.3297
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3476 | Val Loss: 2.3672
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3859 | Val Loss: 2.4057
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4233 | Val Loss: 2.4417
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4565 | Val Loss: 2.4714
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.48317
wandb:   val_loss 2.49506
wandb: 
wandb: üöÄ View run cool-sweep-147 at: https://wandb.ai/7shoe/domShift-extensive/runs/2vutpdi2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172331-2vutpdi2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4832 | Val Loss: 2.4951
2025-03-26 17:24:26,924 - wandb.wandb_agent - INFO - Cleaning up finished run: 2vutpdi2
2025-03-26 17:24:27,508 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:27,508 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:24:27,510 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:24:32,523 - wandb.wandb_agent - INFO - Running runs: ['s8z8dfe1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172433-s8z8dfe1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-153
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s8z8dfe1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s8z8dfe1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5919 | Val Loss: 1.1597
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2346 | Val Loss: 1.3621
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4318 | Val Loss: 1.4868
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4044 | Val Loss: 1.3701
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4505 | Val Loss: 1.4809
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3258 | Val Loss: 1.2486
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2324 | Val Loss: 1.2333
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2480 | Val Loss: 1.2125
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2000 | Val Loss: 1.1981
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20763
wandb:   val_loss 1.21842
wandb: 
wandb: üöÄ View run resilient-sweep-153 at: https://wandb.ai/7shoe/domShift-extensive/runs/s8z8dfe1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172433-s8z8dfe1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2076 | Val Loss: 1.2184
2025-03-26 17:25:43,556 - wandb.wandb_agent - INFO - Cleaning up finished run: s8z8dfe1
2025-03-26 17:25:44,442 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:25:44,442 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:25:44,444 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:25:49,457 - wandb.wandb_agent - INFO - Running runs: ['hbz3yql8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172550-hbz3yql8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-162
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hbz3yql8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: hbz3yql8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.0268 | Val Loss: 1.7166
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6906 | Val Loss: 1.6466
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6219 | Val Loss: 1.5801
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5615 | Val Loss: 1.5257
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5077 | Val Loss: 1.4703
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4544 | Val Loss: 1.4301
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4315 | Val Loss: 1.4292
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4238 | Val Loss: 1.4135
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4148 | Val Loss: 1.4172
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4214
wandb:   val_loss 1.42844
wandb: 
wandb: üöÄ View run dutiful-sweep-162 at: https://wandb.ai/7shoe/domShift-extensive/runs/hbz3yql8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172550-hbz3yql8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4214 | Val Loss: 1.4284
2025-03-26 17:26:14,796 - wandb.wandb_agent - INFO - Cleaning up finished run: hbz3yql8
2025-03-26 17:26:15,251 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:26:15,251 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:26:15,254 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.9
2025-03-26 17:26:20,267 - wandb.wandb_agent - INFO - Running runs: ['atsyw3xn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172621-atsyw3xn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-166
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/atsyw3xn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: atsyw3xn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.1349 | Val Loss: 1.0052
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9735 | Val Loss: 1.0395
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0655 | Val Loss: 1.1542
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2274 | Val Loss: 1.1613
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1832 | Val Loss: 1.1091
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1025 | Val Loss: 1.1084
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1074 | Val Loss: 1.1074
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1078 | Val Loss: 1.1075
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1077 | Val Loss: 1.1077
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.089 MB uploadedwandb: | 32.995 MB of 33.089 MB uploadedwandb: / 33.089 MB of 33.089 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10777
wandb:   val_loss 1.10768
wandb: 
wandb: üöÄ View run jolly-sweep-166 at: https://wandb.ai/7shoe/domShift-extensive/runs/atsyw3xn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172621-atsyw3xn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1078 | Val Loss: 1.1077
2025-03-26 17:27:10,978 - wandb.wandb_agent - INFO - Cleaning up finished run: atsyw3xn
2025-03-26 17:27:11,514 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:27:11,514 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:27:11,516 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:27:16,529 - wandb.wandb_agent - INFO - Running runs: ['p3hh3jnx']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172717-p3hh3jnx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-173
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/p3hh3jnx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: p3hh3jnx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3236 | Val Loss: 1.1140
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0638 | Val Loss: 1.0653
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0584 | Val Loss: 1.0499
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0493 | Val Loss: 1.0484
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0490 | Val Loss: 1.0486
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0521 | Val Loss: 1.0575
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0473 | Val Loss: 1.0499
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0560 | Val Loss: 1.0522
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0522 | Val Loss: 1.0542
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06005
wandb:   val_loss 1.06516
wandb: 
wandb: üöÄ View run fresh-sweep-173 at: https://wandb.ai/7shoe/domShift-extensive/runs/p3hh3jnx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172717-p3hh3jnx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0600 | Val Loss: 1.0652
2025-03-26 17:28:02,157 - wandb.wandb_agent - INFO - Cleaning up finished run: p3hh3jnx
2025-03-26 17:28:02,920 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:28:02,920 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:28:02,923 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:28:07,936 - wandb.wandb_agent - INFO - Running runs: ['0ct4u19b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172807-0ct4u19b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-179
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0ct4u19b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 0ct4u19b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3940 | Val Loss: 1.4157
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5389 | Val Loss: 1.5724
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5649 | Val Loss: 1.5739
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6058 | Val Loss: 1.6357
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.6415 | Val Loss: 1.6241
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4603 | Val Loss: 1.1756
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1800 | Val Loss: 1.1826
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1840 | Val Loss: 1.1799
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1685 | Val Loss: 1.1577
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15231
wandb:   val_loss 1.15121
wandb: 
wandb: üöÄ View run proud-sweep-179 at: https://wandb.ai/7shoe/domShift-extensive/runs/0ct4u19b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172807-0ct4u19b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1523 | Val Loss: 1.1512
2025-03-26 17:28:38,420 - wandb.wandb_agent - INFO - Cleaning up finished run: 0ct4u19b
2025-03-26 17:28:39,008 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:28:39,008 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:28:39,011 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:28:44,021 - wandb.wandb_agent - INFO - Running runs: ['9lebnifh']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172843-9lebnifh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-184
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9lebnifh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9lebnifh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.3401 | Val Loss: 2.8086
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7894 | Val Loss: 2.7239
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6332 | Val Loss: 2.5516
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4997 | Val Loss: 2.4346
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4156 | Val Loss: 2.3961
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3918 | Val Loss: 2.3995
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4190 | Val Loss: 2.4423
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4640 | Val Loss: 2.4888
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5129 | Val Loss: 2.5392
wandb: - 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.55943
wandb:   val_loss 2.58047
wandb: 
wandb: üöÄ View run lively-sweep-184 at: https://wandb.ai/7shoe/domShift-extensive/runs/9lebnifh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172843-9lebnifh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5594 | Val Loss: 2.5805
2025-03-26 17:29:34,736 - wandb.wandb_agent - INFO - Cleaning up finished run: 9lebnifh
2025-03-26 17:29:36,795 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:29:36,795 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:29:36,798 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:29:41,811 - wandb.wandb_agent - INFO - Running runs: ['yggt34sb']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172941-yggt34sb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-190
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yggt34sb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yggt34sb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3515 | Val Loss: 1.3199
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2928 | Val Loss: 1.2528
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2615 | Val Loss: 1.2376
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1703 | Val Loss: 1.1294
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1270 | Val Loss: 1.1070
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1196 | Val Loss: 1.1264
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1226 | Val Loss: 1.1209
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1226 | Val Loss: 1.1253
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1229 | Val Loss: 1.1221
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12276
wandb:   val_loss 1.12456
wandb: 
wandb: üöÄ View run serene-sweep-190 at: https://wandb.ai/7shoe/domShift-extensive/runs/yggt34sb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172941-yggt34sb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1228 | Val Loss: 1.1246
2025-03-26 17:30:32,504 - wandb.wandb_agent - INFO - Cleaning up finished run: yggt34sb
2025-03-26 17:30:33,293 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:30:33,294 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:30:33,296 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:30:38,309 - wandb.wandb_agent - INFO - Running runs: ['wfr59nq4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173039-wfr59nq4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-196
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wfr59nq4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wfr59nq4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3818 | Val Loss: 1.2101
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2167 | Val Loss: 1.2455
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2530 | Val Loss: 1.2418
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2422 | Val Loss: 1.4547
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4897 | Val Loss: 1.2739
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1316 | Val Loss: 1.0163
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9453 | Val Loss: 0.8849
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8624 | Val Loss: 0.8357
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8226 | Val Loss: 0.8108
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.80925
wandb:   val_loss 0.80874
wandb: 
wandb: üöÄ View run olive-sweep-196 at: https://wandb.ai/7shoe/domShift-extensive/runs/wfr59nq4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173039-wfr59nq4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8093 | Val Loss: 0.8087
2025-03-26 17:31:18,904 - wandb.wandb_agent - INFO - Cleaning up finished run: wfr59nq4
2025-03-26 17:31:19,499 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:31:19,499 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:31:19,503 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:31:24,515 - wandb.wandb_agent - INFO - Running runs: ['5vvs38ru']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173124-5vvs38ru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-200
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5vvs38ru
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 5vvs38ru
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0521 | Val Loss: 0.8964
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.8522 | Val Loss: 0.8670
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8931 | Val Loss: 0.8916
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8948 | Val Loss: 0.8954
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8939 | Val Loss: 0.8934
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8932 | Val Loss: 0.8912
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8911 | Val Loss: 0.8917
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8913 | Val Loss: 0.8916
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8912 | Val Loss: 0.8916
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.89108
wandb:   val_loss 0.89155
wandb: 
wandb: üöÄ View run dandy-sweep-200 at: https://wandb.ai/7shoe/domShift-extensive/runs/5vvs38ru
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173124-5vvs38ru/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8911 | Val Loss: 0.8916
2025-03-26 17:32:35,575 - wandb.wandb_agent - INFO - Cleaning up finished run: 5vvs38ru
2025-03-26 17:32:36,253 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:32:36,253 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:32:36,256 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:32:41,269 - wandb.wandb_agent - INFO - Running runs: ['hz1bccbi']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173241-hz1bccbi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-208
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hz1bccbi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hz1bccbi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8941 | Val Loss: 2.3785
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2329 | Val Loss: 2.0850
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0295 | Val Loss: 1.9912
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.9884 | Val Loss: 1.9863
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.9837 | Val Loss: 1.9785
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.9702 | Val Loss: 1.9613
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.9551 | Val Loss: 1.9504
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.8873 | Val Loss: 1.8352
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.7366 | Val Loss: 1.6522
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.469 MB uploadedwandb: | 137.311 MB of 137.469 MB uploadedwandb: / 137.469 MB of 137.469 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.63247
wandb:   val_loss 1.63202
wandb: 
wandb: üöÄ View run swift-sweep-208 at: https://wandb.ai/7shoe/domShift-extensive/runs/hz1bccbi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173241-hz1bccbi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.6325 | Val Loss: 1.6320
2025-03-26 17:34:07,578 - wandb.wandb_agent - INFO - Cleaning up finished run: hz1bccbi
2025-03-26 17:34:09,167 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:34:09,167 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:34:09,170 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:34:14,182 - wandb.wandb_agent - INFO - Running runs: ['uinpdsmg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173415-uinpdsmg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-218
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uinpdsmg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: uinpdsmg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6974 | Val Loss: 1.5119
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1878 | Val Loss: 0.9719
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9583 | Val Loss: 0.9379
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9493 | Val Loss: 0.9629
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9819 | Val Loss: 1.0007
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0207 | Val Loss: 1.0403
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0590 | Val Loss: 1.0773
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0945 | Val Loss: 1.1116
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1277 | Val Loss: 1.1440
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15909
wandb:   val_loss 1.17468
wandb: 
wandb: üöÄ View run astral-sweep-218 at: https://wandb.ai/7shoe/domShift-extensive/runs/uinpdsmg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173415-uinpdsmg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1591 | Val Loss: 1.1747
2025-03-26 17:35:04,932 - wandb.wandb_agent - INFO - Cleaning up finished run: uinpdsmg
2025-03-26 17:35:05,705 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:05,705 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:35:05,708 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:35:10,720 - wandb.wandb_agent - INFO - Running runs: ['kgvrrfq5']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173512-kgvrrfq5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-222
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kgvrrfq5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: kgvrrfq5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1822 | Val Loss: 1.0654
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0751 | Val Loss: 1.0694
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0844 | Val Loss: 1.0751
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0736 | Val Loss: 1.0676
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0624 | Val Loss: 1.0575
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0611 | Val Loss: 1.0686
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0736 | Val Loss: 1.0790
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0877 | Val Loss: 1.0895
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0869 | Val Loss: 1.0859
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0826
wandb:   val_loss 1.07874
wandb: 
wandb: üöÄ View run youthful-sweep-222 at: https://wandb.ai/7shoe/domShift-extensive/runs/kgvrrfq5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173512-kgvrrfq5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0826 | Val Loss: 1.0787
2025-03-26 17:35:57,418 - wandb.wandb_agent - INFO - Cleaning up finished run: kgvrrfq5
2025-03-26 17:35:57,978 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:57,978 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:35:57,980 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:36:02,993 - wandb.wandb_agent - INFO - Running runs: ['ac6sbq4s']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173603-ac6sbq4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-230
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ac6sbq4s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ac6sbq4s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9196 | Val Loss: 1.8403
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.7409 | Val Loss: 1.5866
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4561 | Val Loss: 1.3673
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3646 | Val Loss: 1.3544
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3543 | Val Loss: 1.3530
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3541 | Val Loss: 1.3527
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3549 | Val Loss: 1.3562
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3586 | Val Loss: 1.3601
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3636 | Val Loss: 1.3663
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37075
wandb:   val_loss 1.37506
wandb: 
wandb: üöÄ View run sparkling-sweep-230 at: https://wandb.ai/7shoe/domShift-extensive/runs/ac6sbq4s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173603-ac6sbq4s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3708 | Val Loss: 1.3751
2025-03-26 17:36:33,408 - wandb.wandb_agent - INFO - Cleaning up finished run: ac6sbq4s
2025-03-26 17:36:33,980 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:36:33,981 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:36:33,983 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:36:38,996 - wandb.wandb_agent - INFO - Running runs: ['z7b2542v']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173639-z7b2542v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-233
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/z7b2542v
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: z7b2542v
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2474 | Val Loss: 1.2144
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2563 | Val Loss: 1.2471
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2532 | Val Loss: 1.2608
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2549 | Val Loss: 1.2434
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2452 | Val Loss: 1.2458
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2670 | Val Loss: 1.3034
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3059 | Val Loss: 1.3006
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3125 | Val Loss: 1.3270
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3383 | Val Loss: 1.3508
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñà
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36445
wandb:   val_loss 1.37938
wandb: 
wandb: üöÄ View run devout-sweep-233 at: https://wandb.ai/7shoe/domShift-extensive/runs/z7b2542v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173639-z7b2542v/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3644 | Val Loss: 1.3794
2025-03-26 17:37:49,976 - wandb.wandb_agent - INFO - Cleaning up finished run: z7b2542v
2025-03-26 17:37:50,905 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:37:50,906 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:37:50,908 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:37:55,921 - wandb.wandb_agent - INFO - Running runs: ['xn065e8i']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173755-xn065e8i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-242
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xn065e8i
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: xn065e8i
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6433 | Val Loss: 1.5007
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5821 | Val Loss: 1.6251
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6457 | Val Loss: 1.6647
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6921 | Val Loss: 1.7016
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.7037 | Val Loss: 1.6939
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6785 | Val Loss: 1.6656
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6884 | Val Loss: 1.7068
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6342 | Val Loss: 1.6092
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6713 | Val Loss: 1.7466
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñà
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÑ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.71205
wandb:   val_loss 1.55036
wandb: 
wandb: üöÄ View run feasible-sweep-242 at: https://wandb.ai/7shoe/domShift-extensive/runs/xn065e8i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173755-xn065e8i/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7121 | Val Loss: 1.5504
2025-03-26 17:39:01,785 - wandb.wandb_agent - INFO - Cleaning up finished run: xn065e8i
2025-03-26 17:39:02,295 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:02,296 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:39:02,298 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:39:07,310 - wandb.wandb_agent - INFO - Running runs: ['v16ywsgi']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173907-v16ywsgi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-249
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/v16ywsgi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: v16ywsgi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9515 | Val Loss: 1.7184
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5379 | Val Loss: 1.3365
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2378 | Val Loss: 1.1741
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1991 | Val Loss: 1.2317
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2360 | Val Loss: 1.2175
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2073 | Val Loss: 1.1645
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1260 | Val Loss: 1.1167
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1302 | Val Loss: 1.1362
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1435 | Val Loss: 1.1493
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14893
wandb:   val_loss 1.14492
wandb: 
wandb: üöÄ View run tough-sweep-249 at: https://wandb.ai/7shoe/domShift-extensive/runs/v16ywsgi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173907-v16ywsgi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1489 | Val Loss: 1.1449
2025-03-26 17:39:47,854 - wandb.wandb_agent - INFO - Cleaning up finished run: v16ywsgi
2025-03-26 17:39:48,398 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:48,399 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:39:48,401 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:39:53,414 - wandb.wandb_agent - INFO - Running runs: ['9eask2av']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173953-9eask2av
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-256
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9eask2av
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9eask2av
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.5090 | Val Loss: 2.0959
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.0182 | Val Loss: 1.9626
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.9133 | Val Loss: 1.8622
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.8392 | Val Loss: 1.8168
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.8074 | Val Loss: 1.7992
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.7995 | Val Loss: 1.8003
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.8049 | Val Loss: 1.8090
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.8133 | Val Loss: 1.8171
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.8210 | Val Loss: 1.8240
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.988 MB uploadedwandb: | 32.968 MB of 32.988 MB uploadedwandb: / 32.988 MB of 32.988 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.82671
wandb:   val_loss 1.82886
wandb: 
wandb: üöÄ View run elated-sweep-256 at: https://wandb.ai/7shoe/domShift-extensive/runs/9eask2av
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173953-9eask2av/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.8267 | Val Loss: 1.8289
2025-03-26 17:40:33,984 - wandb.wandb_agent - INFO - Cleaning up finished run: 9eask2av
2025-03-26 17:40:34,637 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:40:34,637 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:40:34,640 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:40:39,652 - wandb.wandb_agent - INFO - Running runs: ['fb3kn01c']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174040-fb3kn01c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-262
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fb3kn01c
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fb3kn01c
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4481 | Val Loss: 1.3257
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3507 | Val Loss: 1.3808
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4067 | Val Loss: 1.4365
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4677 | Val Loss: 1.5113
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5450 | Val Loss: 1.5943
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.6181 | Val Loss: 1.6500
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.6427 | Val Loss: 1.6172
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5851 | Val Loss: 1.5409
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.5141 | Val Loss: 1.4936
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.47748
wandb:   val_loss 1.44603
wandb: 
wandb: üöÄ View run daily-sweep-262 at: https://wandb.ai/7shoe/domShift-extensive/runs/fb3kn01c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174040-fb3kn01c/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4775 | Val Loss: 1.4460
2025-03-26 17:41:10,015 - wandb.wandb_agent - INFO - Cleaning up finished run: fb3kn01c
2025-03-26 17:41:10,621 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:41:10,621 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:41:10,624 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:41:15,636 - wandb.wandb_agent - INFO - Running runs: ['41cysrw8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174116-41cysrw8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-266
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/41cysrw8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 41cysrw8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.4096 | Val Loss: 3.1106
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7760 | Val Loss: 2.5846
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5705 | Val Loss: 2.5654
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5683 | Val Loss: 2.5696
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5734 | Val Loss: 2.5781
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5892 | Val Loss: 2.6023
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6214 | Val Loss: 2.6432
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6681 | Val Loss: 2.6932
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.7168 | Val Loss: 2.7395
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.425 MB uploadedwandb: | 137.266 MB of 137.425 MB uploadedwandb: / 137.425 MB of 137.425 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.76179
wandb:   val_loss 2.7843
wandb: 
wandb: üöÄ View run grateful-sweep-266 at: https://wandb.ai/7shoe/domShift-extensive/runs/41cysrw8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174116-41cysrw8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7618 | Val Loss: 2.7843
2025-03-26 17:42:41,908 - wandb.wandb_agent - INFO - Cleaning up finished run: 41cysrw8
2025-03-26 17:42:42,899 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:42:42,899 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:42:42,902 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:42:47,914 - wandb.wandb_agent - INFO - Running runs: ['8lylstab']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174247-8lylstab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-276
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8lylstab
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 8lylstab
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8013 | Val Loss: 1.4193
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4266 | Val Loss: 1.4415
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4218 | Val Loss: 1.3982
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4296 | Val Loss: 1.4793
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5217 | Val Loss: 1.5012
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4895 | Val Loss: 1.5000
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4564 | Val Loss: 1.4248
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4498 | Val Loss: 1.4542
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4432 | Val Loss: 1.4420
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñÇ‚ñÑ‚ñÅ‚ñá‚ñà‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4492
wandb:   val_loss 1.45058
wandb: 
wandb: üöÄ View run smooth-sweep-276 at: https://wandb.ai/7shoe/domShift-extensive/runs/8lylstab
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174247-8lylstab/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4492 | Val Loss: 1.4506
2025-03-26 17:43:28,477 - wandb.wandb_agent - INFO - Cleaning up finished run: 8lylstab
2025-03-26 17:43:29,250 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:43:29,251 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:43:29,254 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
2025-03-26 17:43:34,266 - wandb.wandb_agent - INFO - Running runs: ['9wl3gct4']
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174334-9wl3gct4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-281
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9wl3gct4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9wl3gct4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6691 | Val Loss: 1.3043
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1449 | Val Loss: 1.1848
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3066 | Val Loss: 1.3805
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4065 | Val Loss: 1.3781
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3255 | Val Loss: 1.3231
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3338 | Val Loss: 1.3386
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3346 | Val Loss: 1.3282
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3221 | Val Loss: 1.3152
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3172 | Val Loss: 1.3252
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÖ‚ñÅ‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3338
wandb:   val_loss 1.33915
wandb: 
wandb: üöÄ View run desert-sweep-281 at: https://wandb.ai/7shoe/domShift-extensive/runs/9wl3gct4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174334-9wl3gct4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3338 | Val Loss: 1.3392
2025-03-26 17:44:19,925 - wandb.wandb_agent - INFO - Cleaning up finished run: 9wl3gct4
2025-03-26 17:44:20,456 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:44:20,456 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:44:20,459 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.4
2025-03-26 17:44:25,472 - wandb.wandb_agent - INFO - Running runs: ['vcn44ueb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174426-vcn44ueb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-287
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vcn44ueb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: vcn44ueb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2467 | Val Loss: 3.0247
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.8985 | Val Loss: 2.7548
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.6114 | Val Loss: 2.4602
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.3851 | Val Loss: 2.3109
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.2811 | Val Loss: 2.2551
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.2433 | Val Loss: 2.2253
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.2225 | Val Loss: 2.2188
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.2188 | Val Loss: 2.2148
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.2134 | Val Loss: 2.2101
wandb: - 32.852 MB of 32.852 MB uploadedwandb: \ 32.852 MB of 32.852 MB uploadedwandb: | 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.21155
wandb:   val_loss 2.20984
wandb: 
wandb: üöÄ View run absurd-sweep-287 at: https://wandb.ai/7shoe/domShift-extensive/runs/vcn44ueb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174426-vcn44ueb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.2115 | Val Loss: 2.2098
2025-03-26 17:44:55,867 - wandb.wandb_agent - INFO - Cleaning up finished run: vcn44ueb
2025-03-26 17:44:57,011 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:44:57,011 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:44:57,014 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:45:02,027 - wandb.wandb_agent - INFO - Running runs: ['xe8l9jfq']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174501-xe8l9jfq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-292
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xe8l9jfq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xe8l9jfq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5943 | Val Loss: 1.2575
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2691 | Val Loss: 1.2694
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2702 | Val Loss: 1.3042
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3433 | Val Loss: 1.3637
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2370 | Val Loss: 1.1475
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1358 | Val Loss: 1.1456
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1552 | Val Loss: 1.1552
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1556 | Val Loss: 1.1726
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1478 | Val Loss: 1.1143
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09388
wandb:   val_loss 1.06383
wandb: 
wandb: üöÄ View run cool-sweep-292 at: https://wandb.ai/7shoe/domShift-extensive/runs/xe8l9jfq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174501-xe8l9jfq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0939 | Val Loss: 1.0638
2025-03-26 17:45:32,483 - wandb.wandb_agent - INFO - Cleaning up finished run: xe8l9jfq
2025-03-26 17:45:33,177 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:45:33,177 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:45:33,181 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:45:38,193 - wandb.wandb_agent - INFO - Running runs: ['7kydp19z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174539-7kydp19z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-294
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7kydp19z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7kydp19z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.3112 | Val Loss: 1.8156
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.7968 | Val Loss: 1.8297
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.8973 | Val Loss: 1.9508
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.9566 | Val Loss: 1.9539
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.9299 | Val Loss: 1.8904
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.8250 | Val Loss: 1.7386
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.6496 | Val Loss: 1.5580
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4643 | Val Loss: 1.3860
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3353 | Val Loss: 1.2930
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.987 MB uploadedwandb: | 32.968 MB of 32.987 MB uploadedwandb: / 32.987 MB of 32.987 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27304
wandb:   val_loss 1.26071
wandb: 
wandb: üöÄ View run deep-sweep-294 at: https://wandb.ai/7shoe/domShift-extensive/runs/7kydp19z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174539-7kydp19z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2730 | Val Loss: 1.2607
2025-03-26 17:46:18,745 - wandb.wandb_agent - INFO - Cleaning up finished run: 7kydp19z
2025-03-26 17:46:19,370 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:46:19,370 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:46:19,373 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:46:24,385 - wandb.wandb_agent - INFO - Running runs: ['ju1yy7li']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174625-ju1yy7li
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-299
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ju1yy7li
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ju1yy7li
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4662 | Val Loss: 1.2051
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2166 | Val Loss: 1.3104
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3672 | Val Loss: 1.3932
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3678 | Val Loss: 1.3561
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3592 | Val Loss: 1.3816
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3800 | Val Loss: 1.3744
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5053 | Val Loss: 1.6614
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5706 | Val Loss: 1.5310
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.5203 | Val Loss: 1.4996
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñá‚ñá
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.51807
wandb:   val_loss 1.55132
wandb: 
wandb: üöÄ View run mild-sweep-299 at: https://wandb.ai/7shoe/domShift-extensive/runs/ju1yy7li
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174625-ju1yy7li/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.5181 | Val Loss: 1.5513
2025-03-26 17:47:15,063 - wandb.wandb_agent - INFO - Cleaning up finished run: ju1yy7li
2025-03-26 17:47:15,523 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:47:15,523 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:47:15,526 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:47:20,539 - wandb.wandb_agent - INFO - Running runs: ['978a3x59']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174721-978a3x59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-307
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/978a3x59
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 978a3x59
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1694 | Val Loss: 3.1176
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.0109 | Val Loss: 2.9387
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9832 | Val Loss: 3.0318
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.0832 | Val Loss: 3.1315
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.1631 | Val Loss: 3.1839
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.1880 | Val Loss: 3.1809
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.1384 | Val Loss: 3.0912
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.0762 | Val Loss: 3.0740
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.9550 | Val Loss: 2.7490
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.051 MB of 137.051 MB uploadedwandb: | 137.051 MB of 137.051 MB uploadedwandb: / 137.051 MB of 137.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÅ
wandb:   val_loss ‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.68783
wandb:   val_loss 2.68561
wandb: 
wandb: üöÄ View run fragrant-sweep-307 at: https://wandb.ai/7shoe/domShift-extensive/runs/978a3x59
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174721-978a3x59/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.6878 | Val Loss: 2.6856
2025-03-26 17:48:16,347 - wandb.wandb_agent - INFO - Cleaning up finished run: 978a3x59
2025-03-26 17:48:16,999 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:48:16,999 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:48:17,003 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:48:22,015 - wandb.wandb_agent - INFO - Running runs: ['vualtvxa']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174823-vualtvxa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-313
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vualtvxa
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: vualtvxa
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2864 | Val Loss: 1.1131
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1657 | Val Loss: 1.2161
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2264 | Val Loss: 1.2598
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2860 | Val Loss: 1.3073
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3046 | Val Loss: 1.2989
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3039 | Val Loss: 1.3150
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3146 | Val Loss: 1.3111
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3096 | Val Loss: 1.3102
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3113 | Val Loss: 1.3114
wandb: - 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÅ‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3125
wandb:   val_loss 1.31303
wandb: 
wandb: üöÄ View run stellar-sweep-313 at: https://wandb.ai/7shoe/domShift-extensive/runs/vualtvxa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174823-vualtvxa/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3125 | Val Loss: 1.3130
2025-03-26 17:49:07,639 - wandb.wandb_agent - INFO - Cleaning up finished run: vualtvxa
2025-03-26 17:49:08,399 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:08,399 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:49:08,401 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:49:13,413 - wandb.wandb_agent - INFO - Running runs: ['l7g6r3eb']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174913-l7g6r3eb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-319
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l7g6r3eb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: l7g6r3eb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.1982 | Val Loss: 3.1796
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.2321 | Val Loss: 3.2345
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.3414 | Val Loss: 3.4717
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 3.5294 | Val Loss: 3.4598
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 3.2887 | Val Loss: 3.1719
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.1525 | Val Loss: 3.1604
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 3.1816 | Val Loss: 3.1700
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 3.1525 | Val Loss: 3.1230
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 3.1007 | Val Loss: 3.0786
wandb: - 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÑ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.0673
wandb:   val_loss 3.05501
wandb: 
wandb: üöÄ View run iconic-sweep-319 at: https://wandb.ai/7shoe/domShift-extensive/runs/l7g6r3eb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174913-l7g6r3eb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.0673 | Val Loss: 3.0550
2025-03-26 17:49:43,849 - wandb.wandb_agent - INFO - Cleaning up finished run: l7g6r3eb
2025-03-26 17:49:44,560 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:44,560 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:49:44,563 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:49:49,575 - wandb.wandb_agent - INFO - Running runs: ['hdaf9oy9']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174949-hdaf9oy9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-322
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hdaf9oy9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hdaf9oy9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.3404 | Val Loss: 4.6801
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.5997 | Val Loss: 4.4480
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 4.4299 | Val Loss: 4.3312
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 4.3020 | Val Loss: 4.2025
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 4.1879 | Val Loss: 4.1143
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 4.1071 | Val Loss: 4.0248
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 4.0141 | Val Loss: 3.9254
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.9224 | Val Loss: 3.8467
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.8513 | Val Loss: 3.7888
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.332 MB uploadedwandb: | 137.312 MB of 137.332 MB uploadedwandb: / 137.332 MB of 137.332 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.80534
wandb:   val_loss 3.77962
wandb: 
wandb: üöÄ View run ethereal-sweep-322 at: https://wandb.ai/7shoe/domShift-extensive/runs/hdaf9oy9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174949-hdaf9oy9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.8053 | Val Loss: 3.7796
2025-03-26 17:50:20,017 - wandb.wandb_agent - INFO - Cleaning up finished run: hdaf9oy9
2025-03-26 17:50:20,522 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:50:20,522 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:50:20,525 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:50:25,538 - wandb.wandb_agent - INFO - Running runs: ['yv23lghr']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175025-yv23lghr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-327
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yv23lghr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yv23lghr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9691 | Val Loss: 1.8847
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.8256 | Val Loss: 1.6835
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5983 | Val Loss: 1.5496
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4442 | Val Loss: 1.3364
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3221 | Val Loss: 1.3165
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3402 | Val Loss: 1.3712
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4022 | Val Loss: 1.4274
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4461 | Val Loss: 1.4663
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5063 | Val Loss: 1.5527
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.58929
wandb:   val_loss 1.61939
wandb: 
wandb: üöÄ View run firm-sweep-327 at: https://wandb.ai/7shoe/domShift-extensive/runs/yv23lghr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175025-yv23lghr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5893 | Val Loss: 1.6194
2025-03-26 17:51:36,637 - wandb.wandb_agent - INFO - Cleaning up finished run: yv23lghr
2025-03-26 17:51:37,210 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:51:37,210 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:51:37,213 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:51:42,225 - wandb.wandb_agent - INFO - Running runs: ['trksupt2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175142-trksupt2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-336
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/trksupt2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: trksupt2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8135 | Val Loss: 1.5823
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5240 | Val Loss: 1.4368
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4100 | Val Loss: 1.3573
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3426 | Val Loss: 1.3112
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3056 | Val Loss: 1.2927
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2970 | Val Loss: 1.2991
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3088 | Val Loss: 1.3182
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3308 | Val Loss: 1.3475
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3617 | Val Loss: 1.3799
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.39411
wandb:   val_loss 1.41225
wandb: 
wandb: üöÄ View run balmy-sweep-336 at: https://wandb.ai/7shoe/domShift-extensive/runs/trksupt2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175142-trksupt2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3941 | Val Loss: 1.4122
2025-03-26 17:52:12,553 - wandb.wandb_agent - INFO - Cleaning up finished run: trksupt2
2025-03-26 17:52:13,167 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:52:13,168 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:52:13,171 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:52:18,183 - wandb.wandb_agent - INFO - Running runs: ['o1zzlm7r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175219-o1zzlm7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-339
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o1zzlm7r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o1zzlm7r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5754 | Val Loss: 1.0971
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0714 | Val Loss: 1.0459
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9944 | Val Loss: 0.9669
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9576 | Val Loss: 0.9563
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9512 | Val Loss: 0.9509
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9509 | Val Loss: 0.9518
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9509 | Val Loss: 0.9519
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9510 | Val Loss: 0.9520
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9512 | Val Loss: 0.9522
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 33.064 MB uploadedwandb: | 32.971 MB of 33.064 MB uploadedwandb: / 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95132
wandb:   val_loss 0.95232
wandb: 
wandb: üöÄ View run vibrant-sweep-339 at: https://wandb.ai/7shoe/domShift-extensive/runs/o1zzlm7r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175219-o1zzlm7r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9513 | Val Loss: 0.9523
2025-03-26 17:53:08,952 - wandb.wandb_agent - INFO - Cleaning up finished run: o1zzlm7r
2025-03-26 17:53:09,514 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:53:09,514 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:53:09,517 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:53:14,530 - wandb.wandb_agent - INFO - Running runs: ['vjvu12sx']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175314-vjvu12sx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-346
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vjvu12sx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vjvu12sx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5103 | Val Loss: 1.4540
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5086 | Val Loss: 1.6046
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5127 | Val Loss: 1.2434
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2397 | Val Loss: 1.3472
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4359 | Val Loss: 1.4259
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3965 | Val Loss: 1.4022
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4124 | Val Loss: 1.4079
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3994 | Val Loss: 1.3939
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3833 | Val Loss: 1.3883
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñà‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÖ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40935
wandb:   val_loss 1.44146
wandb: 
wandb: üöÄ View run bright-sweep-346 at: https://wandb.ai/7shoe/domShift-extensive/runs/vjvu12sx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175314-vjvu12sx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4094 | Val Loss: 1.4415
2025-03-26 17:54:05,256 - wandb.wandb_agent - INFO - Cleaning up finished run: vjvu12sx
2025-03-26 17:54:14,506 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:54:14,507 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:54:14,510 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:54:19,523 - wandb.wandb_agent - INFO - Running runs: ['sa34cnjl']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175419-sa34cnjl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-352
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sa34cnjl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sa34cnjl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6038 | Val Loss: 1.2357
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1839 | Val Loss: 1.2049
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2451 | Val Loss: 1.2875
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2801 | Val Loss: 1.2050
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0250 | Val Loss: 0.9704
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9742 | Val Loss: 0.9717
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9747 | Val Loss: 0.9731
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9746 | Val Loss: 0.9736
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9746 | Val Loss: 0.9736
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.97467
wandb:   val_loss 0.97365
wandb: 
wandb: üöÄ View run neat-sweep-352 at: https://wandb.ai/7shoe/domShift-extensive/runs/sa34cnjl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175419-sa34cnjl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9747 | Val Loss: 0.9737
2025-03-26 17:55:05,193 - wandb.wandb_agent - INFO - Cleaning up finished run: sa34cnjl
2025-03-26 17:55:07,063 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:55:07,063 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:55:07,066 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:55:12,079 - wandb.wandb_agent - INFO - Running runs: ['lp5sy9yv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175512-lp5sy9yv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-360
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lp5sy9yv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: lp5sy9yv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4270 | Val Loss: 1.5260
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6032 | Val Loss: 1.6819
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6449 | Val Loss: 1.6053
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4756 | Val Loss: 1.1235
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1491 | Val Loss: 1.1680
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1648 | Val Loss: 1.1857
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2081 | Val Loss: 1.2375
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2598 | Val Loss: 1.2691
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2719 | Val Loss: 1.2774
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÜ‚ñà‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.28137
wandb:   val_loss 1.28137
wandb: 
wandb: üöÄ View run driven-sweep-360 at: https://wandb.ai/7shoe/domShift-extensive/runs/lp5sy9yv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175512-lp5sy9yv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2814 | Val Loss: 1.2814
2025-03-26 17:55:57,747 - wandb.wandb_agent - INFO - Cleaning up finished run: lp5sy9yv
2025-03-26 17:55:58,292 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:55:58,292 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:55:58,294 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:56:03,307 - wandb.wandb_agent - INFO - Running runs: ['qr62rps8']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175603-qr62rps8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-364
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qr62rps8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qr62rps8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.1267 | Val Loss: 2.1540
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2505 | Val Loss: 2.3652
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4369 | Val Loss: 2.5001
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5414 | Val Loss: 2.5824
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.6119 | Val Loss: 2.6424
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.6468 | Val Loss: 2.6740
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.7227 | Val Loss: 2.7740
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.8048 | Val Loss: 2.8338
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.8450 | Val Loss: 2.8553
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.85155
wandb:   val_loss 2.84683
wandb: 
wandb: üöÄ View run misty-sweep-364 at: https://wandb.ai/7shoe/domShift-extensive/runs/qr62rps8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175603-qr62rps8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.8515 | Val Loss: 2.8468
2025-03-26 17:56:43,910 - wandb.wandb_agent - INFO - Cleaning up finished run: qr62rps8
2025-03-26 17:56:44,558 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:56:44,559 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:56:44,561 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:56:49,573 - wandb.wandb_agent - INFO - Running runs: ['62cfr5m8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175650-62cfr5m8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-371
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/62cfr5m8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 62cfr5m8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5319 | Val Loss: 1.6195
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5816 | Val Loss: 1.4351
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4136 | Val Loss: 1.2593
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1765 | Val Loss: 1.1750
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1724 | Val Loss: 1.1743
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1711 | Val Loss: 1.1734
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1711 | Val Loss: 1.1736
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1711 | Val Loss: 1.1735
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1711 | Val Loss: 1.1737
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17111
wandb:   val_loss 1.17349
wandb: 
wandb: üöÄ View run different-sweep-371 at: https://wandb.ai/7shoe/domShift-extensive/runs/62cfr5m8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175650-62cfr5m8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1711 | Val Loss: 1.1735
2025-03-26 17:57:40,195 - wandb.wandb_agent - INFO - Cleaning up finished run: 62cfr5m8
2025-03-26 17:57:40,819 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:57:40,819 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:57:40,822 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:57:45,835 - wandb.wandb_agent - INFO - Running runs: ['e861vlvp']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175745-e861vlvp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-375
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/e861vlvp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: e861vlvp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3018 | Val Loss: 1.2327
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2410 | Val Loss: 1.2329
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2670 | Val Loss: 1.2781
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2968 | Val Loss: 1.3180
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2989 | Val Loss: 1.2753
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2778 | Val Loss: 1.1914
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1567 | Val Loss: 1.1807
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1948 | Val Loss: 1.1975
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1859 | Val Loss: 1.1903
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñÅ‚ñÉ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19452
wandb:   val_loss 1.19873
wandb: 
wandb: üöÄ View run zany-sweep-375 at: https://wandb.ai/7shoe/domShift-extensive/runs/e861vlvp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175745-e861vlvp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1945 | Val Loss: 1.1987
2025-03-26 17:58:26,392 - wandb.wandb_agent - INFO - Cleaning up finished run: e861vlvp
2025-03-26 17:58:26,972 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:58:26,972 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:58:26,975 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:58:31,987 - wandb.wandb_agent - INFO - Running runs: ['o34irvyi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175832-o34irvyi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-382
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o34irvyi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: o34irvyi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2254 | Val Loss: 1.1394
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2093 | Val Loss: 1.1836
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0465 | Val Loss: 0.9025
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9166 | Val Loss: 0.7397
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.7271 | Val Loss: 0.7106
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.7273 | Val Loss: 0.7563
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.7504 | Val Loss: 0.7127
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.7191 | Val Loss: 0.7516
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.7510 | Val Loss: 0.7227
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.71754
wandb:   val_loss 0.73393
wandb: 
wandb: üöÄ View run jumping-sweep-382 at: https://wandb.ai/7shoe/domShift-extensive/runs/o34irvyi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175832-o34irvyi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.7175 | Val Loss: 0.7339
2025-03-26 17:59:32,883 - wandb.wandb_agent - INFO - Cleaning up finished run: o34irvyi
2025-03-26 17:59:33,445 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:59:33,445 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:59:33,448 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:59:38,461 - wandb.wandb_agent - INFO - Running runs: ['9hvdahoe']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175938-9hvdahoe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-388
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9hvdahoe
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9hvdahoe
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.1673 | Val Loss: 1.8593
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7358 | Val Loss: 1.6202
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5267 | Val Loss: 1.3968
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3338 | Val Loss: 1.2562
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2288 | Val Loss: 1.2001
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1948 | Val Loss: 1.1891
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1893 | Val Loss: 1.1837
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1782 | Val Loss: 1.1695
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1643 | Val Loss: 1.1559
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15083
wandb:   val_loss 1.14441
wandb: 
wandb: üöÄ View run mild-sweep-388 at: https://wandb.ai/7shoe/domShift-extensive/runs/9hvdahoe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175938-9hvdahoe/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1508 | Val Loss: 1.1444
2025-03-26 18:00:08,859 - wandb.wandb_agent - INFO - Cleaning up finished run: 9hvdahoe
2025-03-26 18:00:09,950 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:09,950 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:00:09,953 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:00:14,966 - wandb.wandb_agent - INFO - Running runs: ['gey9tlqu']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180014-gey9tlqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-392
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gey9tlqu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gey9tlqu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.3754 | Val Loss: 3.9001
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.7695 | Val Loss: 3.5929
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.5695 | Val Loss: 3.4673
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.4734 | Val Loss: 3.4037
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.4367 | Val Loss: 3.3779
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.3948 | Val Loss: 3.3527
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.3695 | Val Loss: 3.3093
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.3454 | Val Loss: 3.3118
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.3301 | Val Loss: 3.2655
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.471 MB uploadedwandb: | 137.312 MB of 137.471 MB uploadedwandb: / 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.31043
wandb:   val_loss 3.28094
wandb: 
wandb: üöÄ View run sweet-sweep-392 at: https://wandb.ai/7shoe/domShift-extensive/runs/gey9tlqu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180014-gey9tlqu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.3104 | Val Loss: 3.2809
2025-03-26 18:01:31,094 - wandb.wandb_agent - INFO - Cleaning up finished run: gey9tlqu
2025-03-26 18:01:31,961 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:01:31,961 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:01:31,964 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:01:36,977 - wandb.wandb_agent - INFO - Running runs: ['nw9rgejp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180138-nw9rgejp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-400
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nw9rgejp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: nw9rgejp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6029 | Val Loss: 1.3669
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2937 | Val Loss: 1.3513
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2586 | Val Loss: 1.1535
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0865 | Val Loss: 1.0246
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0973 | Val Loss: 1.2546
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2477 | Val Loss: 1.1967
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1525 | Val Loss: 1.1186
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1059 | Val Loss: 1.0957
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0918 | Val Loss: 1.0863
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08589
wandb:   val_loss 1.08424
wandb: 
wandb: üöÄ View run efficient-sweep-400 at: https://wandb.ai/7shoe/domShift-extensive/runs/nw9rgejp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180138-nw9rgejp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0859 | Val Loss: 1.0842
2025-03-26 18:02:22,615 - wandb.wandb_agent - INFO - Cleaning up finished run: nw9rgejp
2025-03-26 18:02:23,267 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:02:23,267 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:02:23,270 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:02:28,283 - wandb.wandb_agent - INFO - Running runs: ['knj615hh']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180228-knj615hh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-405
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/knj615hh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: knj615hh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1141 | Val Loss: 0.8025
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.7202 | Val Loss: 0.6585
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.6485 | Val Loss: 0.6476
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.6490 | Val Loss: 0.6488
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.6480 | Val Loss: 0.6470
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.6464 | Val Loss: 0.6444
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.6410 | Val Loss: 0.6384
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.6394 | Val Loss: 0.6423
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.6474 | Val Loss: 0.6569
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.67091
wandb:   val_loss 0.69082
wandb: 
wandb: üöÄ View run icy-sweep-405 at: https://wandb.ai/7shoe/domShift-extensive/runs/knj615hh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180228-knj615hh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.6709 | Val Loss: 0.6908
2025-03-26 18:03:03,780 - wandb.wandb_agent - INFO - Cleaning up finished run: knj615hh
2025-03-26 18:03:05,032 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:03:05,035 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:03:05,038 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:03:10,050 - wandb.wandb_agent - INFO - Running runs: ['7cf6tf7t']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180311-7cf6tf7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-410
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7cf6tf7t
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7cf6tf7t
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6823 | Val Loss: 1.4315
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2505 | Val Loss: 1.0813
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0285 | Val Loss: 0.9808
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9662 | Val Loss: 0.9621
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9604 | Val Loss: 0.9603
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9790 | Val Loss: 1.0011
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0143 | Val Loss: 1.0244
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0315 | Val Loss: 1.0383
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0445 | Val Loss: 1.0533
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06313
wandb:   val_loss 1.0708
wandb: 
wandb: üöÄ View run rare-sweep-410 at: https://wandb.ai/7shoe/domShift-extensive/runs/7cf6tf7t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180311-7cf6tf7t/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0631 | Val Loss: 1.0708
2025-03-26 18:03:55,685 - wandb.wandb_agent - INFO - Cleaning up finished run: 7cf6tf7t
2025-03-26 18:03:56,235 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:03:56,235 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:03:56,240 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:04:01,252 - wandb.wandb_agent - INFO - Running runs: ['f0d024to']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180401-f0d024to
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-415
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f0d024to
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: f0d024to
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6862 | Val Loss: 1.3691
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3896 | Val Loss: 1.3948
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3823 | Val Loss: 1.3613
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3426 | Val Loss: 1.3957
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4014 | Val Loss: 1.3205
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3354 | Val Loss: 1.4219
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2498 | Val Loss: 1.1644
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1753 | Val Loss: 1.1796
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1736 | Val Loss: 1.1645
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16126
wandb:   val_loss 1.15928
wandb: 
wandb: üöÄ View run leafy-sweep-415 at: https://wandb.ai/7shoe/domShift-extensive/runs/f0d024to
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180401-f0d024to/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1613 | Val Loss: 1.1593
2025-03-26 18:04:46,967 - wandb.wandb_agent - INFO - Cleaning up finished run: f0d024to
2025-03-26 18:04:47,556 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:04:47,556 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:04:47,559 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 18:04:52,572 - wandb.wandb_agent - INFO - Running runs: ['oftnkh8u']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180453-oftnkh8u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-421
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/oftnkh8u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: oftnkh8u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.5229 | Val Loss: 2.4514
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.5001 | Val Loss: 2.5366
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.6482 | Val Loss: 2.7229
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7584 | Val Loss: 2.7865
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8002 | Val Loss: 2.8108
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.8149 | Val Loss: 2.8178
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.8194 | Val Loss: 2.8206
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.8217 | Val Loss: 2.8230
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.8212 | Val Loss: 2.8165
wandb: - 10.262 MB of 10.262 MB uploadedwandb: \ 10.262 MB of 10.291 MB uploadedwandb: | 10.262 MB of 10.291 MB uploadedwandb: / 10.291 MB of 10.291 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.78124
wandb:   val_loss 2.67211
wandb: 
wandb: üöÄ View run easy-sweep-421 at: https://wandb.ai/7shoe/domShift-extensive/runs/oftnkh8u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180453-oftnkh8u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.7812 | Val Loss: 2.6721
2025-03-26 18:05:38,248 - wandb.wandb_agent - INFO - Cleaning up finished run: oftnkh8u
2025-03-26 18:05:39,172 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:05:39,172 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:05:39,175 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 18:05:44,187 - wandb.wandb_agent - INFO - Running runs: ['207n0whl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180545-207n0whl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-426
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/207n0whl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 207n0whl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.6825 | Val Loss: 1.3330
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.2994 | Val Loss: 1.2382
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.2095 | Val Loss: 1.1619
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.1370 | Val Loss: 1.1062
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.0930 | Val Loss: 1.0789
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.0799 | Val Loss: 1.0822
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.0842 | Val Loss: 1.0837
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.0828 | Val Loss: 1.0801
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.0775 | Val Loss: 1.0729
wandb: - 137.027 MB of 137.027 MB uploadedwandb: \ 137.027 MB of 137.027 MB uploadedwandb: | 137.047 MB of 137.047 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07199
wandb:   val_loss 1.07213
wandb: 
wandb: üöÄ View run legendary-sweep-426 at: https://wandb.ai/7shoe/domShift-extensive/runs/207n0whl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180545-207n0whl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.0720 | Val Loss: 1.0721
2025-03-26 18:06:14,590 - wandb.wandb_agent - INFO - Cleaning up finished run: 207n0whl
2025-03-26 18:06:15,140 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:15,140 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:06:15,143 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 18:06:20,157 - wandb.wandb_agent - INFO - Running runs: ['7ueu8lzp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180621-7ueu8lzp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-432
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7ueu8lzp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7ueu8lzp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2676 | Val Loss: 1.0660
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0787 | Val Loss: 1.0956
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0964 | Val Loss: 1.1055
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1087 | Val Loss: 1.1109
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1125 | Val Loss: 1.1135
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0839 | Val Loss: 0.9770
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9399 | Val Loss: 0.9293
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9369 | Val Loss: 0.9410
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9446 | Val Loss: 0.9481
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95251
wandb:   val_loss 0.95605
wandb: 
wandb: üöÄ View run crimson-sweep-432 at: https://wandb.ai/7shoe/domShift-extensive/runs/7ueu8lzp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180621-7ueu8lzp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9525 | Val Loss: 0.9561
2025-03-26 18:07:00,720 - wandb.wandb_agent - INFO - Cleaning up finished run: 7ueu8lzp
2025-03-26 18:07:01,279 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:07:01,279 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:07:01,283 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:07:06,297 - wandb.wandb_agent - INFO - Running runs: ['j9qq0y20']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180706-j9qq0y20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-438
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/j9qq0y20
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: j9qq0y20
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2019 | Val Loss: 1.0687
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1695 | Val Loss: 1.1995
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2438 | Val Loss: 1.3178
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3533 | Val Loss: 1.3542
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3368 | Val Loss: 1.2581
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1251 | Val Loss: 1.0801
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1212 | Val Loss: 1.1518
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1480 | Val Loss: 1.1325
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1257 | Val Loss: 1.1197
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11651
wandb:   val_loss 1.11478
wandb: 
wandb: üöÄ View run stilted-sweep-438 at: https://wandb.ai/7shoe/domShift-extensive/runs/j9qq0y20
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180706-j9qq0y20/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1165 | Val Loss: 1.1148
2025-03-26 18:07:51,948 - wandb.wandb_agent - INFO - Cleaning up finished run: j9qq0y20
2025-03-26 18:07:52,879 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:07:52,880 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:07:52,883 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:07:57,895 - wandb.wandb_agent - INFO - Running runs: ['bo5ft2pz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180759-bo5ft2pz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-443
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bo5ft2pz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: bo5ft2pz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6552 | Val Loss: 1.4262
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3248 | Val Loss: 1.2818
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2372 | Val Loss: 1.1939
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2178 | Val Loss: 1.2564
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2853 | Val Loss: 1.3157
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3366 | Val Loss: 1.3553
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3633 | Val Loss: 1.3653
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3685 | Val Loss: 1.3718
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3730 | Val Loss: 1.3724
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36776
wandb:   val_loss 1.36652
wandb: 
wandb: üöÄ View run super-sweep-443 at: https://wandb.ai/7shoe/domShift-extensive/runs/bo5ft2pz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180759-bo5ft2pz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3678 | Val Loss: 1.3665
2025-03-26 18:08:42,899 - wandb.wandb_agent - INFO - Cleaning up finished run: bo5ft2pz
2025-03-26 18:08:43,741 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:08:43,742 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:08:43,744 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:08:48,755 - wandb.wandb_agent - INFO - Running runs: ['opw6nrdv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180849-opw6nrdv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-449
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/opw6nrdv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: opw6nrdv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9030 | Val Loss: 1.7642
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.7511 | Val Loss: 1.7239
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6989 | Val Loss: 1.6726
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6432 | Val Loss: 1.5934
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5390 | Val Loss: 1.4682
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4051 | Val Loss: 1.3279
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2928 | Val Loss: 1.2733
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2651 | Val Loss: 1.3676
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3819 | Val Loss: 1.3030
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.991 MB uploadedwandb: | 32.971 MB of 32.991 MB uploadedwandb: / 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31131
wandb:   val_loss 1.32976
wandb: 
wandb: üöÄ View run lucky-sweep-449 at: https://wandb.ai/7shoe/domShift-extensive/runs/opw6nrdv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180849-opw6nrdv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3113 | Val Loss: 1.3298
2025-03-26 18:09:24,240 - wandb.wandb_agent - INFO - Cleaning up finished run: opw6nrdv
2025-03-26 18:09:24,726 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:09:24,726 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:09:24,729 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:09:29,740 - wandb.wandb_agent - INFO - Running runs: ['1tmdf2eu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180929-1tmdf2eu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-453
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1tmdf2eu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1tmdf2eu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9237 | Val Loss: 2.4140
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.2589 | Val Loss: 2.2492
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.2067 | Val Loss: 2.0658
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.0077 | Val Loss: 1.9562
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.9296 | Val Loss: 1.9106
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.9028 | Val Loss: 1.8980
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.8970 | Val Loss: 1.8976
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.8990 | Val Loss: 1.9008
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.9024 | Val Loss: 1.9043
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.90556
wandb:   val_loss 1.90702
wandb: 
wandb: üöÄ View run lively-sweep-453 at: https://wandb.ai/7shoe/domShift-extensive/runs/1tmdf2eu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180929-1tmdf2eu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.9056 | Val Loss: 1.9070
2025-03-26 18:10:15,418 - wandb.wandb_agent - INFO - Cleaning up finished run: 1tmdf2eu
2025-03-26 18:10:15,944 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:10:15,944 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:10:15,947 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:10:20,958 - wandb.wandb_agent - INFO - Running runs: ['edzn1fgv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181021-edzn1fgv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-459
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/edzn1fgv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: edzn1fgv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7922 | Val Loss: 1.4108
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3326 | Val Loss: 1.2392
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2041 | Val Loss: 1.1726
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1551 | Val Loss: 1.1285
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1165 | Val Loss: 1.1119
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1217 | Val Loss: 1.1388
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1607 | Val Loss: 1.1779
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1853 | Val Loss: 1.1832
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1695 | Val Loss: 1.1541
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1456
wandb:   val_loss 1.13847
wandb: 
wandb: üöÄ View run glad-sweep-459 at: https://wandb.ai/7shoe/domShift-extensive/runs/edzn1fgv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181021-edzn1fgv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1456 | Val Loss: 1.1385
2025-03-26 18:11:01,507 - wandb.wandb_agent - INFO - Cleaning up finished run: edzn1fgv
2025-03-26 18:11:02,123 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:11:02,123 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:11:02,126 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:11:07,140 - wandb.wandb_agent - INFO - Running runs: ['utogmnwj']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181107-utogmnwj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-464
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/utogmnwj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: utogmnwj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7220 | Val Loss: 1.4530
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4319 | Val Loss: 1.4370
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4447 | Val Loss: 1.4516
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4784 | Val Loss: 1.5044
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5199 | Val Loss: 1.5255
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5189 | Val Loss: 1.5107
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5021 | Val Loss: 1.4899
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4643 | Val Loss: 1.4384
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4073 | Val Loss: 1.4065
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.41079
wandb:   val_loss 1.42277
wandb: 
wandb: üöÄ View run fearless-sweep-464 at: https://wandb.ai/7shoe/domShift-extensive/runs/utogmnwj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181107-utogmnwj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4108 | Val Loss: 1.4228
2025-03-26 18:11:57,873 - wandb.wandb_agent - INFO - Cleaning up finished run: utogmnwj
2025-03-26 18:11:58,881 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:11:58,881 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:11:58,884 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:12:03,896 - wandb.wandb_agent - INFO - Running runs: ['6bjvkvpp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181204-6bjvkvpp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-469
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6bjvkvpp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6bjvkvpp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3895 | Val Loss: 1.1960
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0789 | Val Loss: 1.0389
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0415 | Val Loss: 1.0435
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0342 | Val Loss: 1.0161
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9960 | Val Loss: 0.9758
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9656 | Val Loss: 0.9582
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9567 | Val Loss: 0.9545
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9512 | Val Loss: 0.9458
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9415 | Val Loss: 0.9370
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93468
wandb:   val_loss 0.93177
wandb: 
wandb: üöÄ View run eager-sweep-469 at: https://wandb.ai/7shoe/domShift-extensive/runs/6bjvkvpp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181204-6bjvkvpp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9347 | Val Loss: 0.9318
2025-03-26 18:12:59,679 - wandb.wandb_agent - INFO - Cleaning up finished run: 6bjvkvpp
2025-03-26 18:13:00,542 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:13:00,542 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:13:00,546 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:13:05,560 - wandb.wandb_agent - INFO - Running runs: ['zhs1aolg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181306-zhs1aolg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-474
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zhs1aolg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zhs1aolg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6834 | Val Loss: 1.6646
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6050 | Val Loss: 1.5273
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4796 | Val Loss: 1.4420
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4364 | Val Loss: 1.4287
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4361 | Val Loss: 1.4340
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4242 | Val Loss: 1.4331
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4533 | Val Loss: 1.4704
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4799 | Val Loss: 1.4751
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4365 | Val Loss: 1.4341
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.333 MB uploadedwandb: | 137.314 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.43415
wandb:   val_loss 1.42107
wandb: 
wandb: üöÄ View run valiant-sweep-474 at: https://wandb.ai/7shoe/domShift-extensive/runs/zhs1aolg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181306-zhs1aolg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4342 | Val Loss: 1.4211
2025-03-26 18:13:46,117 - wandb.wandb_agent - INFO - Cleaning up finished run: zhs1aolg
2025-03-26 18:13:46,686 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:13:46,686 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:13:46,690 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:13:51,703 - wandb.wandb_agent - INFO - Running runs: ['ieivhyze']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181351-ieivhyze
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-479
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ieivhyze
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ieivhyze
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0479 | Val Loss: 0.9377
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0000 | Val Loss: 1.1074
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0659 | Val Loss: 0.8198
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8369 | Val Loss: 0.8594
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8602 | Val Loss: 0.8663
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8657 | Val Loss: 0.8699
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8735 | Val Loss: 0.8770
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8808 | Val Loss: 0.8840
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8884 | Val Loss: 0.8970
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÜ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.90365
wandb:   val_loss 0.90361
wandb: 
wandb: üöÄ View run balmy-sweep-479 at: https://wandb.ai/7shoe/domShift-extensive/runs/ieivhyze
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181351-ieivhyze/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9036 | Val Loss: 0.9036
2025-03-26 18:15:02,650 - wandb.wandb_agent - INFO - Cleaning up finished run: ieivhyze
2025-03-26 18:15:03,210 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:15:03,210 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:15:03,213 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:15:08,226 - wandb.wandb_agent - INFO - Running runs: ['1e5rj1qi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181508-1e5rj1qi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-487
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1e5rj1qi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1e5rj1qi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1741 | Val Loss: 1.0137
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0474 | Val Loss: 1.0922
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0988 | Val Loss: 1.1017
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1099 | Val Loss: 1.1164
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1219 | Val Loss: 1.1283
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1326 | Val Loss: 1.1370
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1292 | Val Loss: 1.1206
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1315 | Val Loss: 1.1331
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1502 | Val Loss: 1.1072
wandb: - 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11499
wandb:   val_loss 1.13251
wandb: 
wandb: üöÄ View run astral-sweep-487 at: https://wandb.ai/7shoe/domShift-extensive/runs/1e5rj1qi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181508-1e5rj1qi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1150 | Val Loss: 1.1325
2025-03-26 18:16:14,171 - wandb.wandb_agent - INFO - Cleaning up finished run: 1e5rj1qi
2025-03-26 18:16:14,685 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:16:14,685 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:16:14,688 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:16:19,701 - wandb.wandb_agent - INFO - Running runs: ['66a17o21']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181619-66a17o21
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-493
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/66a17o21
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 66a17o21
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2903 | Val Loss: 0.9590
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9493 | Val Loss: 0.8915
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.8719 | Val Loss: 0.9376
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0524 | Val Loss: 1.1445
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1444 | Val Loss: 1.0830
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0826 | Val Loss: 1.0818
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0779 | Val Loss: 1.0790
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0816 | Val Loss: 1.0818
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0791 | Val Loss: 1.0811
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08105
wandb:   val_loss 1.07987
wandb: 
wandb: üöÄ View run dazzling-sweep-493 at: https://wandb.ai/7shoe/domShift-extensive/runs/66a17o21
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181619-66a17o21/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0810 | Val Loss: 1.0799
2025-03-26 18:17:05,322 - wandb.wandb_agent - INFO - Cleaning up finished run: 66a17o21
2025-03-26 18:17:05,844 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:17:05,844 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:17:05,849 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:17:10,863 - wandb.wandb_agent - INFO - Running runs: ['4kf9nh3b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181712-4kf9nh3b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-498
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4kf9nh3b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4kf9nh3b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7615 | Val Loss: 2.5977
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.4158 | Val Loss: 2.1727
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1322 | Val Loss: 2.1499
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2285 | Val Loss: 2.3162
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3803 | Val Loss: 2.3813
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4308 | Val Loss: 2.4933
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5424 | Val Loss: 2.5899
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6236 | Val Loss: 2.6592
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6927 | Val Loss: 2.7270
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.031 MB of 137.031 MB uploadedwandb: | 137.190 MB of 137.190 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà
wandb:   val_loss ‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.75727
wandb:   val_loss 2.78608
wandb: 
wandb: üöÄ View run atomic-sweep-498 at: https://wandb.ai/7shoe/domShift-extensive/runs/4kf9nh3b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181712-4kf9nh3b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7573 | Val Loss: 2.7861
2025-03-26 18:18:26,965 - wandb.wandb_agent - INFO - Cleaning up finished run: 4kf9nh3b
2025-03-26 18:18:27,497 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:18:27,497 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:18:27,500 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:18:32,512 - wandb.wandb_agent - INFO - Running runs: ['2vw2n1bc']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181833-2vw2n1bc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-503
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2vw2n1bc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2vw2n1bc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4646 | Val Loss: 1.3428
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3935 | Val Loss: 1.4339
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4518 | Val Loss: 1.5178
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6202 | Val Loss: 1.7619
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3546 | Val Loss: 1.2364
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2362 | Val Loss: 1.2331
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2562 | Val Loss: 1.2848
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3008 | Val Loss: 1.3099
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3089 | Val Loss: 1.3038
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÇ‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.29616
wandb:   val_loss 1.28813
wandb: 
wandb: üöÄ View run dazzling-sweep-503 at: https://wandb.ai/7shoe/domShift-extensive/runs/2vw2n1bc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181833-2vw2n1bc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2962 | Val Loss: 1.2881
2025-03-26 18:19:48,613 - wandb.wandb_agent - INFO - Cleaning up finished run: 2vw2n1bc
2025-03-26 18:19:49,197 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:19:49,197 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:19:49,200 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:19:54,212 - wandb.wandb_agent - INFO - Running runs: ['7zgq0ku1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181955-7zgq0ku1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-512
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7zgq0ku1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7zgq0ku1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 3.6863 | Val Loss: 2.9813
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.7951 | Val Loss: 2.5694
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.5098 | Val Loss: 2.3980
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.3949 | Val Loss: 2.3010
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 2.3356 | Val Loss: 2.2640
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 2.3010 | Val Loss: 2.2397
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 2.2731 | Val Loss: 2.2348
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.2501 | Val Loss: 2.2017
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.2258 | Val Loss: 2.1746
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 33.058 MB uploadedwandb: | 32.965 MB of 33.058 MB uploadedwandb: / 33.058 MB of 33.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.20613
wandb:   val_loss 2.17036
wandb: 
wandb: üöÄ View run crimson-sweep-512 at: https://wandb.ai/7shoe/domShift-extensive/runs/7zgq0ku1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181955-7zgq0ku1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.2061 | Val Loss: 2.1704
2025-03-26 18:20:44,904 - wandb.wandb_agent - INFO - Cleaning up finished run: 7zgq0ku1
2025-03-26 18:20:45,995 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:20:45,995 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:20:45,998 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.4
2025-03-26 18:20:51,010 - wandb.wandb_agent - INFO - Running runs: ['lhniguyf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182051-lhniguyf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-518
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lhniguyf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: lhniguyf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2793 | Val Loss: 2.6865
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.5800 | Val Loss: 2.5006
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.4503 | Val Loss: 2.3836
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.3423 | Val Loss: 2.2967
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.2842 | Val Loss: 2.2609
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.2377 | Val Loss: 2.2344
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.2666 | Val Loss: 2.2925
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.3080 | Val Loss: 2.3192
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3325 | Val Loss: 2.3274
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.872 MB uploadedwandb: | 32.853 MB of 32.872 MB uploadedwandb: / 32.872 MB of 32.872 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.3409
wandb:   val_loss 2.36005
wandb: 
wandb: üöÄ View run celestial-sweep-518 at: https://wandb.ai/7shoe/domShift-extensive/runs/lhniguyf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182051-lhniguyf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.3409 | Val Loss: 2.3601
2025-03-26 18:21:21,383 - wandb.wandb_agent - INFO - Cleaning up finished run: lhniguyf
2025-03-26 18:21:21,847 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:21:21,847 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:21:21,850 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:21:26,861 - wandb.wandb_agent - INFO - Running runs: ['2b1vfog3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182127-2b1vfog3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-521
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2b1vfog3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2b1vfog3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1136 | Val Loss: 2.6602
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6119 | Val Loss: 2.5673
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6113 | Val Loss: 2.7032
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7958 | Val Loss: 2.8622
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8633 | Val Loss: 2.8317
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.7811 | Val Loss: 2.7268
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6678 | Val Loss: 2.6079
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5923 | Val Loss: 2.5813
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5726 | Val Loss: 2.5590
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.55018
wandb:   val_loss 2.54235
wandb: 
wandb: üöÄ View run sweet-sweep-521 at: https://wandb.ai/7shoe/domShift-extensive/runs/2b1vfog3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182127-2b1vfog3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5502 | Val Loss: 2.5424
2025-03-26 18:22:27,767 - wandb.wandb_agent - INFO - Cleaning up finished run: 2b1vfog3
2025-03-26 18:22:28,696 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:22:28,696 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:22:28,699 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:22:33,712 - wandb.wandb_agent - INFO - Running runs: ['j3qjvzqf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182234-j3qjvzqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-528
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/j3qjvzqf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: j3qjvzqf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0446 | Val Loss: 0.8277
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.8090 | Val Loss: 0.8140
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8173 | Val Loss: 0.7895
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8711 | Val Loss: 0.9693
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9329 | Val Loss: 0.9214
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9309 | Val Loss: 0.9241
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9239 | Val Loss: 0.9298
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9257 | Val Loss: 0.9244
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9261 | Val Loss: 0.9247
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92526
wandb:   val_loss 0.92644
wandb: 
wandb: üöÄ View run trim-sweep-528 at: https://wandb.ai/7shoe/domShift-extensive/runs/j3qjvzqf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182234-j3qjvzqf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9253 | Val Loss: 0.9264
2025-03-26 18:23:44,750 - wandb.wandb_agent - INFO - Cleaning up finished run: j3qjvzqf
2025-03-26 18:23:45,699 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:23:45,699 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:23:45,703 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:23:50,715 - wandb.wandb_agent - INFO - Running runs: ['wsh5w2p0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182350-wsh5w2p0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-533
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wsh5w2p0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: wsh5w2p0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6312 | Val Loss: 1.5747
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5185 | Val Loss: 1.4955
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4855 | Val Loss: 1.4775
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4832 | Val Loss: 1.4862
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4908 | Val Loss: 1.5077
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4132 | Val Loss: 1.4024
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3038 | Val Loss: 1.3011
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2947 | Val Loss: 1.3001
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3075 | Val Loss: 1.3104
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31069
wandb:   val_loss 1.30991
wandb: 
wandb: üöÄ View run grateful-sweep-533 at: https://wandb.ai/7shoe/domShift-extensive/runs/wsh5w2p0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182350-wsh5w2p0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3107 | Val Loss: 1.3099
2025-03-26 18:24:56,625 - wandb.wandb_agent - INFO - Cleaning up finished run: wsh5w2p0
2025-03-26 18:24:57,276 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:24:57,276 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:24:57,279 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:25:02,291 - wandb.wandb_agent - INFO - Running runs: ['ia338uem']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182503-ia338uem
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-541
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ia338uem
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ia338uem
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4315 | Val Loss: 1.3159
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3176 | Val Loss: 1.3851
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2337 | Val Loss: 1.2414
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2324 | Val Loss: 1.2690
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2086 | Val Loss: 1.2361
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2783 | Val Loss: 1.2898
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2974 | Val Loss: 1.3042
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3071 | Val Loss: 1.3103
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3117 | Val Loss: 1.3133
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÖ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3139
wandb:   val_loss 1.31515
wandb: 
wandb: üöÄ View run distinctive-sweep-541 at: https://wandb.ai/7shoe/domShift-extensive/runs/ia338uem
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182503-ia338uem/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3139 | Val Loss: 1.3152
2025-03-26 18:26:19,195 - wandb.wandb_agent - INFO - Cleaning up finished run: ia338uem
2025-03-26 18:26:19,998 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:26:19,998 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:26:20,001 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:26:25,014 - wandb.wandb_agent - INFO - Running runs: ['4lpp16sk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182626-4lpp16sk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-549
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4lpp16sk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4lpp16sk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.6073 | Val Loss: 1.1641
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.0333 | Val Loss: 0.9514
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 0.9400 | Val Loss: 0.9356
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 0.9383 | Val Loss: 0.9368
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 0.9426 | Val Loss: 0.9569
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 0.9834 | Val Loss: 1.0109
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0235 | Val Loss: 1.0237
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0406 | Val Loss: 1.0767
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.1125 | Val Loss: 1.1256
wandb: - 32.850 MB of 32.850 MB uploadedwandb: \ 32.850 MB of 32.850 MB uploadedwandb: | 32.869 MB of 32.869 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09452
wandb:   val_loss 1.07759
wandb: 
wandb: üöÄ View run soft-sweep-549 at: https://wandb.ai/7shoe/domShift-extensive/runs/4lpp16sk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182626-4lpp16sk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.0945 | Val Loss: 1.0776
2025-03-26 18:26:55,419 - wandb.wandb_agent - INFO - Cleaning up finished run: 4lpp16sk
2025-03-26 18:26:56,150 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:26:56,151 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:26:56,154 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:27:01,167 - wandb.wandb_agent - INFO - Running runs: ['wpn2mj7p']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182702-wpn2mj7p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-553
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wpn2mj7p
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wpn2mj7p
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4002 | Val Loss: 1.2870
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3601 | Val Loss: 1.3944
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3976 | Val Loss: 1.4086
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4121 | Val Loss: 1.4273
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4288 | Val Loss: 1.4221
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4235 | Val Loss: 1.4315
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4396 | Val Loss: 1.4447
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4557 | Val Loss: 1.4642
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4711 | Val Loss: 1.4777
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4914
wandb:   val_loss 1.50509
wandb: 
wandb: üöÄ View run rose-sweep-553 at: https://wandb.ai/7shoe/domShift-extensive/runs/wpn2mj7p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182702-wpn2mj7p/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4914 | Val Loss: 1.5051
2025-03-26 18:27:56,958 - wandb.wandb_agent - INFO - Cleaning up finished run: wpn2mj7p
2025-03-26 18:27:57,801 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:27:57,801 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:27:57,804 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:28:02,815 - wandb.wandb_agent - INFO - Running runs: ['ky4sr3un']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182802-ky4sr3un
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-558
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ky4sr3un
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ky4sr3un
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1308 | Val Loss: 1.0575
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0939 | Val Loss: 1.0977
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0999 | Val Loss: 1.1095
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1357 | Val Loss: 1.1731
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2402 | Val Loss: 1.3277
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5469 | Val Loss: 1.7557
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2892 | Val Loss: 0.8947
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9135 | Val Loss: 0.9897
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9796 | Val Loss: 0.9869
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.97346
wandb:   val_loss 0.98229
wandb: 
wandb: üöÄ View run dashing-sweep-558 at: https://wandb.ai/7shoe/domShift-extensive/runs/ky4sr3un
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182802-ky4sr3un/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9735 | Val Loss: 0.9823
2025-03-26 18:29:13,820 - wandb.wandb_agent - INFO - Cleaning up finished run: ky4sr3un
2025-03-26 18:29:14,610 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:29:14,610 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:29:14,612 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:29:19,623 - wandb.wandb_agent - INFO - Running runs: ['pz2mrvii']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182919-pz2mrvii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-564
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pz2mrvii
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pz2mrvii
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1456 | Val Loss: 0.8978
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9314 | Val Loss: 0.9703
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9560 | Val Loss: 0.9458
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9405 | Val Loss: 0.9249
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9383 | Val Loss: 0.9366
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9355 | Val Loss: 0.9441
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9362 | Val Loss: 0.9420
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9374 | Val Loss: 0.9379
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9382 | Val Loss: 0.9348
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93866
wandb:   val_loss 0.93293
wandb: 
wandb: üöÄ View run sage-sweep-564 at: https://wandb.ai/7shoe/domShift-extensive/runs/pz2mrvii
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182919-pz2mrvii/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9387 | Val Loss: 0.9329
2025-03-26 18:30:25,569 - wandb.wandb_agent - INFO - Cleaning up finished run: pz2mrvii
2025-03-26 18:30:26,006 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:26,006 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:30:26,009 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:30:31,021 - wandb.wandb_agent - INFO - Running runs: ['6dubpbrz']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183030-6dubpbrz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-573
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6dubpbrz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6dubpbrz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7766 | Val Loss: 1.5392
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5175 | Val Loss: 1.5363
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5521 | Val Loss: 1.5812
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6046 | Val Loss: 1.6236
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6464 | Val Loss: 1.6693
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6764 | Val Loss: 1.6756
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6689 | Val Loss: 1.6548
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6352 | Val Loss: 1.6053
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5853 | Val Loss: 1.5540
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.51777
wandb:   val_loss 1.48063
wandb: 
wandb: üöÄ View run brisk-sweep-573 at: https://wandb.ai/7shoe/domShift-extensive/runs/6dubpbrz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183030-6dubpbrz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5178 | Val Loss: 1.4806
2025-03-26 18:31:06,517 - wandb.wandb_agent - INFO - Cleaning up finished run: 6dubpbrz
2025-03-26 18:31:07,079 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:31:07,079 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:31:07,082 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:31:12,095 - wandb.wandb_agent - INFO - Running runs: ['jh3ilefe']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183113-jh3ilefe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-578
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jh3ilefe
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jh3ilefe
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5265 | Val Loss: 1.2313
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1564 | Val Loss: 1.1625
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1694 | Val Loss: 1.1855
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1777 | Val Loss: 1.1730
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1767 | Val Loss: 1.1787
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1751 | Val Loss: 1.1747
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1756 | Val Loss: 1.1748
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1749 | Val Loss: 1.1771
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1748 | Val Loss: 1.1755
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.473 MB uploadedwandb: | 137.314 MB of 137.473 MB uploadedwandb: / 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17487
wandb:   val_loss 1.17379
wandb: 
wandb: üöÄ View run dutiful-sweep-578 at: https://wandb.ai/7shoe/domShift-extensive/runs/jh3ilefe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183113-jh3ilefe/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1749 | Val Loss: 1.1738
2025-03-26 18:32:23,053 - wandb.wandb_agent - INFO - Cleaning up finished run: jh3ilefe
2025-03-26 18:32:23,916 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:32:23,916 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:32:23,918 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:32:28,931 - wandb.wandb_agent - INFO - Running runs: ['5d3os9c2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183228-5d3os9c2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-584
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5d3os9c2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5d3os9c2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3756 | Val Loss: 1.1929
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1730 | Val Loss: 1.1801
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1886 | Val Loss: 1.1941
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1987 | Val Loss: 1.2027
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2068 | Val Loss: 1.2104
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2137 | Val Loss: 1.2165
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2190 | Val Loss: 1.2210
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2225 | Val Loss: 1.2237
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2247 | Val Loss: 1.2257
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22919
wandb:   val_loss 1.23353
wandb: 
wandb: üöÄ View run unique-sweep-584 at: https://wandb.ai/7shoe/domShift-extensive/runs/5d3os9c2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183228-5d3os9c2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2292 | Val Loss: 1.2335
2025-03-26 18:33:14,586 - wandb.wandb_agent - INFO - Cleaning up finished run: 5d3os9c2
2025-03-26 18:33:15,095 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:33:15,095 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:33:15,098 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:33:20,109 - wandb.wandb_agent - INFO - Running runs: ['hcu5rxcw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183321-hcu5rxcw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-588
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hcu5rxcw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hcu5rxcw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.5189 | Val Loss: 1.3459
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.3795 | Val Loss: 1.4327
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.4767 | Val Loss: 1.5222
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.5682 | Val Loss: 1.5906
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.5979 | Val Loss: 1.6069
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.6239 | Val Loss: 1.4532
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.3819 | Val Loss: 1.3647
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.3651 | Val Loss: 1.3583
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.3491 | Val Loss: 1.3398
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.418 MB uploadedwandb: | 137.259 MB of 137.418 MB uploadedwandb: / 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33694
wandb:   val_loss 1.3317
wandb: 
wandb: üöÄ View run jolly-sweep-588 at: https://wandb.ai/7shoe/domShift-extensive/runs/hcu5rxcw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183321-hcu5rxcw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.3369 | Val Loss: 1.3317
2025-03-26 18:34:31,120 - wandb.wandb_agent - INFO - Cleaning up finished run: hcu5rxcw
2025-03-26 18:34:32,018 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:34:32,018 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:34:32,021 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.4
2025-03-26 18:34:37,033 - wandb.wandb_agent - INFO - Running runs: ['pj04d35u']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183438-pj04d35u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-597
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pj04d35u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: pj04d35u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.7710 | Val Loss: 2.4954
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.5291 | Val Loss: 2.5700
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.4614 | Val Loss: 2.3232
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.2870 | Val Loss: 2.2788
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.2904 | Val Loss: 2.2859
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.2699 | Val Loss: 2.2440
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.2283 | Val Loss: 2.2098
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.2105 | Val Loss: 2.2132
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.2251 | Val Loss: 2.2281
wandb: - 32.852 MB of 32.852 MB uploadedwandb: \ 32.852 MB of 32.871 MB uploadedwandb: | 32.852 MB of 32.871 MB uploadedwandb: / 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.21371
wandb:   val_loss 2.19348
wandb: 
wandb: üöÄ View run sleek-sweep-597 at: https://wandb.ai/7shoe/domShift-extensive/runs/pj04d35u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183438-pj04d35u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.2137 | Val Loss: 2.1935
2025-03-26 18:35:07,581 - wandb.wandb_agent - INFO - Cleaning up finished run: pj04d35u
2025-03-26 18:35:08,109 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:35:08,109 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:35:08,111 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:35:13,124 - wandb.wandb_agent - INFO - Running runs: ['nqvt2van']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183512-nqvt2van
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-601
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nqvt2van
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: nqvt2van
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9019 | Val Loss: 1.8614
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7416 | Val Loss: 1.6521
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6371 | Val Loss: 1.6212
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6392 | Val Loss: 1.6462
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6479 | Val Loss: 1.6423
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6063 | Val Loss: 1.5574
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5505 | Val Loss: 1.5535
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5629 | Val Loss: 1.5618
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5473 | Val Loss: 1.5140
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.48902
wandb:   val_loss 1.46561
wandb: 
wandb: üöÄ View run warm-sweep-601 at: https://wandb.ai/7shoe/domShift-extensive/runs/nqvt2van
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183512-nqvt2van/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4890 | Val Loss: 1.4656
2025-03-26 18:35:43,537 - wandb.wandb_agent - INFO - Cleaning up finished run: nqvt2van
2025-03-26 18:35:44,087 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:35:44,087 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:35:44,090 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:35:49,102 - wandb.wandb_agent - INFO - Running runs: ['4nb0czg9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183550-4nb0czg9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-605
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4nb0czg9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4nb0czg9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.8613 | Val Loss: 1.2385
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1000 | Val Loss: 1.0835
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1087 | Val Loss: 1.1095
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1209 | Val Loss: 1.1290
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1359 | Val Loss: 1.1447
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1565 | Val Loss: 1.1691
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1732 | Val Loss: 1.1293
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0399 | Val Loss: 0.9550
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9878 | Val Loss: 0.9994
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03051
wandb:   val_loss 1.05217
wandb: 
wandb: üöÄ View run dauntless-sweep-605 at: https://wandb.ai/7shoe/domShift-extensive/runs/4nb0czg9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183550-4nb0czg9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0305 | Val Loss: 1.0522
2025-03-26 18:36:24,618 - wandb.wandb_agent - INFO - Cleaning up finished run: 4nb0czg9
2025-03-26 18:36:25,455 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:36:25,455 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:36:25,458 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimCLR --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 18:36:30,471 - wandb.wandb_agent - INFO - Running runs: ['89lotx9f']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183631-89lotx9f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-609
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/89lotx9f
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 89lotx9f
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.0484 | Val Loss: 3.1224
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.8753 | Val Loss: 2.5212
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.3216 | Val Loss: 2.0551
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8712 | Val Loss: 1.6803
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6175 | Val Loss: 1.4780
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4427 | Val Loss: 1.2965
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2718 | Val Loss: 1.2003
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1621 | Val Loss: 1.1273
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1145 | Val Loss: 1.0683
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07136
wandb:   val_loss 1.03469
wandb: 
wandb: üöÄ View run cerulean-sweep-609 at: https://wandb.ai/7shoe/domShift-extensive/runs/89lotx9f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183631-89lotx9f/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0714 | Val Loss: 1.0347
2025-03-26 18:37:11,024 - wandb.wandb_agent - INFO - Cleaning up finished run: 89lotx9f
2025-03-26 18:37:11,853 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:37:11,853 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:37:11,855 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:37:16,868 - wandb.wandb_agent - INFO - Running runs: ['1s3kipef']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183718-1s3kipef
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-613
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1s3kipef
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1s3kipef
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.6349 | Val Loss: 3.3623
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.1658 | Val Loss: 3.1016
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.0253 | Val Loss: 2.9426
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.9067 | Val Loss: 2.8410
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7830 | Val Loss: 2.7034
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5860 | Val Loss: 2.5034
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4550 | Val Loss: 2.4125
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3865 | Val Loss: 2.3678
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3599 | Val Loss: 2.3529
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.35069
wandb:   val_loss 2.3485
wandb: 
wandb: üöÄ View run comfy-sweep-613 at: https://wandb.ai/7shoe/domShift-extensive/runs/1s3kipef
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183718-1s3kipef/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3507 | Val Loss: 2.3485
2025-03-26 18:38:07,600 - wandb.wandb_agent - INFO - Cleaning up finished run: 1s3kipef
2025-03-26 18:38:08,238 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:38:08,239 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:38:08,242 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 18:38:13,256 - wandb.wandb_agent - INFO - Running runs: ['3eobruam']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183814-3eobruam
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-619
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3eobruam
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3eobruam
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2544 | Val Loss: 1.0496
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9770 | Val Loss: 0.9755
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9800 | Val Loss: 0.9731
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9731 | Val Loss: 0.9717
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9692 | Val Loss: 0.9675
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9659 | Val Loss: 0.9651
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9653 | Val Loss: 0.9655
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9659 | Val Loss: 0.9663
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9665 | Val Loss: 0.9669
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.96708
wandb:   val_loss 0.96742
wandb: 
wandb: üöÄ View run lunar-sweep-619 at: https://wandb.ai/7shoe/domShift-extensive/runs/3eobruam
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183814-3eobruam/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9671 | Val Loss: 0.9674
2025-03-26 18:39:24,240 - wandb.wandb_agent - INFO - Cleaning up finished run: 3eobruam
2025-03-26 18:39:24,689 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:39:24,689 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:39:24,692 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:39:29,705 - wandb.wandb_agent - INFO - Running runs: ['6y4swf4s']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183931-6y4swf4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-629
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6y4swf4s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 6y4swf4s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 3.3465 | Val Loss: 2.7499
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.6440 | Val Loss: 2.5308
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.4837 | Val Loss: 2.4496
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.4334 | Val Loss: 2.4068
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 2.3718 | Val Loss: 2.3121
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 2.2540 | Val Loss: 2.2195
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 2.1992 | Val Loss: 2.1816
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.1728 | Val Loss: 2.1653
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.1694 | Val Loss: 2.1741
wandb: - 32.848 MB of 32.848 MB uploadedwandb: \ 32.848 MB of 32.848 MB uploadedwandb: | 32.868 MB of 32.868 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.18512
wandb:   val_loss 2.20356
wandb: 
wandb: üöÄ View run lemon-sweep-629 at: https://wandb.ai/7shoe/domShift-extensive/runs/6y4swf4s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183931-6y4swf4s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.1851 | Val Loss: 2.2036
2025-03-26 18:40:05,201 - wandb.wandb_agent - INFO - Cleaning up finished run: 6y4swf4s
2025-03-26 18:40:05,819 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:40:05,820 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:40:05,823 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 18:40:10,837 - wandb.wandb_agent - INFO - Running runs: ['c6b2vxbm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184012-c6b2vxbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-632
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/c6b2vxbm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: c6b2vxbm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9933 | Val Loss: 2.0551
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2400 | Val Loss: 2.3909
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4428 | Val Loss: 2.4894
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4978 | Val Loss: 2.5059
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4852 | Val Loss: 2.4760
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4560 | Val Loss: 2.4492
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4373 | Val Loss: 2.4412
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3862 | Val Loss: 2.3547
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3319 | Val Loss: 2.2531
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.773 MB uploadedwandb: | 43.729 MB of 43.773 MB uploadedwandb: / 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.23776
wandb:   val_loss 2.24214
wandb: 
wandb: üöÄ View run valiant-sweep-632 at: https://wandb.ai/7shoe/domShift-extensive/runs/c6b2vxbm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184012-c6b2vxbm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2378 | Val Loss: 2.2421
2025-03-26 18:40:56,488 - wandb.wandb_agent - INFO - Cleaning up finished run: c6b2vxbm
2025-03-26 18:40:57,019 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:40:57,019 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:40:57,022 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:41:02,034 - wandb.wandb_agent - INFO - Running runs: ['04m9tj89']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184101-04m9tj89
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-638
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/04m9tj89
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 04m9tj89
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6100 | Val Loss: 1.2691
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2169 | Val Loss: 1.3117
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2693 | Val Loss: 1.2460
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2387 | Val Loss: 1.2375
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2382 | Val Loss: 1.2389
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2381 | Val Loss: 1.2390
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2383 | Val Loss: 1.2388
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2383 | Val Loss: 1.2390
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2384 | Val Loss: 1.2390
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23848
wandb:   val_loss 1.23905
wandb: 
wandb: üöÄ View run jolly-sweep-638 at: https://wandb.ai/7shoe/domShift-extensive/runs/04m9tj89
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184101-04m9tj89/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2385 | Val Loss: 1.2390
2025-03-26 18:41:52,800 - wandb.wandb_agent - INFO - Cleaning up finished run: 04m9tj89
2025-03-26 18:41:53,429 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:41:53,430 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:41:53,433 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:41:58,447 - wandb.wandb_agent - INFO - Running runs: ['krb51pev']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184159-krb51pev
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-643
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/krb51pev
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: krb51pev
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4171 | Val Loss: 1.2459
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1533 | Val Loss: 1.1821
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2253 | Val Loss: 1.2508
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2559 | Val Loss: 1.2560
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2567 | Val Loss: 1.2519
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2493 | Val Loss: 1.2451
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2448 | Val Loss: 1.2426
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2425 | Val Loss: 1.2406
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2413 | Val Loss: 1.2411
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñá‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.24161
wandb:   val_loss 1.24105
wandb: 
wandb: üöÄ View run dulcet-sweep-643 at: https://wandb.ai/7shoe/domShift-extensive/runs/krb51pev
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184159-krb51pev/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2416 | Val Loss: 1.2410
2025-03-26 18:42:38,984 - wandb.wandb_agent - INFO - Cleaning up finished run: krb51pev
2025-03-26 18:42:59,053 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 18:42:59,499 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:42:59,499 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:42:59,502 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:43:04,514 - wandb.wandb_agent - INFO - Running runs: ['4mqy68ny']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184304-4mqy68ny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-649
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4mqy68ny
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4mqy68ny
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1809 | Val Loss: 0.9982
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0337 | Val Loss: 1.0333
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0293 | Val Loss: 1.0276
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0261 | Val Loss: 1.0236
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0229 | Val Loss: 1.0131
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0052 | Val Loss: 1.0356
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0593 | Val Loss: 1.0453
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0443 | Val Loss: 1.0430
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0430 | Val Loss: 1.0429
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04371
wandb:   val_loss 1.04409
wandb: 
wandb: üöÄ View run restful-sweep-649 at: https://wandb.ai/7shoe/domShift-extensive/runs/4mqy68ny
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184304-4mqy68ny/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0437 | Val Loss: 1.0441
2025-03-26 18:44:15,512 - wandb.wandb_agent - INFO - Cleaning up finished run: 4mqy68ny
2025-03-26 18:44:16,857 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:44:16,858 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:44:16,861 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:44:21,874 - wandb.wandb_agent - INFO - Running runs: ['ch9z1d2c']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184421-ch9z1d2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-658
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ch9z1d2c
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ch9z1d2c
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4564 | Val Loss: 1.3441
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3272 | Val Loss: 1.3325
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2450 | Val Loss: 1.0845
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0309 | Val Loss: 1.0109
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0082 | Val Loss: 1.0047
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0026 | Val Loss: 0.9998
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9973 | Val Loss: 0.9948
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9927 | Val Loss: 0.9906
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9889 | Val Loss: 0.9872
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.313 MB of 137.472 MB uploadedwandb: - 137.313 MB of 137.472 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.313 MB of 137.472 MB uploadedwandb: - 137.313 MB of 137.472 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.313 MB of 137.472 MB uploadedwandb: - 137.313 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.9857
wandb:   val_loss 0.98425
wandb: 
wandb: üöÄ View run snowy-sweep-658 at: https://wandb.ai/7shoe/domShift-extensive/runs/ch9z1d2c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184421-ch9z1d2c/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9857 | Val Loss: 0.9843
2025-03-26 18:45:48,111 - wandb.wandb_agent - INFO - Cleaning up finished run: ch9z1d2c
2025-03-26 18:45:48,659 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:45:48,659 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:45:48,664 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:45:53,677 - wandb.wandb_agent - INFO - Running runs: ['u4aktrfm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184554-u4aktrfm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-666
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u4aktrfm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u4aktrfm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7883 | Val Loss: 2.6746
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6088 | Val Loss: 2.6289
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6716 | Val Loss: 2.7128
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5954 | Val Loss: 2.3874
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2208 | Val Loss: 2.0736
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0015 | Val Loss: 1.9508
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.9371 | Val Loss: 1.9329
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.9444 | Val Loss: 1.9607
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9830 | Val Loss: 2.0074
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.03246
wandb:   val_loss 2.0576
wandb: 
wandb: üöÄ View run treasured-sweep-666 at: https://wandb.ai/7shoe/domShift-extensive/runs/u4aktrfm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184554-u4aktrfm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.0325 | Val Loss: 2.0576
2025-03-26 18:47:19,988 - wandb.wandb_agent - INFO - Cleaning up finished run: u4aktrfm
2025-03-26 18:47:20,572 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:47:20,573 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:47:20,575 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:47:25,588 - wandb.wandb_agent - INFO - Running runs: ['mwr9j1fc']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184726-mwr9j1fc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-675
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mwr9j1fc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mwr9j1fc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3834 | Val Loss: 1.3412
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3230 | Val Loss: 1.3146
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3151 | Val Loss: 1.3129
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3059 | Val Loss: 1.2976
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2943 | Val Loss: 1.2914
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2901 | Val Loss: 1.2885
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2876 | Val Loss: 1.2863
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2858 | Val Loss: 1.2849
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2846 | Val Loss: 1.2841
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.284
wandb:   val_loss 1.28364
wandb: 
wandb: üöÄ View run super-sweep-675 at: https://wandb.ai/7shoe/domShift-extensive/runs/mwr9j1fc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184726-mwr9j1fc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2840 | Val Loss: 1.2836
2025-03-26 18:48:21,443 - wandb.wandb_agent - INFO - Cleaning up finished run: mwr9j1fc
2025-03-26 18:48:22,221 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:48:22,221 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:48:22,224 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimCLR --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:48:27,237 - wandb.wandb_agent - INFO - Running runs: ['tk3u0rk2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184828-tk3u0rk2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-681
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tk3u0rk2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tk3u0rk2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.1020 | Val Loss: 3.1310
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.8977 | Val Loss: 2.6508
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5482 | Val Loss: 2.3852
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4069 | Val Loss: 2.2984
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3235 | Val Loss: 2.2905
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2897 | Val Loss: 2.2153
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2414 | Val Loss: 2.1921
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2181 | Val Loss: 2.1566
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1908 | Val Loss: 2.1506
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.17337
wandb:   val_loss 2.10949
wandb: 
wandb: üöÄ View run zany-sweep-681 at: https://wandb.ai/7shoe/domShift-extensive/runs/tk3u0rk2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184828-tk3u0rk2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1734 | Val Loss: 2.1095
2025-03-26 18:49:43,321 - wandb.wandb_agent - INFO - Cleaning up finished run: tk3u0rk2
2025-03-26 18:49:43,807 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:49:43,807 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:49:43,810 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:49:48,823 - wandb.wandb_agent - INFO - Running runs: ['bc68tdon']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184949-bc68tdon
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-692
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bc68tdon
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bc68tdon
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4983 | Val Loss: 1.3594
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3408 | Val Loss: 1.3449
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2803 | Val Loss: 1.2372
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2561 | Val Loss: 1.2572
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2120 | Val Loss: 1.1542
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1302 | Val Loss: 1.1199
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1212 | Val Loss: 1.1235
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1251 | Val Loss: 1.1264
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1279 | Val Loss: 1.1292
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13065
wandb:   val_loss 1.13296
wandb: 
wandb: üöÄ View run desert-sweep-692 at: https://wandb.ai/7shoe/domShift-extensive/runs/bc68tdon
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184949-bc68tdon/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1307 | Val Loss: 1.1330
2025-03-26 18:50:59,811 - wandb.wandb_agent - INFO - Cleaning up finished run: bc68tdon
2025-03-26 18:51:00,424 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:51:00,425 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:51:00,428 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 18:51:05,440 - wandb.wandb_agent - INFO - Running runs: ['mswwqqwn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185106-mswwqqwn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-698
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mswwqqwn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mswwqqwn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7202 | Val Loss: 1.7340
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6611 | Val Loss: 1.4876
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3570 | Val Loss: 1.2147
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1673 | Val Loss: 1.1306
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1507 | Val Loss: 1.1721
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1986 | Val Loss: 1.2251
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2420 | Val Loss: 1.2591
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2646 | Val Loss: 1.2676
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2648 | Val Loss: 1.2574
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25203
wandb:   val_loss 1.24545
wandb: 
wandb: üöÄ View run likely-sweep-698 at: https://wandb.ai/7shoe/domShift-extensive/runs/mswwqqwn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185106-mswwqqwn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2520 | Val Loss: 1.2455
2025-03-26 18:51:35,845 - wandb.wandb_agent - INFO - Cleaning up finished run: mswwqqwn
2025-03-26 18:51:36,508 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:51:36,508 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:51:36,511 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:51:41,524 - wandb.wandb_agent - INFO - Running runs: ['cu9eldqn']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185141-cu9eldqn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-703
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cu9eldqn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cu9eldqn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2805 | Val Loss: 1.0828
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1067 | Val Loss: 1.1096
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0100 | Val Loss: 0.9612
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9537 | Val Loss: 0.9489
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9466 | Val Loss: 0.9433
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9413 | Val Loss: 0.9381
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9358 | Val Loss: 0.9332
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9316 | Val Loss: 0.9296
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9284 | Val Loss: 0.9267
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92577
wandb:   val_loss 0.92684
wandb: 
wandb: üöÄ View run light-sweep-703 at: https://wandb.ai/7shoe/domShift-extensive/runs/cu9eldqn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185141-cu9eldqn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9258 | Val Loss: 0.9268
2025-03-26 18:52:27,175 - wandb.wandb_agent - INFO - Cleaning up finished run: cu9eldqn
2025-03-26 18:52:28,298 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:52:28,298 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:52:28,302 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:52:33,314 - wandb.wandb_agent - INFO - Running runs: ['9hzwb0jd']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185233-9hzwb0jd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-708
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9hzwb0jd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9hzwb0jd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2053 | Val Loss: 3.0084
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.9172 | Val Loss: 2.8874
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8909 | Val Loss: 2.9128
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.9445 | Val Loss: 2.9741
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.9996 | Val Loss: 3.0236
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.0296 | Val Loss: 3.0003
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.9883 | Val Loss: 2.9811
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.9831 | Val Loss: 3.0003
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 3.0282 | Val Loss: 3.0482
wandb: - 32.969 MB of 32.969 MB uploadedwandb: \ 32.969 MB of 32.969 MB uploadedwandb: | 33.062 MB of 33.062 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.05457
wandb:   val_loss 3.05558
wandb: 
wandb: üöÄ View run likely-sweep-708 at: https://wandb.ai/7shoe/domShift-extensive/runs/9hzwb0jd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185233-9hzwb0jd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.0546 | Val Loss: 3.0556
2025-03-26 18:53:23,979 - wandb.wandb_agent - INFO - Cleaning up finished run: 9hzwb0jd
2025-03-26 18:53:24,574 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:53:24,574 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:53:24,577 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:53:29,590 - wandb.wandb_agent - INFO - Running runs: ['p8je4pv8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185330-p8je4pv8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-714
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/p8je4pv8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: p8je4pv8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3225 | Val Loss: 1.1947
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2040 | Val Loss: 1.2144
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2176 | Val Loss: 1.2219
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2227 | Val Loss: 1.2216
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2219 | Val Loss: 1.2216
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2175 | Val Loss: 1.2170
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2073 | Val Loss: 1.2030
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2084 | Val Loss: 1.2167
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2325 | Val Loss: 1.2486
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.26332
wandb:   val_loss 1.27658
wandb: 
wandb: üöÄ View run still-sweep-714 at: https://wandb.ai/7shoe/domShift-extensive/runs/p8je4pv8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185330-p8je4pv8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2633 | Val Loss: 1.2766
2025-03-26 18:54:45,676 - wandb.wandb_agent - INFO - Cleaning up finished run: p8je4pv8
2025-03-26 18:54:46,343 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:54:46,343 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:54:46,348 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:54:51,360 - wandb.wandb_agent - INFO - Running runs: ['3rrhjups']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185451-3rrhjups
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-722
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3rrhjups
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3rrhjups
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4555 | Val Loss: 1.3195
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3507 | Val Loss: 1.3923
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4004 | Val Loss: 1.4097
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3856 | Val Loss: 1.3340
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3169 | Val Loss: 1.3168
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3252 | Val Loss: 1.3224
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2201 | Val Loss: 1.1025
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1147 | Val Loss: 1.1444
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0936 | Val Loss: 1.0246
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04929
wandb:   val_loss 1.09675
wandb: 
wandb: üöÄ View run feasible-sweep-722 at: https://wandb.ai/7shoe/domShift-extensive/runs/3rrhjups
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185451-3rrhjups/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0493 | Val Loss: 1.0967
2025-03-26 18:55:36,973 - wandb.wandb_agent - INFO - Cleaning up finished run: 3rrhjups
2025-03-26 18:55:37,595 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:55:37,595 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:55:37,598 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:55:42,610 - wandb.wandb_agent - INFO - Running runs: ['zy1tye51']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185542-zy1tye51
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-727
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zy1tye51
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zy1tye51
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2818 | Val Loss: 1.1552
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1191 | Val Loss: 1.0495
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0114 | Val Loss: 1.0173
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0527 | Val Loss: 1.0938
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1014 | Val Loss: 1.1016
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0947 | Val Loss: 1.0965
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1013 | Val Loss: 1.1011
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0992 | Val Loss: 1.0978
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0976 | Val Loss: 1.1020
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.014 MB of 33.014 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10068
wandb:   val_loss 1.09859
wandb: 
wandb: üöÄ View run rich-sweep-727 at: https://wandb.ai/7shoe/domShift-extensive/runs/zy1tye51
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185542-zy1tye51/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1007 | Val Loss: 1.0986
2025-03-26 18:56:48,688 - wandb.wandb_agent - INFO - Cleaning up finished run: zy1tye51
2025-03-26 18:56:49,289 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:56:49,289 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:56:49,292 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:56:54,305 - wandb.wandb_agent - INFO - Running runs: ['yk2fa33d']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185654-yk2fa33d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-734
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yk2fa33d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yk2fa33d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2516 | Val Loss: 0.9638
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9672 | Val Loss: 0.8950
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9445 | Val Loss: 0.9536
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9531 | Val Loss: 0.9548
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9511 | Val Loss: 0.9518
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9513 | Val Loss: 0.9519
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9514 | Val Loss: 0.9515
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9514 | Val Loss: 0.9514
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9515 | Val Loss: 0.9514
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95146
wandb:   val_loss 0.95152
wandb: 
wandb: üöÄ View run ancient-sweep-734 at: https://wandb.ai/7shoe/domShift-extensive/runs/yk2fa33d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185654-yk2fa33d/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9515 | Val Loss: 0.9515
2025-03-26 18:57:45,017 - wandb.wandb_agent - INFO - Cleaning up finished run: yk2fa33d
2025-03-26 18:57:45,720 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:45,720 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:57:45,723 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:57:50,736 - wandb.wandb_agent - INFO - Running runs: ['7c6l2o9s']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185751-7c6l2o9s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-741
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7c6l2o9s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7c6l2o9s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3255 | Val Loss: 1.1297
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1286 | Val Loss: 1.1358
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1683 | Val Loss: 1.1999
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2010 | Val Loss: 1.1997
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2068 | Val Loss: 1.2182
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2267 | Val Loss: 1.2347
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2404 | Val Loss: 1.2463
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2491 | Val Loss: 1.2517
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2528 | Val Loss: 1.2540
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25441
wandb:   val_loss 1.25537
wandb: 
wandb: üöÄ View run avid-sweep-741 at: https://wandb.ai/7shoe/domShift-extensive/runs/7c6l2o9s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185751-7c6l2o9s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2544 | Val Loss: 1.2554
2025-03-26 18:58:46,515 - wandb.wandb_agent - INFO - Cleaning up finished run: 7c6l2o9s
2025-03-26 18:58:47,087 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:58:47,088 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:58:47,091 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:58:52,104 - wandb.wandb_agent - INFO - Running runs: ['m3s5maor']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185853-m3s5maor
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-747
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m3s5maor
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: m3s5maor
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.7444 | Val Loss: 1.5824
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.5633 | Val Loss: 1.4103
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.1992 | Val Loss: 1.1244
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.1271 | Val Loss: 1.1274
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.1269 | Val Loss: 1.1258
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.1061 | Val Loss: 1.0677
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.0640 | Val Loss: 1.0655
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.0673 | Val Loss: 1.0681
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.0687 | Val Loss: 1.0690
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06906
wandb:   val_loss 1.0691
wandb: 
wandb: üöÄ View run rare-sweep-747 at: https://wandb.ai/7shoe/domShift-extensive/runs/m3s5maor
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185853-m3s5maor/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.0691 | Val Loss: 1.0691
2025-03-26 19:00:03,101 - wandb.wandb_agent - INFO - Cleaning up finished run: m3s5maor
2025-03-26 19:00:03,757 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:03,757 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:00:03,760 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:00:08,773 - wandb.wandb_agent - INFO - Running runs: ['mn2t1lm5']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190008-mn2t1lm5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-756
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mn2t1lm5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mn2t1lm5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6918 | Val Loss: 1.6451
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6825 | Val Loss: 1.2910
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1894 | Val Loss: 1.1846
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2334 | Val Loss: 1.2869
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3303 | Val Loss: 1.3260
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3127 | Val Loss: 1.3095
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3076 | Val Loss: 1.3032
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2983 | Val Loss: 1.2954
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2986 | Val Loss: 1.3027
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30412
wandb:   val_loss 1.30466
wandb: 
wandb: üöÄ View run ruby-sweep-756 at: https://wandb.ai/7shoe/domShift-extensive/runs/mn2t1lm5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190008-mn2t1lm5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3041 | Val Loss: 1.3047
2025-03-26 19:00:59,587 - wandb.wandb_agent - INFO - Cleaning up finished run: mn2t1lm5
2025-03-26 19:01:00,158 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:01:00,158 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:01:00,161 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimCLR --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 19:01:05,174 - wandb.wandb_agent - INFO - Running runs: ['6nwzbenb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190106-6nwzbenb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-763
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6nwzbenb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6nwzbenb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.1804 | Val Loss: 4.8928
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.8568 | Val Loss: 4.7672
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 4.7822 | Val Loss: 4.7117
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 4.7409 | Val Loss: 4.6985
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 4.7261 | Val Loss: 4.6745
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 4.7114 | Val Loss: 4.6657
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 4.7061 | Val Loss: 4.6598
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 4.7022 | Val Loss: 4.6585
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 4.6953 | Val Loss: 4.6512
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.69169
wandb:   val_loss 4.65335
wandb: 
wandb: üöÄ View run dutiful-sweep-763 at: https://wandb.ai/7shoe/domShift-extensive/runs/6nwzbenb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190106-6nwzbenb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 4.6917 | Val Loss: 4.6534
2025-03-26 19:02:21,226 - wandb.wandb_agent - INFO - Cleaning up finished run: 6nwzbenb
2025-03-26 19:02:21,891 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:02:21,891 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:02:21,894 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:02:26,906 - wandb.wandb_agent - INFO - Running runs: ['ccbxx8lr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190227-ccbxx8lr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-772
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ccbxx8lr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ccbxx8lr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.8940 | Val Loss: 1.8346
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.8176 | Val Loss: 1.8357
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.8112 | Val Loss: 1.7353
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.7144 | Val Loss: 1.6717
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.6499 | Val Loss: 1.5957
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.5478 | Val Loss: 1.4479
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.3981 | Val Loss: 1.3124
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.2743 | Val Loss: 1.2071
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.1871 | Val Loss: 1.1434
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.279 MB uploadedwandb: | 137.259 MB of 137.279 MB uploadedwandb: / 137.279 MB of 137.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13196
wandb:   val_loss 1.11504
wandb: 
wandb: üöÄ View run different-sweep-772 at: https://wandb.ai/7shoe/domShift-extensive/runs/ccbxx8lr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190227-ccbxx8lr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.1320 | Val Loss: 1.1150
2025-03-26 19:02:57,304 - wandb.wandb_agent - INFO - Cleaning up finished run: ccbxx8lr
2025-03-26 19:02:58,003 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:02:58,003 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:02:58,006 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:03:03,018 - wandb.wandb_agent - INFO - Running runs: ['0ufqi5iy']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190304-0ufqi5iy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-776
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0ufqi5iy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0ufqi5iy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6080 | Val Loss: 1.7715
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6906 | Val Loss: 1.4913
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4061 | Val Loss: 1.4725
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6044 | Val Loss: 1.7199
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4361 | Val Loss: 1.2128
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4228 | Val Loss: 1.5157
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4310 | Val Loss: 1.3557
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2396 | Val Loss: 1.1732
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1862 | Val Loss: 1.1975
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÅ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20549
wandb:   val_loss 1.21556
wandb: 
wandb: üöÄ View run morning-sweep-776 at: https://wandb.ai/7shoe/domShift-extensive/runs/0ufqi5iy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190304-0ufqi5iy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2055 | Val Loss: 1.2156
2025-03-26 19:03:58,810 - wandb.wandb_agent - INFO - Cleaning up finished run: 0ufqi5iy
2025-03-26 19:03:59,458 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:03:59,459 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:03:59,462 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 19:04:04,473 - wandb.wandb_agent - INFO - Running runs: ['6zyysd0v']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190405-6zyysd0v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-783
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6zyysd0v
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6zyysd0v
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5931 | Val Loss: 1.4832
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4921 | Val Loss: 1.5276
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6467 | Val Loss: 1.8297
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.9032 | Val Loss: 1.9662
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.9342 | Val Loss: 1.8924
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8128 | Val Loss: 1.6922
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6483 | Val Loss: 1.5959
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5690 | Val Loss: 1.5314
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4900 | Val Loss: 1.4037
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4893
wandb:   val_loss 1.60003
wandb: 
wandb: üöÄ View run generous-sweep-783 at: https://wandb.ai/7shoe/domShift-extensive/runs/6zyysd0v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190405-6zyysd0v/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4893 | Val Loss: 1.6000
2025-03-26 19:04:39,960 - wandb.wandb_agent - INFO - Cleaning up finished run: 6zyysd0v
2025-03-26 19:04:40,513 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:04:40,513 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:04:40,516 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:04:45,528 - wandb.wandb_agent - INFO - Running runs: ['yuqvqz3m']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190446-yuqvqz3m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-787
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yuqvqz3m
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yuqvqz3m
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 2.4953 | Val Loss: 1.9747
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.7683 | Val Loss: 1.6511
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.6931 | Val Loss: 1.7635
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.7617 | Val Loss: 1.7042
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.7796 | Val Loss: 1.8631
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.9329 | Val Loss: 1.9992
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.0551 | Val Loss: 2.1082
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.0846 | Val Loss: 2.0541
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.0844 | Val Loss: 2.1195
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.256 MB uploadedwandb: | 137.276 MB of 137.276 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.14982
wandb:   val_loss 2.17858
wandb: 
wandb: üöÄ View run silvery-sweep-787 at: https://wandb.ai/7shoe/domShift-extensive/runs/yuqvqz3m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190446-yuqvqz3m/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.1498 | Val Loss: 2.1786
2025-03-26 19:05:46,388 - wandb.wandb_agent - INFO - Cleaning up finished run: yuqvqz3m
2025-03-26 19:05:47,080 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:05:47,080 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:05:47,082 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:05:52,095 - wandb.wandb_agent - INFO - Running runs: ['1h7n9nqu']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190551-1h7n9nqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-794
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1h7n9nqu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1h7n9nqu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2206 | Val Loss: 0.9894
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9042 | Val Loss: 0.8374
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8196 | Val Loss: 0.8115
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8045 | Val Loss: 0.7976
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.7938 | Val Loss: 0.7893
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.7867 | Val Loss: 0.7835
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.7814 | Val Loss: 0.7788
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.7771 | Val Loss: 0.7751
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.7736 | Val Loss: 0.7719
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.77075
wandb:   val_loss 0.76935
wandb: 
wandb: üöÄ View run electric-sweep-794 at: https://wandb.ai/7shoe/domShift-extensive/runs/1h7n9nqu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190551-1h7n9nqu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.7708 | Val Loss: 0.7693
2025-03-26 19:07:08,168 - wandb.wandb_agent - INFO - Cleaning up finished run: 1h7n9nqu
2025-03-26 19:07:08,683 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:07:08,683 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:07:08,686 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:07:13,698 - wandb.wandb_agent - INFO - Running runs: ['1431hhzp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190714-1431hhzp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-801
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1431hhzp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1431hhzp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4486 | Val Loss: 1.1755
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1830 | Val Loss: 1.1558
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1303 | Val Loss: 1.1156
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1130 | Val Loss: 1.1103
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1114 | Val Loss: 1.1114
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1117 | Val Loss: 1.1113
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1117 | Val Loss: 1.1113
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1117 | Val Loss: 1.1113
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1117 | Val Loss: 1.1114
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11175
wandb:   val_loss 1.11137
wandb: 
wandb: üöÄ View run glad-sweep-801 at: https://wandb.ai/7shoe/domShift-extensive/runs/1431hhzp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190714-1431hhzp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1118 | Val Loss: 1.1114
2025-03-26 19:08:09,514 - wandb.wandb_agent - INFO - Cleaning up finished run: 1431hhzp
2025-03-26 19:08:10,211 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:08:10,211 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:08:10,214 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 19:08:15,227 - wandb.wandb_agent - INFO - Running runs: ['n1jhls62']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190816-n1jhls62
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-806
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/n1jhls62
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: n1jhls62
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.7027 | Val Loss: 1.4309
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.3140 | Val Loss: 1.2149
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.1367 | Val Loss: 1.0759
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.0881 | Val Loss: 1.1018
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.0974 | Val Loss: 1.0807
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.0644 | Val Loss: 1.0475
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.0401 | Val Loss: 1.0327
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.0323 | Val Loss: 1.0326
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.0374 | Val Loss: 1.0417
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.279 MB of 137.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04603
wandb:   val_loss 1.04899
wandb: 
wandb: üöÄ View run rose-sweep-806 at: https://wandb.ai/7shoe/domShift-extensive/runs/n1jhls62
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190816-n1jhls62/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.0460 | Val Loss: 1.0490
2025-03-26 19:08:55,793 - wandb.wandb_agent - INFO - Cleaning up finished run: n1jhls62
2025-03-26 19:08:56,304 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:08:56,305 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:08:56,307 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:09:01,320 - wandb.wandb_agent - INFO - Running runs: ['kbzb9k0j']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190902-kbzb9k0j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-812
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kbzb9k0j
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: kbzb9k0j
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.7989 | Val Loss: 1.4641
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.3843 | Val Loss: 1.3425
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.3450 | Val Loss: 1.3489
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.3433 | Val Loss: 1.3359
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.3319 | Val Loss: 1.3284
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.3256 | Val Loss: 1.3103
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.2889 | Val Loss: 1.2778
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.2773 | Val Loss: 1.2742
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.2713 | Val Loss: 1.2699
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.279 MB of 137.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27102
wandb:   val_loss 1.26986
wandb: 
wandb: üöÄ View run peachy-sweep-812 at: https://wandb.ai/7shoe/domShift-extensive/runs/kbzb9k0j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190902-kbzb9k0j/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.2710 | Val Loss: 1.2699
2025-03-26 19:09:57,095 - wandb.wandb_agent - INFO - Cleaning up finished run: kbzb9k0j
2025-03-26 19:09:58,595 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:09:58,595 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:09:58,598 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:10:03,611 - wandb.wandb_agent - INFO - Running runs: ['yec59qu3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191004-yec59qu3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-817
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yec59qu3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: yec59qu3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.7538 | Val Loss: 3.0673
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.7333 | Val Loss: 2.4797
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.3493 | Val Loss: 2.2075
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.1502 | Val Loss: 2.1139
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.1214 | Val Loss: 2.1415
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.1834 | Val Loss: 2.2325
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.2815 | Val Loss: 2.3301
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.3704 | Val Loss: 2.4086
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.4378 | Val Loss: 2.4644
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.873 MB uploadedwandb: | 32.853 MB of 32.873 MB uploadedwandb: / 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.48282
wandb:   val_loss 2.49962
wandb: 
wandb: üöÄ View run glamorous-sweep-817 at: https://wandb.ai/7shoe/domShift-extensive/runs/yec59qu3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191004-yec59qu3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.4828 | Val Loss: 2.4996
2025-03-26 19:10:39,097 - wandb.wandb_agent - INFO - Cleaning up finished run: yec59qu3
2025-03-26 19:10:40,003 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:40,004 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:10:40,006 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:10:45,019 - wandb.wandb_agent - INFO - Running runs: ['lreu14v0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191046-lreu14v0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-822
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lreu14v0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lreu14v0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5623 | Val Loss: 1.3948
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3714 | Val Loss: 1.3361
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3138 | Val Loss: 1.3005
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2775 | Val Loss: 1.2487
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2428 | Val Loss: 1.2292
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2133 | Val Loss: 1.1974
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1971 | Val Loss: 1.1947
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1898 | Val Loss: 1.1879
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1861 | Val Loss: 1.1827
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.18231
wandb:   val_loss 1.18151
wandb: 
wandb: üöÄ View run volcanic-sweep-822 at: https://wandb.ai/7shoe/domShift-extensive/runs/lreu14v0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191046-lreu14v0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1823 | Val Loss: 1.1815
2025-03-26 19:11:55,988 - wandb.wandb_agent - INFO - Cleaning up finished run: lreu14v0
2025-03-26 19:11:56,598 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:11:56,598 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:11:56,601 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:12:01,614 - wandb.wandb_agent - INFO - Running runs: ['b8uas724']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191201-b8uas724
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-831
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b8uas724
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: b8uas724
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6324 | Val Loss: 1.5244
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4666 | Val Loss: 1.4337
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3910 | Val Loss: 1.3154
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2684 | Val Loss: 1.2415
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2436 | Val Loss: 1.2580
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2709 | Val Loss: 1.2802
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2841 | Val Loss: 1.2860
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2855 | Val Loss: 1.2859
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2876 | Val Loss: 1.2900
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.29248
wandb:   val_loss 1.29508
wandb: 
wandb: üöÄ View run northern-sweep-831 at: https://wandb.ai/7shoe/domShift-extensive/runs/b8uas724
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191201-b8uas724/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2925 | Val Loss: 1.2951
2025-03-26 19:12:52,293 - wandb.wandb_agent - INFO - Cleaning up finished run: b8uas724
2025-03-26 19:12:52,832 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:12:52,832 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:12:52,835 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 19:12:57,847 - wandb.wandb_agent - INFO - Running runs: ['u37wzgf3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191258-u37wzgf3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-837
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u37wzgf3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u37wzgf3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8898 | Val Loss: 2.3910
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1619 | Val Loss: 2.0236
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0247 | Val Loss: 2.0417
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0684 | Val Loss: 2.0947
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1082 | Val Loss: 2.1286
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1529 | Val Loss: 2.1773
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1987 | Val Loss: 2.2199
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2379 | Val Loss: 2.2555
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2690 | Val Loss: 2.2819
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.29148
wandb:   val_loss 2.30078
wandb: 
wandb: üöÄ View run soft-sweep-837 at: https://wandb.ai/7shoe/domShift-extensive/runs/u37wzgf3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191258-u37wzgf3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2915 | Val Loss: 2.3008
2025-03-26 19:13:58,629 - wandb.wandb_agent - INFO - Cleaning up finished run: u37wzgf3
2025-03-26 19:13:59,537 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:13:59,538 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:13:59,540 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:14:04,553 - wandb.wandb_agent - INFO - Running runs: ['tvo4xa48']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191404-tvo4xa48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-843
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tvo4xa48
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tvo4xa48
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5942 | Val Loss: 1.5459
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4290 | Val Loss: 1.3770
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3518 | Val Loss: 1.3421
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3406 | Val Loss: 1.3433
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3616 | Val Loss: 1.3822
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3901 | Val Loss: 1.3941
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3966 | Val Loss: 1.3997
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3954 | Val Loss: 1.3910
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3873 | Val Loss: 1.3726
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23503
wandb:   val_loss 1.18624
wandb: 
wandb: üöÄ View run lemon-sweep-843 at: https://wandb.ai/7shoe/domShift-extensive/runs/tvo4xa48
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191404-tvo4xa48/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2350 | Val Loss: 1.1862
2025-03-26 19:14:55,266 - wandb.wandb_agent - INFO - Cleaning up finished run: tvo4xa48
2025-03-26 19:14:55,817 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:14:55,818 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:14:55,820 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 19:15:00,833 - wandb.wandb_agent - INFO - Running runs: ['w6wzmfn6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191501-w6wzmfn6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-850
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/w6wzmfn6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: w6wzmfn6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5219 | Val Loss: 1.5059
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3418 | Val Loss: 1.2867
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4380 | Val Loss: 1.6307
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3587 | Val Loss: 1.3593
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2913 | Val Loss: 1.2826
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2352 | Val Loss: 1.0092
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9484 | Val Loss: 0.9119
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8550 | Val Loss: 0.8789
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8954 | Val Loss: 0.8894
wandb: - 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.89093
wandb:   val_loss 0.894
wandb: 
wandb: üöÄ View run restful-sweep-850 at: https://wandb.ai/7shoe/domShift-extensive/runs/w6wzmfn6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191501-w6wzmfn6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8909 | Val Loss: 0.8940
2025-03-26 19:15:51,542 - wandb.wandb_agent - INFO - Cleaning up finished run: w6wzmfn6
2025-03-26 19:15:53,938 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:15:53,939 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 19:15:53,942 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:15:58,953 - wandb.wandb_agent - INFO - Running runs: ['20zbov72']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191558-20zbov72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-856
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/20zbov72
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 20zbov72
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0986 | Val Loss: 2.3364
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1655 | Val Loss: 2.1426
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1851 | Val Loss: 2.2266
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2554 | Val Loss: 2.2839
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3057 | Val Loss: 2.3290
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3528 | Val Loss: 2.3770
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3978 | Val Loss: 2.4178
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3864 | Val Loss: 2.3227
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3094 | Val Loss: 2.3010
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÜ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.28762
wandb:   val_loss 2.2718
wandb: 
wandb: üöÄ View run lemon-sweep-856 at: https://wandb.ai/7shoe/domShift-extensive/runs/20zbov72
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191558-20zbov72/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2876 | Val Loss: 2.2718
2025-03-26 19:16:59,804 - wandb.wandb_agent - INFO - Cleaning up finished run: 20zbov72
2025-03-26 19:17:00,382 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:17:00,383 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:17:00,386 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:17:05,400 - wandb.wandb_agent - INFO - Running runs: ['4rtr4cts']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191706-4rtr4cts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-864
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4rtr4cts
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4rtr4cts
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3823 | Val Loss: 1.0296
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9720 | Val Loss: 0.9374
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9246 | Val Loss: 0.9143
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9115 | Val Loss: 0.9082
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9074 | Val Loss: 0.9060
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9060 | Val Loss: 0.9054
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9040 | Val Loss: 0.8975
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8978 | Val Loss: 0.8997
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9032 | Val Loss: 0.9065
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.90969
wandb:   val_loss 0.91268
wandb: 
wandb: üöÄ View run graceful-sweep-864 at: https://wandb.ai/7shoe/domShift-extensive/runs/4rtr4cts
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191706-4rtr4cts/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9097 | Val Loss: 0.9127
2025-03-26 19:17:51,082 - wandb.wandb_agent - INFO - Cleaning up finished run: 4rtr4cts
2025-03-26 19:17:51,701 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:17:51,701 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:17:51,704 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:17:56,716 - wandb.wandb_agent - INFO - Running runs: ['wnohghe7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191757-wnohghe7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-869
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wnohghe7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wnohghe7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4423 | Val Loss: 1.3035
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2826 | Val Loss: 1.2982
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3038 | Val Loss: 1.3041
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3053 | Val Loss: 1.3049
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3054 | Val Loss: 1.3049
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3054 | Val Loss: 1.3049
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3055 | Val Loss: 1.3050
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3056 | Val Loss: 1.3051
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3056 | Val Loss: 1.3051
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÜ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30561
wandb:   val_loss 1.3051
wandb: 
wandb: üöÄ View run volcanic-sweep-869 at: https://wandb.ai/7shoe/domShift-extensive/runs/wnohghe7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191757-wnohghe7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3056 | Val Loss: 1.3051
2025-03-26 19:18:47,402 - wandb.wandb_agent - INFO - Cleaning up finished run: wnohghe7
2025-03-26 19:18:47,921 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:18:47,921 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:18:47,924 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:18:52,937 - wandb.wandb_agent - INFO - Running runs: ['bom36x95']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191852-bom36x95
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-875
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bom36x95
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bom36x95
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7194 | Val Loss: 1.3589
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3034 | Val Loss: 1.2664
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2823 | Val Loss: 1.2851
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2897 | Val Loss: 1.2888
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2789 | Val Loss: 1.2592
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2318 | Val Loss: 1.1958
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1997 | Val Loss: 1.1877
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1670 | Val Loss: 1.1527
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1559 | Val Loss: 1.1595
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16012
wandb:   val_loss 1.15584
wandb: 
wandb: üöÄ View run trim-sweep-875 at: https://wandb.ai/7shoe/domShift-extensive/runs/bom36x95
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191852-bom36x95/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1601 | Val Loss: 1.1558
2025-03-26 19:19:43,638 - wandb.wandb_agent - INFO - Cleaning up finished run: bom36x95
2025-03-26 19:19:44,202 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:44,202 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:19:44,205 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:19:49,217 - wandb.wandb_agent - INFO - Running runs: ['okpdpz88']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191949-okpdpz88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-881
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/okpdpz88
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: okpdpz88
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5353 | Val Loss: 1.5049
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5261 | Val Loss: 1.3706
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2336 | Val Loss: 1.0219
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1875 | Val Loss: 1.1553
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1525 | Val Loss: 1.0728
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0478 | Val Loss: 0.9917
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1031 | Val Loss: 1.0117
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0131 | Val Loss: 1.0111
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9847 | Val Loss: 0.9744
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.97392
wandb:   val_loss 0.98038
wandb: 
wandb: üöÄ View run dauntless-sweep-881 at: https://wandb.ai/7shoe/domShift-extensive/runs/okpdpz88
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191949-okpdpz88/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9739 | Val Loss: 0.9804
2025-03-26 19:20:34,822 - wandb.wandb_agent - INFO - Cleaning up finished run: okpdpz88
2025-03-26 19:20:36,116 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:20:36,116 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:20:36,119 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:20:41,131 - wandb.wandb_agent - INFO - Running runs: ['twrzpd8h']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_192041-twrzpd8h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-886
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/twrzpd8h
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: twrzpd8h
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3710 | Val Loss: 1.3714
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3325 | Val Loss: 1.2454
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1635 | Val Loss: 1.1087
