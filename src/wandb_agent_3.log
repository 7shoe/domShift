nohup: ignoring input
wandb: Starting wandb agent üïµÔ∏è
2025-03-26 17:04:29,017 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 17:04:29,262 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:04:29,263 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:04:29,266 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.5
2025-03-26 17:04:34,279 - wandb.wandb_agent - INFO - Running runs: ['1631sa6t']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170435-1631sa6t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1631sa6t
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1631sa6t
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.8358 | Val Loss: 5.3840
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 5.3530 | Val Loss: 5.1183
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 5.1620 | Val Loss: 4.9160
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 5.0640 | Val Loss: 4.8087
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 4.9721 | Val Loss: 4.7523
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 4.9527 | Val Loss: 4.7323
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 4.9201 | Val Loss: 4.6963
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 4.8754 | Val Loss: 4.6524
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 4.8431 | Val Loss: 4.6245
wandb: - 34.332 MB of 34.332 MB uploadedwandb: \ 34.332 MB of 34.351 MB uploadedwandb: | 34.332 MB of 34.351 MB uploadedwandb: / 34.351 MB of 34.351 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.82302
wandb:   val_loss 4.6048
wandb: 
wandb: üöÄ View run cerulean-sweep-3 at: https://wandb.ai/7shoe/domShift-extensive/runs/1631sa6t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170435-1631sa6t/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 4.8230 | Val Loss: 4.6048
2025-03-26 17:04:59,632 - wandb.wandb_agent - INFO - Cleaning up finished run: 1631sa6t
2025-03-26 17:05:00,157 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:00,157 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:05:00,160 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:05:05,173 - wandb.wandb_agent - INFO - Running runs: ['s2uqmboh']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170505-s2uqmboh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s2uqmboh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: s2uqmboh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.9697 | Val Loss: 5.4484
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 5.6234 | Val Loss: 5.3642
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 5.5544 | Val Loss: 5.2820
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 5.4625 | Val Loss: 5.2056
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 5.3817 | Val Loss: 5.1486
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 5.3244 | Val Loss: 5.0899
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 5.2831 | Val Loss: 5.0695
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 5.2348 | Val Loss: 5.0245
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 5.2146 | Val Loss: 5.0137
wandb: - 137.081 MB of 137.081 MB uploadedwandb: \ 137.081 MB of 137.081 MB uploadedwandb: | 137.101 MB of 137.101 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.20425
wandb:   val_loss 4.98866
wandb: 
wandb: üöÄ View run likely-sweep-7 at: https://wandb.ai/7shoe/domShift-extensive/runs/s2uqmboh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170505-s2uqmboh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 5.2042 | Val Loss: 4.9887
2025-03-26 17:05:35,581 - wandb.wandb_agent - INFO - Cleaning up finished run: s2uqmboh
2025-03-26 17:05:38,767 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:38,767 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:05:38,770 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimCLR --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:05:43,783 - wandb.wandb_agent - INFO - Running runs: ['pis76zil']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170545-pis76zil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pis76zil
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pis76zil
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.9766 | Val Loss: 4.6320
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 4.5573 | Val Loss: 4.4063
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 4.3139 | Val Loss: 4.1346
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 4.0933 | Val Loss: 3.9724
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 3.9622 | Val Loss: 3.8721
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.8738 | Val Loss: 3.7925
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 3.7960 | Val Loss: 3.7245
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 3.7354 | Val Loss: 3.6675
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 3.6971 | Val Loss: 3.6413
wandb: - 32.969 MB of 32.969 MB uploadedwandb: \ 32.969 MB of 32.988 MB uploadedwandb: | 32.969 MB of 32.988 MB uploadedwandb: / 32.988 MB of 32.988 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.65684
wandb:   val_loss 3.6087
wandb: 
wandb: üöÄ View run rural-sweep-13 at: https://wandb.ai/7shoe/domShift-extensive/runs/pis76zil
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170545-pis76zil/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.6568 | Val Loss: 3.6087
2025-03-26 17:06:14,199 - wandb.wandb_agent - INFO - Cleaning up finished run: pis76zil
2025-03-26 17:06:18,782 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:06:18,782 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:06:18,786 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:06:23,799 - wandb.wandb_agent - INFO - Running runs: ['50okvzx3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170624-50okvzx3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/50okvzx3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 50okvzx3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.6376 | Val Loss: 1.5825
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.6046 | Val Loss: 1.6237
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.6509 | Val Loss: 1.6557
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.5966 | Val Loss: 1.5470
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.5538 | Val Loss: 1.5371
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.5095 | Val Loss: 1.4683
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.4370 | Val Loss: 1.4088
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.3997 | Val Loss: 1.3856
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.3795 | Val Loss: 1.3688
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36214
wandb:   val_loss 1.35284
wandb: 
wandb: üöÄ View run devout-sweep-18 at: https://wandb.ai/7shoe/domShift-extensive/runs/50okvzx3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170624-50okvzx3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.3621 | Val Loss: 1.3528
2025-03-26 17:06:59,280 - wandb.wandb_agent - INFO - Cleaning up finished run: 50okvzx3
2025-03-26 17:07:00,004 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:00,004 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:07:00,007 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:07:05,019 - wandb.wandb_agent - INFO - Running runs: ['kz8p1fcn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170705-kz8p1fcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kz8p1fcn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: kz8p1fcn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9340 | Val Loss: 2.6816
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.5241 | Val Loss: 2.3893
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.3234 | Val Loss: 2.2724
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.1991 | Val Loss: 2.2213
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.2768 | Val Loss: 2.3283
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.3593 | Val Loss: 2.3863
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.4075 | Val Loss: 2.4290
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.4466 | Val Loss: 2.4641
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.4789 | Val Loss: 2.4940
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.5082
wandb:   val_loss 2.52338
wandb: 
wandb: üöÄ View run ancient-sweep-24 at: https://wandb.ai/7shoe/domShift-extensive/runs/kz8p1fcn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170705-kz8p1fcn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.5082 | Val Loss: 2.5234
2025-03-26 17:07:45,591 - wandb.wandb_agent - INFO - Cleaning up finished run: kz8p1fcn
2025-03-26 17:07:46,091 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:46,091 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:07:46,094 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:07:51,106 - wandb.wandb_agent - INFO - Running runs: ['rwfvrvru']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170752-rwfvrvru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rwfvrvru
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: rwfvrvru
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2983 | Val Loss: 3.0455
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.9489 | Val Loss: 2.8037
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.6726 | Val Loss: 2.4994
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.3697 | Val Loss: 2.2422
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.1863 | Val Loss: 2.1351
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.1186 | Val Loss: 2.1022
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.1016 | Val Loss: 2.0992
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.1053 | Val Loss: 2.1114
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.1228 | Val Loss: 2.1333
wandb: - 32.851 MB of 32.851 MB uploadedwandb: \ 32.851 MB of 32.871 MB uploadedwandb: | 32.871 MB of 32.871 MB uploadedwandb: / 32.871 MB of 32.871 MB uploadedwandb: - 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.14655
wandb:   val_loss 2.15921
wandb: 
wandb: üöÄ View run zesty-sweep-30 at: https://wandb.ai/7shoe/domShift-extensive/runs/rwfvrvru
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170752-rwfvrvru/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.1466 | Val Loss: 2.1592
2025-03-26 17:08:21,528 - wandb.wandb_agent - INFO - Cleaning up finished run: rwfvrvru
2025-03-26 17:08:22,005 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:08:22,005 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:08:22,009 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:08:27,021 - wandb.wandb_agent - INFO - Running runs: ['av7tw21s']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170826-av7tw21s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/av7tw21s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: av7tw21s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2186 | Val Loss: 1.0733
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1310 | Val Loss: 1.1873
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2538 | Val Loss: 1.3283
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3982 | Val Loss: 1.4908
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5277 | Val Loss: 1.5477
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5599 | Val Loss: 1.5565
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5504 | Val Loss: 1.5709
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6323 | Val Loss: 1.7164
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6428 | Val Loss: 1.5343
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.47097
wandb:   val_loss 1.48602
wandb: 
wandb: üöÄ View run vibrant-sweep-36 at: https://wandb.ai/7shoe/domShift-extensive/runs/av7tw21s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170826-av7tw21s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4710 | Val Loss: 1.4860
2025-03-26 17:08:57,526 - wandb.wandb_agent - INFO - Cleaning up finished run: av7tw21s
2025-03-26 17:09:08,686 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:08,686 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:09:08,688 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:09:13,701 - wandb.wandb_agent - INFO - Running runs: ['b4g9qm30']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170914-b4g9qm30
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b4g9qm30
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: b4g9qm30
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.4764 | Val Loss: 3.3748
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.5224 | Val Loss: 3.6435
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.6833 | Val Loss: 3.7038
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.7259 | Val Loss: 3.7387
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.7477 | Val Loss: 3.7429
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.7514 | Val Loss: 3.7527
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.7618 | Val Loss: 3.7664
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.7819 | Val Loss: 3.7960
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.8064 | Val Loss: 3.8086
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.80433
wandb:   val_loss 3.783
wandb: 
wandb: üöÄ View run autumn-sweep-41 at: https://wandb.ai/7shoe/domShift-extensive/runs/b4g9qm30
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170914-b4g9qm30/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.8043 | Val Loss: 3.7830
2025-03-26 17:09:49,205 - wandb.wandb_agent - INFO - Cleaning up finished run: b4g9qm30
2025-03-26 17:09:49,889 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:49,889 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:09:49,892 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:09:54,904 - wandb.wandb_agent - INFO - Running runs: ['g8kqi5a6']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170954-g8kqi5a6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-48
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/g8kqi5a6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: g8kqi5a6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0214 | Val Loss: 2.0883
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1333 | Val Loss: 2.0974
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0432 | Val Loss: 1.9365
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8585 | Val Loss: 1.7263
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6844 | Val Loss: 1.6083
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5826 | Val Loss: 1.5328
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5165 | Val Loss: 1.4784
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4704 | Val Loss: 1.4423
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4357 | Val Loss: 1.4142
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.41065
wandb:   val_loss 1.39587
wandb: 
wandb: üöÄ View run feasible-sweep-48 at: https://wandb.ai/7shoe/domShift-extensive/runs/g8kqi5a6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170954-g8kqi5a6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4107 | Val Loss: 1.3959
2025-03-26 17:10:25,359 - wandb.wandb_agent - INFO - Cleaning up finished run: g8kqi5a6
2025-03-26 17:10:26,556 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:10:26,556 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:10:26,558 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:10:31,570 - wandb.wandb_agent - INFO - Running runs: ['co4sugea']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171031-co4sugea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/co4sugea
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: co4sugea
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5735 | Val Loss: 1.4299
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3054 | Val Loss: 1.2090
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1681 | Val Loss: 1.1280
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1179 | Val Loss: 1.1336
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1407 | Val Loss: 1.1355
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1261 | Val Loss: 1.1156
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1105 | Val Loss: 1.1052
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1054 | Val Loss: 1.1036
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1014 | Val Loss: 1.1001
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09998
wandb:   val_loss 1.09882
wandb: 
wandb: üöÄ View run feasible-sweep-52 at: https://wandb.ai/7shoe/domShift-extensive/runs/co4sugea
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171031-co4sugea/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1000 | Val Loss: 1.0988
2025-03-26 17:11:42,579 - wandb.wandb_agent - INFO - Cleaning up finished run: co4sugea
2025-03-26 17:11:43,090 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:11:43,090 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:11:43,093 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:11:48,105 - wandb.wandb_agent - INFO - Running runs: ['4ygrh6c8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171148-4ygrh6c8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4ygrh6c8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4ygrh6c8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8052 | Val Loss: 1.8293
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.9927 | Val Loss: 2.1924
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1913 | Val Loss: 2.1959
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0987 | Val Loss: 1.9259
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.7475 | Val Loss: 1.5837
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5660 | Val Loss: 1.5499
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5318 | Val Loss: 1.4965
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4423 | Val Loss: 1.3586
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2910 | Val Loss: 1.1909
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13138
wandb:   val_loss 1.07436
wandb: 
wandb: üöÄ View run rich-sweep-62 at: https://wandb.ai/7shoe/domShift-extensive/runs/4ygrh6c8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171148-4ygrh6c8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1314 | Val Loss: 1.0744
2025-03-26 17:12:18,602 - wandb.wandb_agent - INFO - Cleaning up finished run: 4ygrh6c8
2025-03-26 17:12:19,122 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:19,122 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:12:19,124 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:12:24,137 - wandb.wandb_agent - INFO - Running runs: ['5e760wgf']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171224-5e760wgf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5e760wgf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 5e760wgf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.4809 | Val Loss: 3.1204
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.0004 | Val Loss: 2.8520
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6615 | Val Loss: 2.4380
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3236 | Val Loss: 2.2397
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2305 | Val Loss: 2.2318
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2579 | Val Loss: 2.2827
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3062 | Val Loss: 2.3292
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3491 | Val Loss: 2.3679
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3841 | Val Loss: 2.3993
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.41139
wandb:   val_loss 2.42154
wandb: 
wandb: üöÄ View run major-sweep-65 at: https://wandb.ai/7shoe/domShift-extensive/runs/5e760wgf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171224-5e760wgf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4114 | Val Loss: 2.4215
2025-03-26 17:13:04,692 - wandb.wandb_agent - INFO - Cleaning up finished run: 5e760wgf
2025-03-26 17:13:05,199 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:05,200 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:13:05,202 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:13:10,215 - wandb.wandb_agent - INFO - Running runs: ['mj4esx06']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171311-mj4esx06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-73
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mj4esx06
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mj4esx06
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6496 | Val Loss: 1.4109
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3298 | Val Loss: 1.2779
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2952 | Val Loss: 1.2985
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3038 | Val Loss: 1.3030
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3009 | Val Loss: 1.3023
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3014 | Val Loss: 1.3051
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3151 | Val Loss: 1.3321
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3416 | Val Loss: 1.3438
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3370 | Val Loss: 1.3245
wandb: - 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33877
wandb:   val_loss 1.38504
wandb: 
wandb: üöÄ View run wobbly-sweep-73 at: https://wandb.ai/7shoe/domShift-extensive/runs/mj4esx06
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171311-mj4esx06/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3388 | Val Loss: 1.3850
2025-03-26 17:13:45,706 - wandb.wandb_agent - INFO - Cleaning up finished run: mj4esx06
2025-03-26 17:13:46,433 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:46,433 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:13:46,436 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:13:51,448 - wandb.wandb_agent - INFO - Running runs: ['4ecplc6k']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171352-4ecplc6k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-79
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4ecplc6k
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4ecplc6k
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0358 | Val Loss: 1.6621
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6060 | Val Loss: 1.5515
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4987 | Val Loss: 1.4088
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3465 | Val Loss: 1.2941
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2463 | Val Loss: 1.2072
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2048 | Val Loss: 1.2110
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2267 | Val Loss: 1.2415
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2464 | Val Loss: 1.2465
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2428 | Val Loss: 1.2383
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23655
wandb:   val_loss 1.23521
wandb: 
wandb: üöÄ View run light-sweep-79 at: https://wandb.ai/7shoe/domShift-extensive/runs/4ecplc6k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171352-4ecplc6k/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2365 | Val Loss: 1.2352
2025-03-26 17:14:32,004 - wandb.wandb_agent - INFO - Cleaning up finished run: 4ecplc6k
2025-03-26 17:14:32,731 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:14:32,731 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:14:32,734 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:14:37,747 - wandb.wandb_agent - INFO - Running runs: ['mzg525v2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171437-mzg525v2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-83
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mzg525v2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: mzg525v2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6651 | Val Loss: 1.3508
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2807 | Val Loss: 1.2109
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1847 | Val Loss: 1.1390
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1134 | Val Loss: 1.0797
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0716 | Val Loss: 1.0578
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0580 | Val Loss: 1.0535
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0585 | Val Loss: 1.0595
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0668 | Val Loss: 1.0702
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0758 | Val Loss: 1.0771
wandb: - 32.856 MB of 32.876 MB uploadedwandb: \ 32.856 MB of 32.876 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07935
wandb:   val_loss 1.0742
wandb: 
wandb: üöÄ View run dazzling-sweep-83 at: https://wandb.ai/7shoe/domShift-extensive/runs/mzg525v2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171437-mzg525v2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0794 | Val Loss: 1.0742
2025-03-26 17:15:03,090 - wandb.wandb_agent - INFO - Cleaning up finished run: mzg525v2
2025-03-26 17:15:03,689 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:15:03,689 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:15:03,692 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:15:08,704 - wandb.wandb_agent - INFO - Running runs: ['ceokcykx']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171508-ceokcykx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ceokcykx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ceokcykx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5348 | Val Loss: 1.3827
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3591 | Val Loss: 1.2968
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2785 | Val Loss: 1.3064
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2994 | Val Loss: 1.3242
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3419 | Val Loss: 1.3320
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3089 | Val Loss: 1.2715
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2422 | Val Loss: 1.2073
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2063 | Val Loss: 1.2169
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2162 | Val Loss: 1.2128
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21881
wandb:   val_loss 1.21589
wandb: 
wandb: üöÄ View run whole-sweep-87 at: https://wandb.ai/7shoe/domShift-extensive/runs/ceokcykx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171508-ceokcykx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2188 | Val Loss: 1.2159
2025-03-26 17:15:59,458 - wandb.wandb_agent - INFO - Cleaning up finished run: ceokcykx
2025-03-26 17:15:59,994 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:15:59,994 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:15:59,997 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:16:05,009 - wandb.wandb_agent - INFO - Running runs: ['afczy5yl']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171604-afczy5yl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-94
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/afczy5yl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: afczy5yl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6279 | Val Loss: 1.5199
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5192 | Val Loss: 1.5011
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4795 | Val Loss: 1.4466
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3714 | Val Loss: 1.2944
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2751 | Val Loss: 1.2203
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1058 | Val Loss: 1.0459
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0262 | Val Loss: 1.0365
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0728 | Val Loss: 1.0505
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0414 | Val Loss: 0.9992
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.00332
wandb:   val_loss 0.98247
wandb: 
wandb: üöÄ View run confused-sweep-94 at: https://wandb.ai/7shoe/domShift-extensive/runs/afczy5yl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171604-afczy5yl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0033 | Val Loss: 0.9825
2025-03-26 17:16:55,739 - wandb.wandb_agent - INFO - Cleaning up finished run: afczy5yl
2025-03-26 17:16:56,267 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:56,267 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:16:56,270 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:17:01,283 - wandb.wandb_agent - INFO - Running runs: ['59l44l65']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171702-59l44l65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-102
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/59l44l65
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 59l44l65
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.4281 | Val Loss: 2.0456
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2541 | Val Loss: 2.3839
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.3808 | Val Loss: 2.3846
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3713 | Val Loss: 2.3449
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3298 | Val Loss: 2.3168
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3083 | Val Loss: 2.3043
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3085 | Val Loss: 2.3140
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3188 | Val Loss: 2.3221
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3283 | Val Loss: 2.3353
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.031 MB of 137.031 MB uploadedwandb: | 137.190 MB of 137.190 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.34356
wandb:   val_loss 2.35282
wandb: 
wandb: üöÄ View run splendid-sweep-102 at: https://wandb.ai/7shoe/domShift-extensive/runs/59l44l65
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171702-59l44l65/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3436 | Val Loss: 2.3528
2025-03-26 17:18:22,475 - wandb.wandb_agent - INFO - Cleaning up finished run: 59l44l65
2025-03-26 17:18:22,901 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:18:22,901 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:18:22,904 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:18:27,917 - wandb.wandb_agent - INFO - Running runs: ['7zro1wfj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171828-7zro1wfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-111
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7zro1wfj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7zro1wfj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4048 | Val Loss: 1.1668
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0943 | Val Loss: 1.0900
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0996 | Val Loss: 1.1046
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1106 | Val Loss: 1.1146
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1175 | Val Loss: 1.1202
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1232 | Val Loss: 1.1312
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1320 | Val Loss: 1.1192
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1238 | Val Loss: 1.1292
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1306 | Val Loss: 1.1307
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13118
wandb:   val_loss 1.13171
wandb: 
wandb: üöÄ View run usual-sweep-111 at: https://wandb.ai/7shoe/domShift-extensive/runs/7zro1wfj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171828-7zro1wfj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1312 | Val Loss: 1.1317
2025-03-26 17:19:18,616 - wandb.wandb_agent - INFO - Cleaning up finished run: 7zro1wfj
2025-03-26 17:19:19,099 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:19:19,099 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:19:19,102 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:19:24,114 - wandb.wandb_agent - INFO - Running runs: ['btxt8zc8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171924-btxt8zc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-119
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/btxt8zc8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: btxt8zc8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1890 | Val Loss: 0.9943
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0348 | Val Loss: 1.0770
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0877 | Val Loss: 1.0712
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0730 | Val Loss: 1.0769
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0800 | Val Loss: 1.0865
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0913 | Val Loss: 1.0944
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1110 | Val Loss: 1.1278
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1257 | Val Loss: 1.1224
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1431 | Val Loss: 1.1594
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá
wandb:   val_loss ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16155
wandb:   val_loss 1.16754
wandb: 
wandb: üöÄ View run fresh-sweep-119 at: https://wandb.ai/7shoe/domShift-extensive/runs/btxt8zc8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171924-btxt8zc8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1616 | Val Loss: 1.1675
2025-03-26 17:20:14,898 - wandb.wandb_agent - INFO - Cleaning up finished run: btxt8zc8
2025-03-26 17:20:15,581 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:15,581 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:20:15,584 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:20:20,597 - wandb.wandb_agent - INFO - Running runs: ['0tggio1o']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172021-0tggio1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-126
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0tggio1o
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 0tggio1o
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5163 | Val Loss: 1.3210
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2463 | Val Loss: 1.1668
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1341 | Val Loss: 1.1192
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1152 | Val Loss: 1.1019
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0830 | Val Loss: 1.0656
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0963 | Val Loss: 1.1261
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1337 | Val Loss: 1.1275
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1105 | Val Loss: 1.0942
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0805 | Val Loss: 1.0635
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.095 MB uploadedwandb: | 137.075 MB of 137.095 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05267
wandb:   val_loss 1.04281
wandb: 
wandb: üöÄ View run soft-sweep-126 at: https://wandb.ai/7shoe/domShift-extensive/runs/0tggio1o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172021-0tggio1o/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0527 | Val Loss: 1.0428
2025-03-26 17:20:56,014 - wandb.wandb_agent - INFO - Cleaning up finished run: 0tggio1o
2025-03-26 17:20:56,805 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:56,805 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:20:56,808 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.9
2025-03-26 17:21:01,820 - wandb.wandb_agent - INFO - Running runs: ['zhdj16qe']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172102-zhdj16qe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-131
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zhdj16qe
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zhdj16qe
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.3410 | Val Loss: 1.3782
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.4741 | Val Loss: 1.5441
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.5482 | Val Loss: 1.5139
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.4844 | Val Loss: 1.4568
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.4174 | Val Loss: 1.3696
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.3451 | Val Loss: 1.3221
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.3095 | Val Loss: 1.2996
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.2985 | Val Loss: 1.3016
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.3043 | Val Loss: 1.3039
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30287
wandb:   val_loss 1.30259
wandb: 
wandb: üöÄ View run misunderstood-sweep-131 at: https://wandb.ai/7shoe/domShift-extensive/runs/zhdj16qe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172102-zhdj16qe/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.3029 | Val Loss: 1.3026
2025-03-26 17:21:42,342 - wandb.wandb_agent - INFO - Cleaning up finished run: zhdj16qe
2025-03-26 17:21:43,348 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:21:43,349 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:21:43,351 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:21:48,364 - wandb.wandb_agent - INFO - Running runs: ['m1gmzn4r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172149-m1gmzn4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-134
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m1gmzn4r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: m1gmzn4r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2205 | Val Loss: 0.9946
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2718 | Val Loss: 1.4485
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3820 | Val Loss: 1.2261
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2561 | Val Loss: 1.2783
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2804 | Val Loss: 1.2808
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2806 | Val Loss: 1.2808
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2806 | Val Loss: 1.2808
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2805 | Val Loss: 1.2808
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2805 | Val Loss: 1.2808
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.28053
wandb:   val_loss 1.28073
wandb: 
wandb: üöÄ View run major-sweep-134 at: https://wandb.ai/7shoe/domShift-extensive/runs/m1gmzn4r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172149-m1gmzn4r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2805 | Val Loss: 1.2807
2025-03-26 17:22:33,954 - wandb.wandb_agent - INFO - Cleaning up finished run: m1gmzn4r
2025-03-26 17:22:34,556 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:22:34,556 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:22:34,560 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:22:39,573 - wandb.wandb_agent - INFO - Running runs: ['1t6qmn5h']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172240-1t6qmn5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-141
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1t6qmn5h
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1t6qmn5h
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5293 | Val Loss: 1.2575
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1245 | Val Loss: 1.0125
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9676 | Val Loss: 0.9037
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.8643 | Val Loss: 0.8266
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.8154 | Val Loss: 0.8021
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.7984 | Val Loss: 0.7901
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.7858 | Val Loss: 0.7778
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.7743 | Val Loss: 0.7679
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.7656 | Val Loss: 0.7610
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.76015
wandb:   val_loss 0.7567
wandb: 
wandb: üöÄ View run charmed-sweep-141 at: https://wandb.ai/7shoe/domShift-extensive/runs/1t6qmn5h
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172240-1t6qmn5h/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.7601 | Val Loss: 0.7567
2025-03-26 17:23:09,986 - wandb.wandb_agent - INFO - Cleaning up finished run: 1t6qmn5h
2025-03-26 17:23:10,899 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:23:10,899 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:23:10,901 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:23:15,914 - wandb.wandb_agent - INFO - Running runs: ['2wkwzhey']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172315-2wkwzhey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-145
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2wkwzhey
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 2wkwzhey
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6169 | Val Loss: 1.2815
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1915 | Val Loss: 1.0987
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9766 | Val Loss: 0.8478
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.8568 | Val Loss: 0.9339
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0186 | Val Loss: 1.0269
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0028 | Val Loss: 0.9944
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0176 | Val Loss: 1.0430
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0342 | Val Loss: 1.0092
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0152 | Val Loss: 1.0312
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0311
wandb:   val_loss 1.0326
wandb: 
wandb: üöÄ View run devoted-sweep-145 at: https://wandb.ai/7shoe/domShift-extensive/runs/2wkwzhey
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172315-2wkwzhey/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0311 | Val Loss: 1.0326
2025-03-26 17:23:51,435 - wandb.wandb_agent - INFO - Cleaning up finished run: 2wkwzhey
2025-03-26 17:23:52,229 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:23:52,229 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:23:52,232 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:23:57,240 - wandb.wandb_agent - INFO - Running runs: ['5v4azufw']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172357-5v4azufw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-149
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5v4azufw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5v4azufw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7658 | Val Loss: 1.5469
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4062 | Val Loss: 1.3820
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3874 | Val Loss: 1.3685
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3535 | Val Loss: 1.3403
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3364 | Val Loss: 1.3348
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3427 | Val Loss: 1.3461
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3508 | Val Loss: 1.3536
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3553 | Val Loss: 1.3547
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3529 | Val Loss: 1.3500
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34693
wandb:   val_loss 1.34389
wandb: 
wandb: üöÄ View run vital-sweep-149 at: https://wandb.ai/7shoe/domShift-extensive/runs/5v4azufw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172357-5v4azufw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3469 | Val Loss: 1.3439
2025-03-26 17:24:42,909 - wandb.wandb_agent - INFO - Cleaning up finished run: 5v4azufw
2025-03-26 17:24:44,083 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:44,083 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:24:44,087 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:24:49,099 - wandb.wandb_agent - INFO - Running runs: ['sj9iofmi']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172449-sj9iofmi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-157
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sj9iofmi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sj9iofmi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.2315 | Val Loss: 2.1901
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0483 | Val Loss: 1.7057
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3749 | Val Loss: 1.2453
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2516 | Val Loss: 1.2622
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2353 | Val Loss: 1.3070
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3820 | Val Loss: 1.4202
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4161 | Val Loss: 1.4014
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3766 | Val Loss: 1.3556
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3594 | Val Loss: 1.3660
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36842
wandb:   val_loss 1.36737
wandb: 
wandb: üöÄ View run different-sweep-157 at: https://wandb.ai/7shoe/domShift-extensive/runs/sj9iofmi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172449-sj9iofmi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3684 | Val Loss: 1.3674
2025-03-26 17:25:39,774 - wandb.wandb_agent - INFO - Cleaning up finished run: sj9iofmi
2025-03-26 17:25:40,578 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:25:40,578 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.8
2025-03-26 17:25:40,581 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.8
2025-03-26 17:25:45,593 - wandb.wandb_agent - INFO - Running runs: ['q1khp7hk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172546-q1khp7hk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-161
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/q1khp7hk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: q1khp7hk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9829 | Val Loss: 2.9600
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.0582 | Val Loss: 3.1249
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.1247 | Val Loss: 3.1010
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 3.0601 | Val Loss: 3.0204
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.9891 | Val Loss: 2.9600
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.9383 | Val Loss: 2.9180
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.9020 | Val Loss: 2.8870
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.8758 | Val Loss: 2.8653
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.8574 | Val Loss: 2.8496
wandb: - 10.262 MB of 10.262 MB uploadedwandb: \ 10.262 MB of 10.262 MB uploadedwandb: | 10.291 MB of 10.291 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.84407
wandb:   val_loss 2.83912
wandb: 
wandb: üöÄ View run cool-sweep-161 at: https://wandb.ai/7shoe/domShift-extensive/runs/q1khp7hk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172546-q1khp7hk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.8441 | Val Loss: 2.8391
2025-03-26 17:26:31,291 - wandb.wandb_agent - INFO - Cleaning up finished run: q1khp7hk
2025-03-26 17:26:31,861 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:26:31,862 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:26:31,865 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:26:36,877 - wandb.wandb_agent - INFO - Running runs: ['jvgx0ze1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172638-jvgx0ze1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-169
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jvgx0ze1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jvgx0ze1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3984 | Val Loss: 1.2078
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1545 | Val Loss: 1.1070
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0910 | Val Loss: 1.0805
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0767 | Val Loss: 1.0713
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0680 | Val Loss: 1.0651
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0638 | Val Loss: 1.0615
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0600 | Val Loss: 1.0583
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0573 | Val Loss: 1.0558
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0549 | Val Loss: 1.0537
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05302
wandb:   val_loss 1.05197
wandb: 
wandb: üöÄ View run soft-sweep-169 at: https://wandb.ai/7shoe/domShift-extensive/runs/jvgx0ze1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172638-jvgx0ze1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0530 | Val Loss: 1.0520
2025-03-26 17:27:32,666 - wandb.wandb_agent - INFO - Cleaning up finished run: jvgx0ze1
2025-03-26 17:27:33,890 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:27:33,890 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:27:33,893 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:27:38,905 - wandb.wandb_agent - INFO - Running runs: ['jjil4p1u']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172739-jjil4p1u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-175
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jjil4p1u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jjil4p1u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0281 | Val Loss: 1.9742
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.9661 | Val Loss: 1.8743
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7041 | Val Loss: 1.5616
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5158 | Val Loss: 1.4639
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4296 | Val Loss: 1.3774
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3426 | Val Loss: 1.3084
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2926 | Val Loss: 1.2724
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2572 | Val Loss: 1.2358
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2217 | Val Loss: 1.2055
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19749
wandb:   val_loss 1.1876
wandb: 
wandb: üöÄ View run denim-sweep-175 at: https://wandb.ai/7shoe/domShift-extensive/runs/jjil4p1u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172739-jjil4p1u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1975 | Val Loss: 1.1876
2025-03-26 17:28:19,532 - wandb.wandb_agent - INFO - Cleaning up finished run: jjil4p1u
2025-03-26 17:28:20,065 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:28:20,065 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 1
2025-03-26 17:28:20,067 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=1
2025-03-26 17:28:25,080 - wandb.wandb_agent - INFO - Running runs: ['ukg0m5ob']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172826-ukg0m5ob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-182
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ukg0m5ob
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ukg0m5ob
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 5.4719 | Val Loss: 5.3300
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 5.3424 | Val Loss: 5.2845
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 5.3147 | Val Loss: 5.2684
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 5.3049 | Val Loss: 5.2592
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 5.3017 | Val Loss: 5.2567
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 5.2978 | Val Loss: 5.2520
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 5.2948 | Val Loss: 5.2527
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 5.2943 | Val Loss: 5.2515
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 5.2933 | Val Loss: 5.2524
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 33.058 MB uploadedwandb: | 32.965 MB of 33.058 MB uploadedwandb: / 33.058 MB of 33.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.29217
wandb:   val_loss 5.24733
wandb: 
wandb: üöÄ View run fragrant-sweep-182 at: https://wandb.ai/7shoe/domShift-extensive/runs/ukg0m5ob
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172826-ukg0m5ob/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 5.2922 | Val Loss: 5.2473
2025-03-26 17:29:10,741 - wandb.wandb_agent - INFO - Cleaning up finished run: ukg0m5ob
2025-03-26 17:29:11,334 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:29:11,334 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:29:11,337 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:29:16,348 - wandb.wandb_agent - INFO - Running runs: ['9dy1ur9z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172916-9dy1ur9z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-188
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9dy1ur9z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9dy1ur9z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2429 | Val Loss: 3.3668
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.2517 | Val Loss: 3.0205
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.8028 | Val Loss: 2.7228
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.6806 | Val Loss: 2.6189
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5596 | Val Loss: 2.4954
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4486 | Val Loss: 2.4036
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3706 | Val Loss: 2.3377
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3166 | Val Loss: 2.2930
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2825 | Val Loss: 2.2776
wandb: - 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.27511
wandb:   val_loss 2.27182
wandb: 
wandb: üöÄ View run charmed-sweep-188 at: https://wandb.ai/7shoe/domShift-extensive/runs/9dy1ur9z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172916-9dy1ur9z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2751 | Val Loss: 2.2718
2025-03-26 17:30:06,971 - wandb.wandb_agent - INFO - Cleaning up finished run: 9dy1ur9z
2025-03-26 17:30:31,818 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:30:31,819 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:30:31,822 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 17:30:36,834 - wandb.wandb_agent - INFO - Running runs: ['s84erdb9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173038-s84erdb9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-194
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s84erdb9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: s84erdb9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6052 | Val Loss: 1.5632
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4583 | Val Loss: 1.4048
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4791 | Val Loss: 1.5165
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4777 | Val Loss: 1.4422
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3645 | Val Loss: 1.2138
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1881 | Val Loss: 1.1688
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1455 | Val Loss: 1.1221
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1188 | Val Loss: 1.1168
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1166 | Val Loss: 1.1162
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11598
wandb:   val_loss 1.1153
wandb: 
wandb: üöÄ View run amber-sweep-194 at: https://wandb.ai/7shoe/domShift-extensive/runs/s84erdb9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173038-s84erdb9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1160 | Val Loss: 1.1153
2025-03-26 17:31:22,531 - wandb.wandb_agent - INFO - Cleaning up finished run: s84erdb9
2025-03-26 17:31:23,199 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:31:23,199 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:31:23,201 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:31:28,214 - wandb.wandb_agent - INFO - Running runs: ['s50bzygw']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173128-s50bzygw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-201
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s50bzygw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s50bzygw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9974 | Val Loss: 1.8598
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7945 | Val Loss: 1.7025
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6639 | Val Loss: 1.6285
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6344 | Val Loss: 1.6379
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6410 | Val Loss: 1.6326
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6167 | Val Loss: 1.5703
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5406 | Val Loss: 1.5117
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4993 | Val Loss: 1.4782
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4578 | Val Loss: 1.4345
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4136
wandb:   val_loss 1.38575
wandb: 
wandb: üöÄ View run light-sweep-201 at: https://wandb.ai/7shoe/domShift-extensive/runs/s50bzygw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173128-s50bzygw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4136 | Val Loss: 1.3858
2025-03-26 17:31:58,627 - wandb.wandb_agent - INFO - Cleaning up finished run: s50bzygw
2025-03-26 17:31:59,136 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:31:59,137 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:31:59,140 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:32:04,151 - wandb.wandb_agent - INFO - Running runs: ['3txqgpbm']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173204-3txqgpbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-204
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3txqgpbm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3txqgpbm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0166 | Val Loss: 2.5005
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.4885 | Val Loss: 2.5069
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4845 | Val Loss: 2.4578
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4283 | Val Loss: 2.3967
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3654 | Val Loss: 2.3341
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3070 | Val Loss: 2.2803
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2575 | Val Loss: 2.2354
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2162 | Val Loss: 2.1974
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1809 | Val Loss: 2.1648
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.469 MB of 137.469 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.15058
wandb:   val_loss 2.13662
wandb: 
wandb: üöÄ View run royal-sweep-204 at: https://wandb.ai/7shoe/domShift-extensive/runs/3txqgpbm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173204-3txqgpbm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1506 | Val Loss: 2.1366
2025-03-26 17:33:30,438 - wandb.wandb_agent - INFO - Cleaning up finished run: 3txqgpbm
2025-03-26 17:33:30,993 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:33:30,994 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:33:30,997 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:33:36,011 - wandb.wandb_agent - INFO - Running runs: ['n4efddxn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173337-n4efddxn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-213
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/n4efddxn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: n4efddxn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5742 | Val Loss: 1.3114
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2110 | Val Loss: 1.2119
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1958 | Val Loss: 1.1721
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1722 | Val Loss: 1.1790
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1806 | Val Loss: 1.1771
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1769 | Val Loss: 1.1782
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1798 | Val Loss: 1.1793
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1793 | Val Loss: 1.1800
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1797 | Val Loss: 1.1797
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17947
wandb:   val_loss 1.17973
wandb: 
wandb: üöÄ View run olive-sweep-213 at: https://wandb.ai/7shoe/domShift-extensive/runs/n4efddxn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173337-n4efddxn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1795 | Val Loss: 1.1797
2025-03-26 17:34:31,791 - wandb.wandb_agent - INFO - Cleaning up finished run: n4efddxn
2025-03-26 17:34:32,437 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:34:32,437 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:34:32,440 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimCLR --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:34:37,452 - wandb.wandb_agent - INFO - Running runs: ['k6nsvl7j']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173438-k6nsvl7j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-220
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/k6nsvl7j
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: k6nsvl7j
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.3547 | Val Loss: 3.7954
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.5806 | Val Loss: 3.3339
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.1830 | Val Loss: 3.0396
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.9515 | Val Loss: 2.8691
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8337 | Val Loss: 2.7212
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.6785 | Val Loss: 2.5829
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.5753 | Val Loss: 2.5022
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.5029 | Val Loss: 2.4357
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.4460 | Val Loss: 2.4041
wandb: - 32.969 MB of 32.969 MB uploadedwandb: \ 32.969 MB of 32.969 MB uploadedwandb: | 32.988 MB of 32.988 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.4204
wandb:   val_loss 2.37089
wandb: 
wandb: üöÄ View run pretty-sweep-220 at: https://wandb.ai/7shoe/domShift-extensive/runs/k6nsvl7j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173438-k6nsvl7j/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.4204 | Val Loss: 2.3709
2025-03-26 17:35:12,955 - wandb.wandb_agent - INFO - Cleaning up finished run: k6nsvl7j
2025-03-26 17:35:13,569 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:13,569 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:35:13,572 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:35:18,584 - wandb.wandb_agent - INFO - Running runs: ['jvqvdpny']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173518-jvqvdpny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-224
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jvqvdpny
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: jvqvdpny
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2656 | Val Loss: 1.1761
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2137 | Val Loss: 1.2440
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2665 | Val Loss: 1.2935
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2610 | Val Loss: 1.2250
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2142 | Val Loss: 1.2021
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1703 | Val Loss: 1.1483
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1326 | Val Loss: 1.1204
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1178 | Val Loss: 1.1254
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1333 | Val Loss: 1.1368
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.082 MB of 137.082 MB uploadedwandb: / 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13429
wandb:   val_loss 1.1323
wandb: 
wandb: üöÄ View run silver-sweep-224 at: https://wandb.ai/7shoe/domShift-extensive/runs/jvqvdpny
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173518-jvqvdpny/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1343 | Val Loss: 1.1323
2025-03-26 17:35:54,079 - wandb.wandb_agent - INFO - Cleaning up finished run: jvqvdpny
2025-03-26 17:35:54,674 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:54,674 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:35:54,677 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:35:59,689 - wandb.wandb_agent - INFO - Running runs: ['kmt25p9r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173559-kmt25p9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-228
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kmt25p9r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: kmt25p9r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4092 | Val Loss: 1.3344
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3863 | Val Loss: 1.4237
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4541 | Val Loss: 1.4687
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4197 | Val Loss: 1.3631
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3439 | Val Loss: 1.3292
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3277 | Val Loss: 1.3192
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3127 | Val Loss: 1.3043
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3007 | Val Loss: 1.2927
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2828 | Val Loss: 1.2668
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25713
wandb:   val_loss 1.24901
wandb: 
wandb: üöÄ View run neat-sweep-228 at: https://wandb.ai/7shoe/domShift-extensive/runs/kmt25p9r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173559-kmt25p9r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2571 | Val Loss: 1.2490
2025-03-26 17:36:30,120 - wandb.wandb_agent - INFO - Cleaning up finished run: kmt25p9r
2025-03-26 17:36:30,923 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:36:30,924 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:36:30,926 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:36:35,939 - wandb.wandb_agent - INFO - Running runs: ['ar096dqc']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173636-ar096dqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-232
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ar096dqc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ar096dqc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4287 | Val Loss: 1.3175
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3991 | Val Loss: 1.5009
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5302 | Val Loss: 1.5217
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4868 | Val Loss: 1.4558
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4433 | Val Loss: 1.4268
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4187 | Val Loss: 1.4069
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3969 | Val Loss: 1.3836
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3878 | Val Loss: 1.3874
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3812 | Val Loss: 1.3706
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÇ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36608
wandb:   val_loss 1.37602
wandb: 
wandb: üöÄ View run ruby-sweep-232 at: https://wandb.ai/7shoe/domShift-extensive/runs/ar096dqc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173636-ar096dqc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3661 | Val Loss: 1.3760
2025-03-26 17:37:11,442 - wandb.wandb_agent - INFO - Cleaning up finished run: ar096dqc
2025-03-26 17:37:11,998 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:37:11,998 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:37:12,001 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:37:17,013 - wandb.wandb_agent - INFO - Running runs: ['em1ia60n']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173718-em1ia60n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-237
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/em1ia60n
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: em1ia60n
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4874 | Val Loss: 1.2643
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1737 | Val Loss: 1.1008
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1303 | Val Loss: 1.1383
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1295 | Val Loss: 1.1082
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1104 | Val Loss: 1.1387
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1528 | Val Loss: 1.1361
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1385 | Val Loss: 1.1586
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1458 | Val Loss: 1.1322
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1475 | Val Loss: 1.1499
wandb: - 32.875 MB of 32.875 MB uploadedwandb: \ 32.875 MB of 32.875 MB uploadedwandb: | 32.968 MB of 32.968 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14416
wandb:   val_loss 1.15537
wandb: 
wandb: üöÄ View run wobbly-sweep-237 at: https://wandb.ai/7shoe/domShift-extensive/runs/em1ia60n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173718-em1ia60n/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1442 | Val Loss: 1.1554
2025-03-26 17:38:22,913 - wandb.wandb_agent - INFO - Cleaning up finished run: em1ia60n
2025-03-26 17:38:23,479 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:38:23,479 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:38:23,482 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:38:28,495 - wandb.wandb_agent - INFO - Running runs: ['gnvwib0n']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173830-gnvwib0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-246
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gnvwib0n
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: gnvwib0n
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4381 | Val Loss: 1.1799
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1368 | Val Loss: 1.1277
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1042 | Val Loss: 1.0796
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1025 | Val Loss: 1.1177
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1333 | Val Loss: 1.1570
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1801 | Val Loss: 1.1987
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2119 | Val Loss: 1.2159
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2139 | Val Loss: 1.2138
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2177 | Val Loss: 1.2223
wandb: - 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22897
wandb:   val_loss 1.2353
wandb: 
wandb: üöÄ View run valiant-sweep-246 at: https://wandb.ai/7shoe/domShift-extensive/runs/gnvwib0n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173830-gnvwib0n/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2290 | Val Loss: 1.2353
2025-03-26 17:39:03,957 - wandb.wandb_agent - INFO - Cleaning up finished run: gnvwib0n
2025-03-26 17:39:04,987 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:04,987 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:39:04,990 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:39:10,002 - wandb.wandb_agent - INFO - Running runs: ['xx99jtg5']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173910-xx99jtg5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-251
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xx99jtg5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: xx99jtg5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.4181 | Val Loss: 2.8762
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.7602 | Val Loss: 2.7172
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.7021 | Val Loss: 2.6780
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.6578 | Val Loss: 2.6167
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.5592 | Val Loss: 2.4885
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.4273 | Val Loss: 2.3655
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.3280 | Val Loss: 2.2907
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.2729 | Val Loss: 2.2548
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.2439 | Val Loss: 2.2327
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.872 MB of 32.872 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.22505
wandb:   val_loss 2.21801
wandb: 
wandb: üöÄ View run divine-sweep-251 at: https://wandb.ai/7shoe/domShift-extensive/runs/xx99jtg5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173910-xx99jtg5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.2250 | Val Loss: 2.2180
2025-03-26 17:39:40,433 - wandb.wandb_agent - INFO - Cleaning up finished run: xx99jtg5
2025-03-26 17:39:41,415 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:41,415 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:39:41,418 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:39:46,430 - wandb.wandb_agent - INFO - Running runs: ['sxnwksi4']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173946-sxnwksi4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-254
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sxnwksi4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sxnwksi4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1288 | Val Loss: 2.5543
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.4143 | Val Loss: 2.3613
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.3103 | Val Loss: 2.2504
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1724 | Val Loss: 2.1029
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0875 | Val Loss: 2.0838
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0870 | Val Loss: 2.0900
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0895 | Val Loss: 2.0853
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0776 | Val Loss: 2.0662
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0568 | Val Loss: 2.0477
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.330 MB uploadedwandb: | 137.311 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.03861
wandb:   val_loss 2.02884
wandb: 
wandb: üöÄ View run worthy-sweep-254 at: https://wandb.ai/7shoe/domShift-extensive/runs/sxnwksi4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173946-sxnwksi4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.0386 | Val Loss: 2.0288
2025-03-26 17:40:47,300 - wandb.wandb_agent - INFO - Cleaning up finished run: sxnwksi4
2025-03-26 17:40:48,140 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:40:48,140 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:40:48,143 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:40:53,156 - wandb.wandb_agent - INFO - Running runs: ['9h5oa5cv']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174053-9h5oa5cv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-263
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9h5oa5cv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9h5oa5cv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7850 | Val Loss: 1.7277
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2980 | Val Loss: 1.1794
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1111 | Val Loss: 1.0563
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1448 | Val Loss: 1.1821
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1823 | Val Loss: 1.1223
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1100 | Val Loss: 1.1098
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1119 | Val Loss: 1.1195
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1220 | Val Loss: 1.1237
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1225 | Val Loss: 1.1237
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.102 MB uploadedwandb: | 137.082 MB of 137.102 MB uploadedwandb: / 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12344
wandb:   val_loss 1.1245
wandb: 
wandb: üöÄ View run rural-sweep-263 at: https://wandb.ai/7shoe/domShift-extensive/runs/9h5oa5cv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174053-9h5oa5cv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1234 | Val Loss: 1.1245
2025-03-26 17:41:38,791 - wandb.wandb_agent - INFO - Cleaning up finished run: 9h5oa5cv
2025-03-26 17:41:39,505 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:41:39,505 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:41:39,507 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:41:44,519 - wandb.wandb_agent - INFO - Running runs: ['30kqjxni']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174145-30kqjxni
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-270
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/30kqjxni
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 30kqjxni
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7998 | Val Loss: 2.2998
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0412 | Val Loss: 1.9223
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.9104 | Val Loss: 1.8981
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8770 | Val Loss: 1.8497
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8173 | Val Loss: 1.7831
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.7573 | Val Loss: 1.7347
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.7216 | Val Loss: 1.7114
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.7072 | Val Loss: 1.7046
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.7047 | Val Loss: 1.7055
wandb: - 137.190 MB of 137.190 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.70711
wandb:   val_loss 1.70895
wandb: 
wandb: üöÄ View run firm-sweep-270 at: https://wandb.ai/7shoe/domShift-extensive/runs/30kqjxni
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174145-30kqjxni/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7071 | Val Loss: 1.7090
2025-03-26 17:43:00,559 - wandb.wandb_agent - INFO - Cleaning up finished run: 30kqjxni
2025-03-26 17:43:01,129 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:43:01,129 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:43:01,132 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:43:06,140 - wandb.wandb_agent - INFO - Running runs: ['2o2oqw11']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174307-2o2oqw11
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-278
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2o2oqw11
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2o2oqw11
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.7611 | Val Loss: 1.7916
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.8497 | Val Loss: 1.7505
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.7112 | Val Loss: 1.6793
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.6611 | Val Loss: 1.6530
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.6452 | Val Loss: 1.5878
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.3971 | Val Loss: 1.2013
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.1456 | Val Loss: 1.1276
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.1268 | Val Loss: 1.1219
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.1161 | Val Loss: 1.1101
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.279 MB uploadedwandb: | 137.259 MB of 137.279 MB uploadedwandb: / 137.279 MB of 137.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10672
wandb:   val_loss 1.10027
wandb: 
wandb: üöÄ View run grateful-sweep-278 at: https://wandb.ai/7shoe/domShift-extensive/runs/2o2oqw11
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174307-2o2oqw11/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.1067 | Val Loss: 1.1003
2025-03-26 17:44:01,815 - wandb.wandb_agent - INFO - Cleaning up finished run: 2o2oqw11
2025-03-26 17:44:02,340 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:44:02,341 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:44:02,343 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:44:07,356 - wandb.wandb_agent - INFO - Running runs: ['1v11iuc0']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174407-1v11iuc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-286
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1v11iuc0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1v11iuc0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3619 | Val Loss: 1.1676
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1241 | Val Loss: 1.0688
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0501 | Val Loss: 1.0402
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0394 | Val Loss: 1.0378
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0369 | Val Loss: 1.0362
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0364 | Val Loss: 1.0366
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0375 | Val Loss: 1.0382
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0390 | Val Loss: 1.0397
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0402 | Val Loss: 1.0391
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03963
wandb:   val_loss 1.04035
wandb: 
wandb: üöÄ View run dazzling-sweep-286 at: https://wandb.ai/7shoe/domShift-extensive/runs/1v11iuc0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174407-1v11iuc0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0396 | Val Loss: 1.0404
2025-03-26 17:45:23,460 - wandb.wandb_agent - INFO - Cleaning up finished run: 1v11iuc0
2025-03-26 17:45:24,204 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:45:24,204 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:45:24,206 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:45:29,219 - wandb.wandb_agent - INFO - Running runs: ['horavjlk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174530-horavjlk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-293
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/horavjlk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: horavjlk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.3996 | Val Loss: 1.0830
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.0710 | Val Loss: 1.0596
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.0581 | Val Loss: 1.0532
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.0538 | Val Loss: 1.0530
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.0530 | Val Loss: 1.0529
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0528 | Val Loss: 1.0530
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0529 | Val Loss: 1.0530
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0528 | Val Loss: 1.0529
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.0527 | Val Loss: 1.0528
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 33.059 MB of 33.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05264
wandb:   val_loss 1.05274
wandb: 
wandb: üöÄ View run crisp-sweep-293 at: https://wandb.ai/7shoe/domShift-extensive/runs/horavjlk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174530-horavjlk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.0526 | Val Loss: 1.0527
2025-03-26 17:46:19,975 - wandb.wandb_agent - INFO - Cleaning up finished run: horavjlk
2025-03-26 17:46:20,662 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:46:20,662 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:46:20,665 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:46:25,677 - wandb.wandb_agent - INFO - Running runs: ['awq55d8l']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174626-awq55d8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-300
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/awq55d8l
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: awq55d8l
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.1909 | Val Loss: 0.9706
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9386 | Val Loss: 0.9337
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9582 | Val Loss: 1.0005
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0312 | Val Loss: 1.0438
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0484 | Val Loss: 1.0561
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0630 | Val Loss: 1.0681
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0577 | Val Loss: 1.0337
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0086 | Val Loss: 0.9952
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9943 | Val Loss: 0.9934
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99415
wandb:   val_loss 0.9967
wandb: 
wandb: üöÄ View run jumping-sweep-300 at: https://wandb.ai/7shoe/domShift-extensive/runs/awq55d8l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174626-awq55d8l/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9942 | Val Loss: 0.9967
2025-03-26 17:47:11,344 - wandb.wandb_agent - INFO - Cleaning up finished run: awq55d8l
2025-03-26 17:47:11,955 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:47:11,955 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:47:11,958 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:47:16,970 - wandb.wandb_agent - INFO - Running runs: ['cgf8vvn4']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174716-cgf8vvn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-306
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cgf8vvn4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: cgf8vvn4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9306 | Val Loss: 1.7364
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6674 | Val Loss: 1.6539
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5610 | Val Loss: 1.4847
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4499 | Val Loss: 1.4290
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4553 | Val Loss: 1.4711
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4690 | Val Loss: 1.4649
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4563 | Val Loss: 1.4324
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4234 | Val Loss: 1.4212
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4269 | Val Loss: 1.4223
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.42704
wandb:   val_loss 1.42498
wandb: 
wandb: üöÄ View run fragrant-sweep-306 at: https://wandb.ai/7shoe/domShift-extensive/runs/cgf8vvn4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174716-cgf8vvn4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4270 | Val Loss: 1.4250
2025-03-26 17:47:52,417 - wandb.wandb_agent - INFO - Cleaning up finished run: cgf8vvn4
2025-03-26 17:47:53,009 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:47:53,009 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:47:53,012 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
2025-03-26 17:47:58,024 - wandb.wandb_agent - INFO - Running runs: ['20goah2o']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174759-20goah2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-310
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/20goah2o
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 20goah2o
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8349 | Val Loss: 1.8003
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7273 | Val Loss: 1.6309
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5930 | Val Loss: 1.5180
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4872 | Val Loss: 1.4372
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4229 | Val Loss: 1.3976
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3917 | Val Loss: 1.3757
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3728 | Val Loss: 1.3666
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3678 | Val Loss: 1.3666
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3696 | Val Loss: 1.3738
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37943
wandb:   val_loss 1.38816
wandb: 
wandb: üöÄ View run silvery-sweep-310 at: https://wandb.ai/7shoe/domShift-extensive/runs/20goah2o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174759-20goah2o/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3794 | Val Loss: 1.3882
2025-03-26 17:48:28,430 - wandb.wandb_agent - INFO - Cleaning up finished run: 20goah2o
2025-03-26 17:48:29,226 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:48:29,227 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:48:29,230 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:48:34,240 - wandb.wandb_agent - INFO - Running runs: ['an2v3wxq']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174834-an2v3wxq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-315
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/an2v3wxq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: an2v3wxq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5433 | Val Loss: 1.2848
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1983 | Val Loss: 1.2034
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2129 | Val Loss: 1.2103
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1993 | Val Loss: 1.1929
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1910 | Val Loss: 1.1903
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1897 | Val Loss: 1.1897
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1893 | Val Loss: 1.1895
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1892 | Val Loss: 1.1894
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1892 | Val Loss: 1.1894
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.18919
wandb:   val_loss 1.18941
wandb: 
wandb: üöÄ View run youthful-sweep-315 at: https://wandb.ai/7shoe/domShift-extensive/runs/an2v3wxq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174834-an2v3wxq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1892 | Val Loss: 1.1894
2025-03-26 17:49:50,384 - wandb.wandb_agent - INFO - Cleaning up finished run: an2v3wxq
2025-03-26 17:49:51,054 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:51,054 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:49:51,057 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:49:56,069 - wandb.wandb_agent - INFO - Running runs: ['4p4hmcbq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174957-4p4hmcbq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-324
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4p4hmcbq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4p4hmcbq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5346 | Val Loss: 1.3696
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4299 | Val Loss: 1.4340
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4866 | Val Loss: 1.5395
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5767 | Val Loss: 1.5925
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5898 | Val Loss: 1.5770
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5583 | Val Loss: 1.5375
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5192 | Val Loss: 1.5049
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4946 | Val Loss: 1.4825
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4661 | Val Loss: 1.4512
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÉ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.44332
wandb:   val_loss 1.43274
wandb: 
wandb: üöÄ View run leafy-sweep-324 at: https://wandb.ai/7shoe/domShift-extensive/runs/4p4hmcbq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174957-4p4hmcbq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4433 | Val Loss: 1.4327
2025-03-26 17:50:36,634 - wandb.wandb_agent - INFO - Cleaning up finished run: 4p4hmcbq
2025-03-26 17:50:37,303 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:50:37,303 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:50:37,305 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:50:42,318 - wandb.wandb_agent - INFO - Running runs: ['xk7cxphl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175043-xk7cxphl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-330
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xk7cxphl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xk7cxphl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.2216 | Val Loss: 2.9890
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.6121 | Val Loss: 2.3854
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.3852 | Val Loss: 2.4010
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.4185 | Val Loss: 2.4334
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.4493 | Val Loss: 2.4644
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.4736 | Val Loss: 2.4823
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.4888 | Val Loss: 2.4960
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.5037 | Val Loss: 2.5132
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.5213 | Val Loss: 2.5285
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.415 MB uploadedwandb: | 137.256 MB of 137.415 MB uploadedwandb: / 137.415 MB of 137.415 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.53361
wandb:   val_loss 2.53851
wandb: 
wandb: üöÄ View run resilient-sweep-330 at: https://wandb.ai/7shoe/domShift-extensive/runs/xk7cxphl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175043-xk7cxphl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.5336 | Val Loss: 2.5385
2025-03-26 17:52:08,567 - wandb.wandb_agent - INFO - Cleaning up finished run: xk7cxphl
2025-03-26 17:52:09,146 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:52:09,147 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:52:09,150 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:52:14,162 - wandb.wandb_agent - INFO - Running runs: ['57gs0npn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175214-57gs0npn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-338
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/57gs0npn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 57gs0npn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.0246 | Val Loss: 2.6345
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.3595 | Val Loss: 2.0408
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.8105 | Val Loss: 1.6839
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6731 | Val Loss: 1.6831
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.7146 | Val Loss: 1.7469
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.7758 | Val Loss: 1.8028
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.8226 | Val Loss: 1.8416
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.8596 | Val Loss: 1.8770
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.8923 | Val Loss: 1.9070
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.872 MB of 32.872 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.92079
wandb:   val_loss 1.93452
wandb: 
wandb: üöÄ View run kind-sweep-338 at: https://wandb.ai/7shoe/domShift-extensive/runs/57gs0npn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175214-57gs0npn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.9208 | Val Loss: 1.9345
2025-03-26 17:52:49,635 - wandb.wandb_agent - INFO - Cleaning up finished run: 57gs0npn
2025-03-26 17:52:50,763 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:52:50,763 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:52:50,766 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:52:55,778 - wandb.wandb_agent - INFO - Running runs: ['4w10yn3k']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175255-4w10yn3k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-343
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4w10yn3k
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4w10yn3k
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4282 | Val Loss: 1.3939
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3517 | Val Loss: 1.3295
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3259 | Val Loss: 1.3186
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3125 | Val Loss: 1.3070
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3046 | Val Loss: 1.3017
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2998 | Val Loss: 1.2979
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3063 | Val Loss: 1.3568
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4147 | Val Loss: 1.4552
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4684 | Val Loss: 1.5115
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.015 MB uploadedwandb: | 32.995 MB of 33.015 MB uploadedwandb: / 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñÖ
wandb:   val_loss ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40341
wandb:   val_loss 1.32233
wandb: 
wandb: üöÄ View run effortless-sweep-343 at: https://wandb.ai/7shoe/domShift-extensive/runs/4w10yn3k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175255-4w10yn3k/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4034 | Val Loss: 1.3223
2025-03-26 17:53:36,315 - wandb.wandb_agent - INFO - Cleaning up finished run: 4w10yn3k
2025-03-26 17:53:36,925 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:53:36,925 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:53:36,928 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:53:41,940 - wandb.wandb_agent - INFO - Running runs: ['mypfdnld']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175343-mypfdnld
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-349
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mypfdnld
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: mypfdnld
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6533 | Val Loss: 1.5846
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5864 | Val Loss: 1.5410
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5044 | Val Loss: 1.5343
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4628 | Val Loss: 1.3526
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3264 | Val Loss: 1.2984
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2915 | Val Loss: 1.2841
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2820 | Val Loss: 1.2800
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2815 | Val Loss: 1.2816
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2827 | Val Loss: 1.2834
wandb: - 32.875 MB of 32.875 MB uploadedwandb: \ 32.875 MB of 32.875 MB uploadedwandb: | 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.28613
wandb:   val_loss 1.28813
wandb: 
wandb: üöÄ View run generous-sweep-349 at: https://wandb.ai/7shoe/domShift-extensive/runs/mypfdnld
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175343-mypfdnld/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2861 | Val Loss: 1.2881
2025-03-26 17:54:17,399 - wandb.wandb_agent - INFO - Cleaning up finished run: mypfdnld
2025-03-26 17:54:17,910 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:54:17,911 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:54:17,914 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:54:22,926 - wandb.wandb_agent - INFO - Running runs: ['dd9naw0b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175424-dd9naw0b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-353
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dd9naw0b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dd9naw0b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3623 | Val Loss: 1.2438
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0491 | Val Loss: 0.9492
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9903 | Val Loss: 0.9881
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9464 | Val Loss: 0.9166
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9249 | Val Loss: 0.9346
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9401 | Val Loss: 0.9436
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9514 | Val Loss: 0.9584
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9659 | Val Loss: 0.9729
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9794 | Val Loss: 0.9849
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99042
wandb:   val_loss 0.9956
wandb: 
wandb: üöÄ View run lyric-sweep-353 at: https://wandb.ai/7shoe/domShift-extensive/runs/dd9naw0b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175424-dd9naw0b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9904 | Val Loss: 0.9956
2025-03-26 17:55:03,486 - wandb.wandb_agent - INFO - Cleaning up finished run: dd9naw0b
2025-03-26 17:55:04,552 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:55:04,552 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:55:04,555 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:55:09,565 - wandb.wandb_agent - INFO - Running runs: ['lf6arbgc']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175509-lf6arbgc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-359
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lf6arbgc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lf6arbgc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5537 | Val Loss: 1.4257
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4565 | Val Loss: 1.5182
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5145 | Val Loss: 1.5093
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4942 | Val Loss: 1.4811
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4383 | Val Loss: 1.4114
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4017 | Val Loss: 1.4020
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3982 | Val Loss: 1.3990
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3952 | Val Loss: 1.3957
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3920 | Val Loss: 1.3929
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.38956
wandb:   val_loss 1.39055
wandb: 
wandb: üöÄ View run mild-sweep-359 at: https://wandb.ai/7shoe/domShift-extensive/runs/lf6arbgc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175509-lf6arbgc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3896 | Val Loss: 1.3906
2025-03-26 17:55:45,021 - wandb.wandb_agent - INFO - Cleaning up finished run: lf6arbgc
2025-03-26 17:55:45,645 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:55:45,645 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:55:45,647 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:55:50,660 - wandb.wandb_agent - INFO - Running runs: ['n36xjsrp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175552-n36xjsrp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-362
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/n36xjsrp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: n36xjsrp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8730 | Val Loss: 1.6839
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5828 | Val Loss: 1.3959
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3168 | Val Loss: 1.2710
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2295 | Val Loss: 1.1840
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1699 | Val Loss: 1.1511
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1379 | Val Loss: 1.1368
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1459 | Val Loss: 1.1483
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1434 | Val Loss: 1.1400
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1437 | Val Loss: 1.1504
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.075 MB of 137.075 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15145
wandb:   val_loss 1.14704
wandb: 
wandb: üöÄ View run cosmic-sweep-362 at: https://wandb.ai/7shoe/domShift-extensive/runs/n36xjsrp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175552-n36xjsrp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1514 | Val Loss: 1.1470
2025-03-26 17:56:21,083 - wandb.wandb_agent - INFO - Cleaning up finished run: n36xjsrp
2025-03-26 17:56:21,553 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:56:21,553 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:56:21,556 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:56:26,569 - wandb.wandb_agent - INFO - Running runs: ['ic43jg3i']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175626-ic43jg3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-366
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ic43jg3i
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ic43jg3i
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9530 | Val Loss: 2.7138
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7128 | Val Loss: 2.6963
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.7026 | Val Loss: 2.6982
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.6951 | Val Loss: 2.6758
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.6677 | Val Loss: 2.6565
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.6539 | Val Loss: 2.6449
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6468 | Val Loss: 2.6456
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6487 | Val Loss: 2.6490
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6537 | Val Loss: 2.6573
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.330 MB uploadedwandb: | 137.311 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.66391
wandb:   val_loss 2.66939
wandb: 
wandb: üöÄ View run ancient-sweep-366 at: https://wandb.ai/7shoe/domShift-extensive/runs/ic43jg3i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175626-ic43jg3i/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.6639 | Val Loss: 2.6694
2025-03-26 17:57:12,207 - wandb.wandb_agent - INFO - Cleaning up finished run: ic43jg3i
2025-03-26 17:57:12,738 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:57:12,738 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:57:12,741 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:57:17,753 - wandb.wandb_agent - INFO - Running runs: ['499xgy7z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175718-499xgy7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-372
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/499xgy7z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 499xgy7z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5770 | Val Loss: 1.4286
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4087 | Val Loss: 1.4157
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4452 | Val Loss: 1.4902
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5303 | Val Loss: 1.5805
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6143 | Val Loss: 1.6261
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6085 | Val Loss: 1.5925
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5951 | Val Loss: 1.5998
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6068 | Val Loss: 1.6124
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6166 | Val Loss: 1.6228
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.62738
wandb:   val_loss 1.63014
wandb: 
wandb: üöÄ View run zany-sweep-372 at: https://wandb.ai/7shoe/domShift-extensive/runs/499xgy7z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175718-499xgy7z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.6274 | Val Loss: 1.6301
2025-03-26 17:58:13,553 - wandb.wandb_agent - INFO - Cleaning up finished run: 499xgy7z
2025-03-26 17:58:14,116 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:58:14,117 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:58:14,119 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:58:19,132 - wandb.wandb_agent - INFO - Running runs: ['c8s0gbk7']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175819-c8s0gbk7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-379
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/c8s0gbk7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: c8s0gbk7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4657 | Val Loss: 1.4164
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0902 | Val Loss: 0.9564
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9418 | Val Loss: 0.9357
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9424 | Val Loss: 0.9436
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8794 | Val Loss: 0.8491
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8562 | Val Loss: 0.8568
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8586 | Val Loss: 0.8611
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8627 | Val Loss: 0.8642
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8669 | Val Loss: 0.8688
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.86968
wandb:   val_loss 0.87111
wandb: 
wandb: üöÄ View run deft-sweep-379 at: https://wandb.ai/7shoe/domShift-extensive/runs/c8s0gbk7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175819-c8s0gbk7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8697 | Val Loss: 0.8711
2025-03-26 17:59:25,095 - wandb.wandb_agent - INFO - Cleaning up finished run: c8s0gbk7
2025-03-26 17:59:25,571 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:59:25,571 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:59:25,574 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:59:30,586 - wandb.wandb_agent - INFO - Running runs: ['d0w4u3km']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175930-d0w4u3km
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-385
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/d0w4u3km
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: d0w4u3km
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2744 | Val Loss: 2.6121
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5683 | Val Loss: 2.4463
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4291 | Val Loss: 2.3141
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3402 | Val Loss: 2.2595
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2881 | Val Loss: 2.2217
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2557 | Val Loss: 2.1803
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2207 | Val Loss: 2.1495
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1904 | Val Loss: 2.1290
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1736 | Val Loss: 2.1083
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.773 MB uploadedwandb: | 43.729 MB of 43.773 MB uploadedwandb: / 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.15953
wandb:   val_loss 2.10658
wandb: 
wandb: üöÄ View run vital-sweep-385 at: https://wandb.ai/7shoe/domShift-extensive/runs/d0w4u3km
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175930-d0w4u3km/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1595 | Val Loss: 2.1066
2025-03-26 18:00:16,228 - wandb.wandb_agent - INFO - Cleaning up finished run: d0w4u3km
2025-03-26 18:00:16,801 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:16,801 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:00:16,804 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:00:21,817 - wandb.wandb_agent - INFO - Running runs: ['pb3ibdbr']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180021-pb3ibdbr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-393
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pb3ibdbr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pb3ibdbr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2971 | Val Loss: 1.2109
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2584 | Val Loss: 1.3606
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3210 | Val Loss: 1.1770
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1155 | Val Loss: 1.0307
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0083 | Val Loss: 0.9933
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9876 | Val Loss: 0.9900
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9868 | Val Loss: 0.9883
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9873 | Val Loss: 0.9888
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9883 | Val Loss: 0.9901
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñá‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.98953
wandb:   val_loss 0.99117
wandb: 
wandb: üöÄ View run good-sweep-393 at: https://wandb.ai/7shoe/domShift-extensive/runs/pb3ibdbr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180021-pb3ibdbr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9895 | Val Loss: 0.9912
2025-03-26 18:01:02,423 - wandb.wandb_agent - INFO - Cleaning up finished run: pb3ibdbr
2025-03-26 18:01:03,069 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:01:03,069 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:01:03,072 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 18:01:08,084 - wandb.wandb_agent - INFO - Running runs: ['yf8l79ls']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180109-yf8l79ls
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-398
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yf8l79ls
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yf8l79ls
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.4714 | Val Loss: 3.2092
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.0611 | Val Loss: 2.8591
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.7178 | Val Loss: 2.6058
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.5365 | Val Loss: 2.4761
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.4428 | Val Loss: 2.4123
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.3855 | Val Loss: 2.2992
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.1405 | Val Loss: 2.1098
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.1569 | Val Loss: 2.2283
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3028 | Val Loss: 2.3798
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 33.060 MB of 33.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.45809
wandb:   val_loss 2.53991
wandb: 
wandb: üöÄ View run silvery-sweep-398 at: https://wandb.ai/7shoe/domShift-extensive/runs/yf8l79ls
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180109-yf8l79ls/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.4581 | Val Loss: 2.5399
2025-03-26 18:01:58,873 - wandb.wandb_agent - INFO - Cleaning up finished run: yf8l79ls
2025-03-26 18:01:59,666 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:01:59,666 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:01:59,669 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:02:04,681 - wandb.wandb_agent - INFO - Running runs: ['orgtqmzz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180205-orgtqmzz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-403
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/orgtqmzz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: orgtqmzz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.5372 | Val Loss: 2.6614
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6708 | Val Loss: 2.6186
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4730 | Val Loss: 2.3655
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3050 | Val Loss: 2.2453
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1560 | Val Loss: 2.0833
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0738 | Val Loss: 2.0681
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0651 | Val Loss: 2.0597
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0544 | Val Loss: 2.0477
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0187 | Val Loss: 1.9619
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.031 MB of 137.031 MB uploadedwandb: | 137.051 MB of 137.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.87785
wandb:   val_loss 1.84873
wandb: 
wandb: üöÄ View run grateful-sweep-403 at: https://wandb.ai/7shoe/domShift-extensive/runs/orgtqmzz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180205-orgtqmzz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.8778 | Val Loss: 1.8487
2025-03-26 18:02:55,373 - wandb.wandb_agent - INFO - Cleaning up finished run: orgtqmzz
2025-03-26 18:02:55,837 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:02:55,837 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:02:55,840 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:03:00,852 - wandb.wandb_agent - INFO - Running runs: ['ngjb8if9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180300-ngjb8if9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-409
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ngjb8if9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ngjb8if9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7555 | Val Loss: 1.5808
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5501 | Val Loss: 1.5196
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5145 | Val Loss: 1.5123
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5405 | Val Loss: 1.5715
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6085 | Val Loss: 1.6396
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6509 | Val Loss: 1.6603
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6525 | Val Loss: 1.6352
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6274 | Val Loss: 1.6192
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6093 | Val Loss: 1.6069
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.58903
wandb:   val_loss 1.56837
wandb: 
wandb: üöÄ View run resilient-sweep-409 at: https://wandb.ai/7shoe/domShift-extensive/runs/ngjb8if9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180300-ngjb8if9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5890 | Val Loss: 1.5684
2025-03-26 18:03:36,342 - wandb.wandb_agent - INFO - Cleaning up finished run: ngjb8if9
2025-03-26 18:03:37,091 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:03:37,091 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:03:37,094 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:03:42,106 - wandb.wandb_agent - INFO - Running runs: ['sf15l46z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180343-sf15l46z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-412
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sf15l46z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sf15l46z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0613 | Val Loss: 1.0104
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9943 | Val Loss: 0.9532
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9146 | Val Loss: 0.8604
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8193 | Val Loss: 0.8014
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8157 | Val Loss: 0.8092
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8023 | Val Loss: 0.8096
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.7959 | Val Loss: 0.7894
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.7877 | Val Loss: 0.7840
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.7843 | Val Loss: 0.7845
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.78379
wandb:   val_loss 0.78509
wandb: 
wandb: üöÄ View run solar-sweep-412 at: https://wandb.ai/7shoe/domShift-extensive/runs/sf15l46z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180343-sf15l46z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.7838 | Val Loss: 0.7851
2025-03-26 18:04:37,916 - wandb.wandb_agent - INFO - Cleaning up finished run: sf15l46z
2025-03-26 18:04:38,502 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:04:38,502 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:04:38,505 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:04:43,517 - wandb.wandb_agent - INFO - Running runs: ['hgh3whhl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180444-hgh3whhl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-419
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hgh3whhl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hgh3whhl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5490 | Val Loss: 1.3709
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3372 | Val Loss: 1.3256
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2391 | Val Loss: 1.1841
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1811 | Val Loss: 1.1734
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1722 | Val Loss: 1.1688
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1629 | Val Loss: 1.1604
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1586 | Val Loss: 1.1564
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1555 | Val Loss: 1.1544
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1535 | Val Loss: 1.1527
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15203
wandb:   val_loss 1.15134
wandb: 
wandb: üöÄ View run deep-sweep-419 at: https://wandb.ai/7shoe/domShift-extensive/runs/hgh3whhl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180444-hgh3whhl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1520 | Val Loss: 1.1513
2025-03-26 18:05:59,525 - wandb.wandb_agent - INFO - Cleaning up finished run: hgh3whhl
2025-03-26 18:06:00,194 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:00,194 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:06:00,200 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:06:05,212 - wandb.wandb_agent - INFO - Running runs: ['cm6ys0qk']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180605-cm6ys0qk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-430
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cm6ys0qk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: cm6ys0qk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5732 | Val Loss: 1.3505
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2291 | Val Loss: 1.0322
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9582 | Val Loss: 0.9272
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9198 | Val Loss: 0.9101
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9077 | Val Loss: 0.9046
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9023 | Val Loss: 0.9004
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9115 | Val Loss: 0.9172
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9205 | Val Loss: 0.9217
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9237 | Val Loss: 0.9248
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92634
wandb:   val_loss 0.9262
wandb: 
wandb: üöÄ View run cosmic-sweep-430 at: https://wandb.ai/7shoe/domShift-extensive/runs/cm6ys0qk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180605-cm6ys0qk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9263 | Val Loss: 0.9262
2025-03-26 18:06:55,923 - wandb.wandb_agent - INFO - Cleaning up finished run: cm6ys0qk
2025-03-26 18:06:56,673 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:56,673 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:06:56,676 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:07:01,687 - wandb.wandb_agent - INFO - Running runs: ['l1z13y4r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180703-l1z13y4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-437
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l1z13y4r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: l1z13y4r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6643 | Val Loss: 1.5331
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4460 | Val Loss: 1.2274
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2316 | Val Loss: 1.2592
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2692 | Val Loss: 1.2717
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3125 | Val Loss: 1.2723
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1871 | Val Loss: 1.1771
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3306 | Val Loss: 1.2566
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2312 | Val Loss: 1.2264
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2249 | Val Loss: 1.2228
wandb: - 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21646
wandb:   val_loss 1.20843
wandb: 
wandb: üöÄ View run deep-sweep-437 at: https://wandb.ai/7shoe/domShift-extensive/runs/l1z13y4r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180703-l1z13y4r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2165 | Val Loss: 1.2084
2025-03-26 18:08:07,688 - wandb.wandb_agent - INFO - Cleaning up finished run: l1z13y4r
2025-03-26 18:08:08,579 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:08:08,579 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:08:08,582 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:08:13,594 - wandb.wandb_agent - INFO - Running runs: ['tf5whigm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180814-tf5whigm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-444
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tf5whigm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tf5whigm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.8676 | Val Loss: 3.6086
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.2531 | Val Loss: 2.8203
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5456 | Val Loss: 2.3327
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2103 | Val Loss: 2.0802
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0042 | Val Loss: 1.9225
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8806 | Val Loss: 1.8387
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8256 | Val Loss: 1.8101
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.8047 | Val Loss: 1.7961
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.7916 | Val Loss: 1.7837
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.7786
wandb:   val_loss 1.77072
wandb: 
wandb: üöÄ View run curious-sweep-444 at: https://wandb.ai/7shoe/domShift-extensive/runs/tf5whigm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180814-tf5whigm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7786 | Val Loss: 1.7707
2025-03-26 18:08:59,206 - wandb.wandb_agent - INFO - Cleaning up finished run: tf5whigm
2025-03-26 18:09:00,790 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:09:00,790 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:09:00,793 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:09:05,804 - wandb.wandb_agent - INFO - Running runs: ['zyuyknim']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180905-zyuyknim
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-450
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zyuyknim
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: zyuyknim
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1441 | Val Loss: 2.8486
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5411 | Val Loss: 2.2372
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1784 | Val Loss: 2.1819
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1896 | Val Loss: 2.2136
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2567 | Val Loss: 2.3023
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3489 | Val Loss: 2.3959
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4324 | Val Loss: 2.4655
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4877 | Val Loss: 2.5078
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5214 | Val Loss: 2.5339
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.54311
wandb:   val_loss 2.55232
wandb: 
wandb: üöÄ View run solar-sweep-450 at: https://wandb.ai/7shoe/domShift-extensive/runs/zyuyknim
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180905-zyuyknim/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.5431 | Val Loss: 2.5523
2025-03-26 18:09:56,496 - wandb.wandb_agent - INFO - Cleaning up finished run: zyuyknim
2025-03-26 18:09:57,066 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:09:57,067 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:09:57,070 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:10:02,084 - wandb.wandb_agent - INFO - Running runs: ['jfvry566']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181003-jfvry566
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-456
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jfvry566
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jfvry566
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6940 | Val Loss: 1.3280
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0237 | Val Loss: 0.9822
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9426 | Val Loss: 0.8609
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8701 | Val Loss: 0.8596
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8631 | Val Loss: 0.8768
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8794 | Val Loss: 0.8771
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8781 | Val Loss: 0.8784
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8795 | Val Loss: 0.8804
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8819 | Val Loss: 0.8828
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.88406
wandb:   val_loss 0.88481
wandb: 
wandb: üöÄ View run restful-sweep-456 at: https://wandb.ai/7shoe/domShift-extensive/runs/jfvry566
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181003-jfvry566/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8841 | Val Loss: 0.8848
2025-03-26 18:11:18,198 - wandb.wandb_agent - INFO - Cleaning up finished run: jfvry566
2025-03-26 18:11:18,959 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:11:18,959 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:11:18,962 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:11:23,973 - wandb.wandb_agent - INFO - Running runs: ['1pgc2n9o']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181124-1pgc2n9o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-465
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1pgc2n9o
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1pgc2n9o
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4117 | Val Loss: 1.2646
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2757 | Val Loss: 1.2665
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2877 | Val Loss: 1.3044
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3208 | Val Loss: 1.3346
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3407 | Val Loss: 1.3466
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3491 | Val Loss: 1.3273
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3040 | Val Loss: 1.3043
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3048 | Val Loss: 1.2999
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3029 | Val Loss: 1.3029
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.949 MB uploadedwandb: | 32.856 MB of 32.949 MB uploadedwandb: / 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.28282
wandb:   val_loss 1.19918
wandb: 
wandb: üöÄ View run magic-sweep-465 at: https://wandb.ai/7shoe/domShift-extensive/runs/1pgc2n9o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181124-1pgc2n9o/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2828 | Val Loss: 1.1992
2025-03-26 18:12:09,600 - wandb.wandb_agent - INFO - Cleaning up finished run: 1pgc2n9o
2025-03-26 18:12:10,200 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:12:10,200 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:12:10,203 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:12:15,216 - wandb.wandb_agent - INFO - Running runs: ['1sd4uosd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181216-1sd4uosd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-472
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1sd4uosd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1sd4uosd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5505 | Val Loss: 1.4943
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3906 | Val Loss: 1.3236
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2796 | Val Loss: 1.2496
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2683 | Val Loss: 1.3554
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4248 | Val Loss: 1.4156
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3135 | Val Loss: 1.2670
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2668 | Val Loss: 1.2423
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1988 | Val Loss: 1.1399
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1078 | Val Loss: 1.1053
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12436
wandb:   val_loss 1.14044
wandb: 
wandb: üöÄ View run lyric-sweep-472 at: https://wandb.ai/7shoe/domShift-extensive/runs/1sd4uosd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181216-1sd4uosd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1244 | Val Loss: 1.1404
2025-03-26 18:13:00,830 - wandb.wandb_agent - INFO - Cleaning up finished run: 1sd4uosd
2025-03-26 18:13:01,377 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:13:01,377 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:13:01,380 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:13:06,392 - wandb.wandb_agent - INFO - Running runs: ['3awfihj7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181306-3awfihj7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-475
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3awfihj7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3awfihj7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4113 | Val Loss: 1.2038
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2099 | Val Loss: 1.1967
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1922 | Val Loss: 1.1842
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1805 | Val Loss: 1.1907
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1602 | Val Loss: 1.1360
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1395 | Val Loss: 1.1363
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1350 | Val Loss: 1.1342
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1333 | Val Loss: 1.1346
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1326 | Val Loss: 1.1330
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13224
wandb:   val_loss 1.13349
wandb: 
wandb: üöÄ View run hardy-sweep-475 at: https://wandb.ai/7shoe/domShift-extensive/runs/3awfihj7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181306-3awfihj7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1322 | Val Loss: 1.1335
2025-03-26 18:14:12,365 - wandb.wandb_agent - INFO - Cleaning up finished run: 3awfihj7
2025-03-26 18:14:12,847 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:14:12,847 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:14:12,853 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:14:17,866 - wandb.wandb_agent - INFO - Running runs: ['pobybd11']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181417-pobybd11
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-481
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pobybd11
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pobybd11
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3423 | Val Loss: 1.1802
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0984 | Val Loss: 1.0318
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9831 | Val Loss: 0.9777
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9319 | Val Loss: 0.8861
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8703 | Val Loss: 0.8804
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8743 | Val Loss: 0.8743
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8701 | Val Loss: 0.8680
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8662 | Val Loss: 0.8693
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8686 | Val Loss: 0.8727
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.8712
wandb:   val_loss 0.87524
wandb: 
wandb: üöÄ View run dauntless-sweep-481 at: https://wandb.ai/7shoe/domShift-extensive/runs/pobybd11
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181417-pobybd11/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8712 | Val Loss: 0.8752
2025-03-26 18:14:58,405 - wandb.wandb_agent - INFO - Cleaning up finished run: pobybd11
2025-03-26 18:14:59,256 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:14:59,257 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:14:59,259 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:15:04,272 - wandb.wandb_agent - INFO - Running runs: ['6hahv5sf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181505-6hahv5sf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-486
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6hahv5sf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6hahv5sf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4340 | Val Loss: 1.0957
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1282 | Val Loss: 1.1840
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1996 | Val Loss: 1.2119
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2215 | Val Loss: 1.2267
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2289 | Val Loss: 1.2313
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2363 | Val Loss: 1.2393
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2370 | Val Loss: 1.2375
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2404 | Val Loss: 1.2380
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2369 | Val Loss: 1.2367
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20444
wandb:   val_loss 1.16536
wandb: 
wandb: üöÄ View run smooth-sweep-486 at: https://wandb.ai/7shoe/domShift-extensive/runs/6hahv5sf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181505-6hahv5sf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2044 | Val Loss: 1.1654
2025-03-26 18:16:20,361 - wandb.wandb_agent - INFO - Cleaning up finished run: 6hahv5sf
2025-03-26 18:16:20,821 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:16:20,822 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:16:20,825 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:16:25,839 - wandb.wandb_agent - INFO - Running runs: ['wg62bhvm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181627-wg62bhvm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-494
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wg62bhvm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: wg62bhvm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.2058 | Val Loss: 2.1084
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2455 | Val Loss: 2.3546
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4096 | Val Loss: 2.4433
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4577 | Val Loss: 2.4978
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4964 | Val Loss: 2.4923
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5054 | Val Loss: 2.5326
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5627 | Val Loss: 2.5838
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5795 | Val Loss: 2.5003
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4747 | Val Loss: 2.4778
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.47058
wandb:   val_loss 2.44364
wandb: 
wandb: üöÄ View run lyric-sweep-494 at: https://wandb.ai/7shoe/domShift-extensive/runs/wg62bhvm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181627-wg62bhvm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4706 | Val Loss: 2.4436
2025-03-26 18:17:41,931 - wandb.wandb_agent - INFO - Cleaning up finished run: wg62bhvm
2025-03-26 18:17:42,493 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:17:42,493 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:17:42,496 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:17:47,509 - wandb.wandb_agent - INFO - Running runs: ['t2twc4z0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181749-t2twc4z0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-500
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/t2twc4z0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: t2twc4z0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8390 | Val Loss: 1.7648
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7836 | Val Loss: 1.8323
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.8723 | Val Loss: 1.8943
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8456 | Val Loss: 1.7480
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6700 | Val Loss: 1.6532
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6615 | Val Loss: 1.6712
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6772 | Val Loss: 1.6806
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6842 | Val Loss: 1.6863
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6648 | Val Loss: 1.6233
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÜ‚ñà‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.57192
wandb:   val_loss 1.48348
wandb: 
wandb: üöÄ View run swift-sweep-500 at: https://wandb.ai/7shoe/domShift-extensive/runs/t2twc4z0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181749-t2twc4z0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5719 | Val Loss: 1.4835
2025-03-26 18:18:33,175 - wandb.wandb_agent - INFO - Cleaning up finished run: t2twc4z0
2025-03-26 18:18:33,737 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:18:33,737 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:18:33,739 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimCLR --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:18:38,752 - wandb.wandb_agent - INFO - Running runs: ['v6q35fml']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181840-v6q35fml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-504
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/v6q35fml
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: v6q35fml
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 5.5826 | Val Loss: 5.1160
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 5.2185 | Val Loss: 4.8527
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 4.9467 | Val Loss: 4.5355
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 4.6256 | Val Loss: 4.6092
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 4.5405 | Val Loss: 4.1151
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 4.3185 | Val Loss: 3.9749
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 4.1865 | Val Loss: 3.8756
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 4.1261 | Val Loss: 3.8377
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 4.0339 | Val Loss: 3.6706
wandb: - 32.853 MB of 32.872 MB uploadedwandb: \ 32.853 MB of 32.872 MB uploadedwandb: | 32.872 MB of 32.872 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.91363
wandb:   val_loss 3.67503
wandb: 
wandb: üöÄ View run serene-sweep-504 at: https://wandb.ai/7shoe/domShift-extensive/runs/v6q35fml
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181840-v6q35fml/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.9136 | Val Loss: 3.6750
2025-03-26 18:19:09,166 - wandb.wandb_agent - INFO - Cleaning up finished run: v6q35fml
2025-03-26 18:19:10,043 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:19:10,043 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:19:10,046 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:19:15,058 - wandb.wandb_agent - INFO - Running runs: ['4xcafddz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181916-4xcafddz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-509
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4xcafddz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4xcafddz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1517 | Val Loss: 1.0128
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9899 | Val Loss: 0.9785
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9810 | Val Loss: 0.9901
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9955 | Val Loss: 0.9998
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0005 | Val Loss: 1.0020
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0005 | Val Loss: 1.0003
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0000 | Val Loss: 1.0010
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0001 | Val Loss: 1.0004
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9999 | Val Loss: 1.0006
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.333 MB uploadedwandb: | 137.314 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99981
wandb:   val_loss 1.00042
wandb: 
wandb: üöÄ View run hardy-sweep-509 at: https://wandb.ai/7shoe/domShift-extensive/runs/4xcafddz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181916-4xcafddz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9998 | Val Loss: 1.0004
2025-03-26 18:20:10,841 - wandb.wandb_agent - INFO - Cleaning up finished run: 4xcafddz
2025-03-26 18:20:11,363 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:20:11,363 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:20:11,366 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:20:16,379 - wandb.wandb_agent - INFO - Running runs: ['n3cx7vuu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182016-n3cx7vuu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-515
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/n3cx7vuu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: n3cx7vuu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5998 | Val Loss: 1.4838
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4055 | Val Loss: 1.3084
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2406 | Val Loss: 1.2066
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2349 | Val Loss: 1.2512
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2571 | Val Loss: 1.2696
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2804 | Val Loss: 1.2902
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3047 | Val Loss: 1.3169
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3284 | Val Loss: 1.3422
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3471 | Val Loss: 1.3530
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.35822
wandb:   val_loss 1.36524
wandb: 
wandb: üöÄ View run treasured-sweep-515 at: https://wandb.ai/7shoe/domShift-extensive/runs/n3cx7vuu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182016-n3cx7vuu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3582 | Val Loss: 1.3652
2025-03-26 18:21:27,436 - wandb.wandb_agent - INFO - Cleaning up finished run: n3cx7vuu
2025-03-26 18:21:27,856 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:21:27,856 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:21:27,859 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:21:32,871 - wandb.wandb_agent - INFO - Running runs: ['q68gc1ir']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182134-q68gc1ir
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-522
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/q68gc1ir
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: q68gc1ir
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5527 | Val Loss: 2.6945
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.8018 | Val Loss: 2.8763
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9005 | Val Loss: 2.9068
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.8991 | Val Loss: 2.8848
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8822 | Val Loss: 2.8809
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8790 | Val Loss: 2.8763
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.8716 | Val Loss: 2.8666
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.8649 | Val Loss: 2.8639
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.8633 | Val Loss: 2.8626
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.86186
wandb:   val_loss 2.86103
wandb: 
wandb: üöÄ View run swept-sweep-522 at: https://wandb.ai/7shoe/domShift-extensive/runs/q68gc1ir
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182134-q68gc1ir/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.8619 | Val Loss: 2.8610
2025-03-26 18:22:18,487 - wandb.wandb_agent - INFO - Cleaning up finished run: q68gc1ir
2025-03-26 18:22:19,036 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:22:19,037 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:22:19,040 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:22:24,054 - wandb.wandb_agent - INFO - Running runs: ['kwtvqcyk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182225-kwtvqcyk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-526
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kwtvqcyk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: kwtvqcyk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6646 | Val Loss: 1.4532
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4358 | Val Loss: 1.4230
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3854 | Val Loss: 1.3297
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3059 | Val Loss: 1.2915
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2934 | Val Loss: 1.2954
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2976 | Val Loss: 1.2987
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3011 | Val Loss: 1.3020
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3021 | Val Loss: 1.3012
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3014 | Val Loss: 1.3010
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30242
wandb:   val_loss 1.30331
wandb: 
wandb: üöÄ View run eternal-sweep-526 at: https://wandb.ai/7shoe/domShift-extensive/runs/kwtvqcyk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182225-kwtvqcyk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3024 | Val Loss: 1.3033
2025-03-26 18:23:19,861 - wandb.wandb_agent - INFO - Cleaning up finished run: kwtvqcyk
2025-03-26 18:23:20,460 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:23:20,460 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:23:20,463 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.2
2025-03-26 18:23:25,475 - wandb.wandb_agent - INFO - Running runs: ['aweiqw6s']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182326-aweiqw6s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-531
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/aweiqw6s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: aweiqw6s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 2.0040 | Val Loss: 1.8502
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.8007 | Val Loss: 1.7239
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.6995 | Val Loss: 1.6560
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.6371 | Val Loss: 1.5941
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.5799 | Val Loss: 1.5418
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.5283 | Val Loss: 1.4928
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.4808 | Val Loss: 1.4498
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.4406 | Val Loss: 1.4169
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.4115 | Val Loss: 1.3954
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.39262
wandb:   val_loss 1.38152
wandb: 
wandb: üöÄ View run icy-sweep-531 at: https://wandb.ai/7shoe/domShift-extensive/runs/aweiqw6s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182326-aweiqw6s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.3926 | Val Loss: 1.3815
2025-03-26 18:23:50,808 - wandb.wandb_agent - INFO - Cleaning up finished run: aweiqw6s
2025-03-26 18:23:51,277 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:23:51,278 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:23:51,280 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:23:56,293 - wandb.wandb_agent - INFO - Running runs: ['ka5itjjn']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182356-ka5itjjn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-534
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ka5itjjn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ka5itjjn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6503 | Val Loss: 1.4597
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2729 | Val Loss: 1.1956
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1691 | Val Loss: 1.1149
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1299 | Val Loss: 1.1616
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1241 | Val Loss: 1.0774
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0647 | Val Loss: 1.0789
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0994 | Val Loss: 1.0956
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1404 | Val Loss: 1.2344
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2749 | Val Loss: 1.3748
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4492
wandb:   val_loss 1.40903
wandb: 
wandb: üöÄ View run wild-sweep-534 at: https://wandb.ai/7shoe/domShift-extensive/runs/ka5itjjn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182356-ka5itjjn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4492 | Val Loss: 1.4090
2025-03-26 18:24:36,839 - wandb.wandb_agent - INFO - Cleaning up finished run: ka5itjjn
2025-03-26 18:24:37,835 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:24:37,836 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:24:37,838 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:24:42,851 - wandb.wandb_agent - INFO - Running runs: ['tqvsg9dm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182442-tqvsg9dm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-540
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tqvsg9dm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: tqvsg9dm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9104 | Val Loss: 1.7583
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7454 | Val Loss: 1.7537
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7763 | Val Loss: 1.8733
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.7453 | Val Loss: 1.1429
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2379 | Val Loss: 1.2970
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2606 | Val Loss: 1.2405
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2367 | Val Loss: 1.2353
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2262 | Val Loss: 1.2135
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1824 | Val Loss: 1.1511
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñá‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14376
wandb:   val_loss 1.14313
wandb: 
wandb: üöÄ View run restful-sweep-540 at: https://wandb.ai/7shoe/domShift-extensive/runs/tqvsg9dm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182442-tqvsg9dm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1438 | Val Loss: 1.1431
2025-03-26 18:25:43,691 - wandb.wandb_agent - INFO - Cleaning up finished run: tqvsg9dm
2025-03-26 18:25:44,474 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:25:44,475 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:25:44,478 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:25:49,491 - wandb.wandb_agent - INFO - Running runs: ['kkukr5ov']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182550-kkukr5ov
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-546
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kkukr5ov
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: kkukr5ov
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4849 | Val Loss: 1.4310
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2805 | Val Loss: 1.2092
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1681 | Val Loss: 1.1550
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2177 | Val Loss: 1.2231
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3165 | Val Loss: 1.3289
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3216 | Val Loss: 1.3619
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2943 | Val Loss: 1.0681
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1719 | Val Loss: 1.1870
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1268 | Val Loss: 1.0923
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09177
wandb:   val_loss 1.09576
wandb: 
wandb: üöÄ View run true-sweep-546 at: https://wandb.ai/7shoe/domShift-extensive/runs/kkukr5ov
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182550-kkukr5ov/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0918 | Val Loss: 1.0958
2025-03-26 18:27:05,584 - wandb.wandb_agent - INFO - Cleaning up finished run: kkukr5ov
2025-03-26 18:27:06,428 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:27:06,428 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:27:06,431 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 18:27:11,440 - wandb.wandb_agent - INFO - Running runs: ['bg2kn4b9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182712-bg2kn4b9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-555
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bg2kn4b9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bg2kn4b9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3271 | Val Loss: 1.2025
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1425 | Val Loss: 1.1177
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0801 | Val Loss: 1.0344
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0280 | Val Loss: 1.0213
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0192 | Val Loss: 1.0170
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0153 | Val Loss: 1.0135
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0117 | Val Loss: 1.0100
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0086 | Val Loss: 1.0072
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0059 | Val Loss: 1.0045
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.00328
wandb:   val_loss 1.00255
wandb: 
wandb: üöÄ View run lunar-sweep-555 at: https://wandb.ai/7shoe/domShift-extensive/runs/bg2kn4b9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182712-bg2kn4b9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0033 | Val Loss: 1.0026
2025-03-26 18:28:27,494 - wandb.wandb_agent - INFO - Cleaning up finished run: bg2kn4b9
2025-03-26 18:28:28,022 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:28:28,022 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:28:28,025 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 18:28:33,037 - wandb.wandb_agent - INFO - Running runs: ['329wodlo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182833-329wodlo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-562
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/329wodlo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 329wodlo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2394 | Val Loss: 3.1103
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.0076 | Val Loss: 2.8980
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8314 | Val Loss: 2.7762
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7599 | Val Loss: 2.7528
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.7574 | Val Loss: 2.7556
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.6907 | Val Loss: 2.6778
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.6835 | Val Loss: 2.6874
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.6941 | Val Loss: 2.7007
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.7071 | Val Loss: 2.7132
wandb: - 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.71908
wandb:   val_loss 2.7246
wandb: 
wandb: üöÄ View run volcanic-sweep-562 at: https://wandb.ai/7shoe/domShift-extensive/runs/329wodlo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182833-329wodlo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.7191 | Val Loss: 2.7246
2025-03-26 18:29:18,658 - wandb.wandb_agent - INFO - Cleaning up finished run: 329wodlo
2025-03-26 18:29:19,185 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:29:19,186 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:29:19,189 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:29:24,200 - wandb.wandb_agent - INFO - Running runs: ['iqmzs1td']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182925-iqmzs1td
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-565
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/iqmzs1td
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: iqmzs1td
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2860 | Val Loss: 1.1958
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1553 | Val Loss: 1.1410
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1654 | Val Loss: 1.1561
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1648 | Val Loss: 1.1693
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1690 | Val Loss: 1.1711
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1704 | Val Loss: 1.1725
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1717 | Val Loss: 1.1727
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1718 | Val Loss: 1.1727
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1721 | Val Loss: 1.1728
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.471 MB uploadedwandb: | 137.311 MB of 137.471 MB uploadedwandb: / 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17217
wandb:   val_loss 1.1726
wandb: 
wandb: üöÄ View run devout-sweep-565 at: https://wandb.ai/7shoe/domShift-extensive/runs/iqmzs1td
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182925-iqmzs1td/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1722 | Val Loss: 1.1726
2025-03-26 18:30:35,190 - wandb.wandb_agent - INFO - Cleaning up finished run: iqmzs1td
2025-03-26 18:30:35,731 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:35,732 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:30:35,734 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:30:40,747 - wandb.wandb_agent - INFO - Running runs: ['2h3vsn1b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183041-2h3vsn1b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-574
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2h3vsn1b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2h3vsn1b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.1801 | Val Loss: 2.6883
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.5888 | Val Loss: 2.5926
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.5988 | Val Loss: 2.5444
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.5891 | Val Loss: 2.6202
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.5035 | Val Loss: 2.3401
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.2927 | Val Loss: 2.2490
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.1443 | Val Loss: 2.0563
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.9859 | Val Loss: 1.9122
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.8507 | Val Loss: 1.7973
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.415 MB uploadedwandb: | 137.256 MB of 137.415 MB uploadedwandb: / 137.415 MB of 137.415 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.75152
wandb:   val_loss 1.69028
wandb: 
wandb: üöÄ View run different-sweep-574 at: https://wandb.ai/7shoe/domShift-extensive/runs/2h3vsn1b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183041-2h3vsn1b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.7515 | Val Loss: 1.6903
2025-03-26 18:32:06,994 - wandb.wandb_agent - INFO - Cleaning up finished run: 2h3vsn1b
2025-03-26 18:32:07,550 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:32:07,550 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:32:07,553 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:32:12,566 - wandb.wandb_agent - INFO - Running runs: ['4uojhtcs']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183213-4uojhtcs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-581
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4uojhtcs
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4uojhtcs
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3849 | Val Loss: 1.1431
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1265 | Val Loss: 1.1117
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0953 | Val Loss: 1.0675
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0866 | Val Loss: 1.2396
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1819 | Val Loss: 1.0961
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0713 | Val Loss: 1.0574
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0482 | Val Loss: 1.0432
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0380 | Val Loss: 1.0338
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0304 | Val Loss: 1.0276
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.02509
wandb:   val_loss 1.02277
wandb: 
wandb: üöÄ View run olive-sweep-581 at: https://wandb.ai/7shoe/domShift-extensive/runs/4uojhtcs
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183213-4uojhtcs/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0251 | Val Loss: 1.0228
2025-03-26 18:33:18,471 - wandb.wandb_agent - INFO - Cleaning up finished run: 4uojhtcs
2025-03-26 18:33:18,963 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:33:18,966 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:33:18,969 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:33:23,981 - wandb.wandb_agent - INFO - Running runs: ['go0aeh3u']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183325-go0aeh3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-589
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/go0aeh3u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: go0aeh3u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4844 | Val Loss: 1.4450
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3956 | Val Loss: 1.3304
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2901 | Val Loss: 1.2508
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2280 | Val Loss: 1.2191
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2133 | Val Loss: 1.1978
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1720 | Val Loss: 1.1433
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1219 | Val Loss: 1.0985
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0781 | Val Loss: 1.0580
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0464 | Val Loss: 1.0359
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03337
wandb:   val_loss 1.03212
wandb: 
wandb: üöÄ View run likely-sweep-589 at: https://wandb.ai/7shoe/domShift-extensive/runs/go0aeh3u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183325-go0aeh3u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0334 | Val Loss: 1.0321
2025-03-26 18:34:09,607 - wandb.wandb_agent - INFO - Cleaning up finished run: go0aeh3u
2025-03-26 18:34:10,265 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:34:10,265 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:34:10,268 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:34:15,281 - wandb.wandb_agent - INFO - Running runs: ['0fotvy2f']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183416-0fotvy2f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-594
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0fotvy2f
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0fotvy2f
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5448 | Val Loss: 1.3021
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1746 | Val Loss: 1.1713
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1175 | Val Loss: 1.0754
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0492 | Val Loss: 1.0195
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0028 | Val Loss: 0.9972
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0112 | Val Loss: 1.0230
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0268 | Val Loss: 1.0277
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0306 | Val Loss: 1.0344
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0392 | Val Loss: 1.0428
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0463
wandb:   val_loss 1.04964
wandb: 
wandb: üöÄ View run kind-sweep-594 at: https://wandb.ai/7shoe/domShift-extensive/runs/0fotvy2f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183416-0fotvy2f/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0463 | Val Loss: 1.0496
2025-03-26 18:35:11,091 - wandb.wandb_agent - INFO - Cleaning up finished run: 0fotvy2f
2025-03-26 18:35:11,931 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:35:11,932 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:35:11,935 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:35:16,947 - wandb.wandb_agent - INFO - Running runs: ['c4ud84xf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183517-c4ud84xf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-602
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/c4ud84xf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: c4ud84xf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.4671 | Val Loss: 1.8499
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.8115 | Val Loss: 1.8583
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.9353 | Val Loss: 1.9937
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0194 | Val Loss: 2.0351
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0072 | Val Loss: 2.0173
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0578 | Val Loss: 2.1011
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1414 | Val Loss: 2.1810
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2147 | Val Loss: 2.2469
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2727 | Val Loss: 2.2961
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.31263
wandb:   val_loss 2.32621
wandb: 
wandb: üöÄ View run laced-sweep-602 at: https://wandb.ai/7shoe/domShift-extensive/runs/c4ud84xf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183517-c4ud84xf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3126 | Val Loss: 2.3262
2025-03-26 18:36:38,111 - wandb.wandb_agent - INFO - Cleaning up finished run: c4ud84xf
2025-03-26 18:36:39,028 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:36:39,028 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:36:39,031 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:36:44,040 - wandb.wandb_agent - INFO - Running runs: ['ihgn2f4g']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183645-ihgn2f4g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-611
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ihgn2f4g
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ihgn2f4g
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5257 | Val Loss: 1.2632
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2885 | Val Loss: 1.2918
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2607 | Val Loss: 1.2590
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2550 | Val Loss: 1.2583
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2614 | Val Loss: 1.2640
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2618 | Val Loss: 1.2621
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2630 | Val Loss: 1.2649
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2637 | Val Loss: 1.2646
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2645 | Val Loss: 1.2658
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.26495
wandb:   val_loss 1.2662
wandb: 
wandb: üöÄ View run stilted-sweep-611 at: https://wandb.ai/7shoe/domShift-extensive/runs/ihgn2f4g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183645-ihgn2f4g/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2650 | Val Loss: 1.2662
2025-03-26 18:37:24,549 - wandb.wandb_agent - INFO - Cleaning up finished run: ihgn2f4g
2025-03-26 18:37:25,108 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:37:25,108 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:37:25,110 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:37:30,123 - wandb.wandb_agent - INFO - Running runs: ['y3zgat83']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183730-y3zgat83
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-617
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y3zgat83
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: y3zgat83
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2005 | Val Loss: 1.3015
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3663 | Val Loss: 1.3949
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3879 | Val Loss: 1.4059
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4030 | Val Loss: 1.4179
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4417 | Val Loss: 1.4587
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5076 | Val Loss: 1.5005
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1272 | Val Loss: 0.7603
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8272 | Val Loss: 0.8410
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8424 | Val Loss: 0.8506
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.84951
wandb:   val_loss 0.84817
wandb: 
wandb: üöÄ View run wise-sweep-617 at: https://wandb.ai/7shoe/domShift-extensive/runs/y3zgat83
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183730-y3zgat83/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8495 | Val Loss: 0.8482
2025-03-26 18:38:30,972 - wandb.wandb_agent - INFO - Cleaning up finished run: y3zgat83
2025-03-26 18:38:31,694 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:38:31,694 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:38:31,697 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:38:36,710 - wandb.wandb_agent - INFO - Running runs: ['lk4trvar']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183836-lk4trvar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-622
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lk4trvar
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lk4trvar
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5109 | Val Loss: 2.2944
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3074 | Val Loss: 2.3122
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2961 | Val Loss: 2.2511
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2088 | Val Loss: 2.1192
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0741 | Val Loss: 2.0444
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0435 | Val Loss: 2.0457
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0589 | Val Loss: 2.0741
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0926 | Val Loss: 2.1115
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1294 | Val Loss: 2.1468
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.16206
wandb:   val_loss 2.17694
wandb: 
wandb: üöÄ View run fine-sweep-622 at: https://wandb.ai/7shoe/domShift-extensive/runs/lk4trvar
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183836-lk4trvar/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1621 | Val Loss: 2.1769
2025-03-26 18:39:17,223 - wandb.wandb_agent - INFO - Cleaning up finished run: lk4trvar
2025-03-26 18:39:17,724 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:39:17,724 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:39:17,727 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:39:22,739 - wandb.wandb_agent - INFO - Running runs: ['m00a3dhe']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183922-m00a3dhe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-627
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m00a3dhe
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: m00a3dhe
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3661 | Val Loss: 1.2006
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1800 | Val Loss: 1.1693
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1732 | Val Loss: 1.1762
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1751 | Val Loss: 1.1741
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1686 | Val Loss: 1.1648
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1655 | Val Loss: 1.1650
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1674 | Val Loss: 1.1692
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1794 | Val Loss: 1.1871
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1955 | Val Loss: 1.2190
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22046
wandb:   val_loss 1.21125
wandb: 
wandb: üöÄ View run valiant-sweep-627 at: https://wandb.ai/7shoe/domShift-extensive/runs/m00a3dhe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183922-m00a3dhe/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2205 | Val Loss: 1.2112
2025-03-26 18:40:33,701 - wandb.wandb_agent - INFO - Cleaning up finished run: m00a3dhe
2025-03-26 18:40:34,339 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:40:34,339 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:40:34,342 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:40:39,355 - wandb.wandb_agent - INFO - Running runs: ['ji71jinw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184040-ji71jinw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-635
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ji71jinw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ji71jinw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2597 | Val Loss: 1.1884
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2393 | Val Loss: 1.2456
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2283 | Val Loss: 1.2204
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2253 | Val Loss: 1.2512
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2501 | Val Loss: 1.2476
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2501 | Val Loss: 1.2520
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2550 | Val Loss: 1.2564
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2561 | Val Loss: 1.2560
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2564 | Val Loss: 1.2567
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñÅ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25718
wandb:   val_loss 1.25757
wandb: 
wandb: üöÄ View run zesty-sweep-635 at: https://wandb.ai/7shoe/domShift-extensive/runs/ji71jinw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184040-ji71jinw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2572 | Val Loss: 1.2576
2025-03-26 18:41:55,411 - wandb.wandb_agent - INFO - Cleaning up finished run: ji71jinw
2025-03-26 18:41:55,956 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:41:55,956 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:41:55,959 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 18:42:00,972 - wandb.wandb_agent - INFO - Running runs: ['00tmvf65']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184202-00tmvf65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-644
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/00tmvf65
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 00tmvf65
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7438 | Val Loss: 1.5419
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5022 | Val Loss: 1.4073
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3218 | Val Loss: 1.2829
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2831 | Val Loss: 1.2780
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2739 | Val Loss: 1.2698
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2672 | Val Loss: 1.2638
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2577 | Val Loss: 1.2473
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2197 | Val Loss: 1.1308
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0743 | Val Loss: 1.0907
wandb: - 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07574
wandb:   val_loss 1.06395
wandb: 
wandb: üöÄ View run whole-sweep-644 at: https://wandb.ai/7shoe/domShift-extensive/runs/00tmvf65
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184202-00tmvf65/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0757 | Val Loss: 1.0639
2025-03-26 18:42:46,575 - wandb.wandb_agent - INFO - Cleaning up finished run: 00tmvf65
2025-03-26 18:42:58,782 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:42:58,783 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:42:58,786 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 18:43:03,798 - wandb.wandb_agent - INFO - Running runs: ['timv28dn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184303-timv28dn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-648
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/timv28dn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: timv28dn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2712 | Val Loss: 1.0744
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0714 | Val Loss: 1.0765
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0565 | Val Loss: 1.0366
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0264 | Val Loss: 1.0160
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0076 | Val Loss: 0.9990
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9959 | Val Loss: 0.9931
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9927 | Val Loss: 0.9919
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9919 | Val Loss: 0.9914
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9919 | Val Loss: 0.9919
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99272
wandb:   val_loss 0.99343
wandb: 
wandb: üöÄ View run prime-sweep-648 at: https://wandb.ai/7shoe/domShift-extensive/runs/timv28dn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184303-timv28dn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9927 | Val Loss: 0.9934
2025-03-26 18:43:59,581 - wandb.wandb_agent - INFO - Cleaning up finished run: timv28dn
2025-03-26 18:44:00,077 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:44:00,077 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:44:00,079 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:44:05,091 - wandb.wandb_agent - INFO - Running runs: ['s2oskaw6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184405-s2oskaw6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-657
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s2oskaw6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: s2oskaw6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3230 | Val Loss: 1.0031
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0871 | Val Loss: 1.1592
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1838 | Val Loss: 1.1992
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2014 | Val Loss: 1.2022
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2024 | Val Loss: 1.2032
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2022 | Val Loss: 1.2030
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2022 | Val Loss: 1.2025
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2022 | Val Loss: 1.2024
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2021 | Val Loss: 1.2024
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.856 MB of 32.856 MB uploadedwandb: / 32.856 MB of 32.856 MB uploadedwandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20197
wandb:   val_loss 1.20222
wandb: 
wandb: üöÄ View run celestial-sweep-657 at: https://wandb.ai/7shoe/domShift-extensive/runs/s2oskaw6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184405-s2oskaw6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2020 | Val Loss: 1.2022
2025-03-26 18:44:50,717 - wandb.wandb_agent - INFO - Cleaning up finished run: s2oskaw6
2025-03-26 18:44:51,358 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:44:51,358 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:44:51,361 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:44:56,373 - wandb.wandb_agent - INFO - Running runs: ['inompggs']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184456-inompggs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-662
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/inompggs
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: inompggs
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.7572 | Val Loss: 2.9502
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.5988 | Val Loss: 2.1151
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.9700 | Val Loss: 1.7614
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6622 | Val Loss: 1.5053
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4437 | Val Loss: 1.3802
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3181 | Val Loss: 1.2489
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2447 | Val Loss: 1.2199
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2129 | Val Loss: 1.1632
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1612 | Val Loss: 1.1334
wandb: - 32.989 MB of 32.989 MB uploadedwandb: \ 32.989 MB of 32.989 MB uploadedwandb: | 33.009 MB of 33.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14172
wandb:   val_loss 1.10516
wandb: 
wandb: üöÄ View run amber-sweep-662 at: https://wandb.ai/7shoe/domShift-extensive/runs/inompggs
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184456-inompggs/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1417 | Val Loss: 1.1052
2025-03-26 18:45:26,767 - wandb.wandb_agent - INFO - Cleaning up finished run: inompggs
2025-03-26 18:45:27,448 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:45:27,448 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:45:27,451 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimCLR --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 18:45:32,464 - wandb.wandb_agent - INFO - Running runs: ['d30pae1e']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184533-d30pae1e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-664
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/d30pae1e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: d30pae1e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.5439 | Val Loss: 5.0009
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 5.0789 | Val Loss: 4.7079
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 4.9045 | Val Loss: 4.5948
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 4.8328 | Val Loss: 4.5442
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 4.7631 | Val Loss: 4.4908
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 4.7416 | Val Loss: 4.4852
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 4.7113 | Val Loss: 4.4574
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 4.6978 | Val Loss: 4.4631
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 4.6792 | Val Loss: 4.4291
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.195 MB of 137.195 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.66716
wandb:   val_loss 4.43294
wandb: 
wandb: üöÄ View run vivid-sweep-664 at: https://wandb.ai/7shoe/domShift-extensive/runs/d30pae1e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184533-d30pae1e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 4.6672 | Val Loss: 4.4329
2025-03-26 18:46:38,405 - wandb.wandb_agent - INFO - Cleaning up finished run: d30pae1e
2025-03-26 18:46:38,960 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:46:38,961 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:46:38,964 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:46:43,976 - wandb.wandb_agent - INFO - Running runs: ['7xlrf5x6']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184643-7xlrf5x6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-670
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7xlrf5x6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7xlrf5x6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.8897 | Val Loss: 1.8164
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6945 | Val Loss: 1.6184
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5937 | Val Loss: 1.5716
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5127 | Val Loss: 1.4524
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4066 | Val Loss: 1.3759
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3781 | Val Loss: 1.3751
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3713 | Val Loss: 1.3653
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3600 | Val Loss: 1.3500
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3387 | Val Loss: 1.3244
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31272
wandb:   val_loss 1.29967
wandb: 
wandb: üöÄ View run jolly-sweep-670 at: https://wandb.ai/7shoe/domShift-extensive/runs/7xlrf5x6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184643-7xlrf5x6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3127 | Val Loss: 1.2997
2025-03-26 18:47:19,435 - wandb.wandb_agent - INFO - Cleaning up finished run: 7xlrf5x6
2025-03-26 18:47:19,992 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:47:19,992 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:47:19,995 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:47:25,006 - wandb.wandb_agent - INFO - Running runs: ['y28vivs2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184725-y28vivs2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-674
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y28vivs2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: y28vivs2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6182 | Val Loss: 1.3839
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2617 | Val Loss: 1.1535
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1105 | Val Loss: 1.0685
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0650 | Val Loss: 1.0652
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0659 | Val Loss: 1.0647
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0649 | Val Loss: 1.0643
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0637 | Val Loss: 1.0635
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0636 | Val Loss: 1.0635
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0631 | Val Loss: 1.0635
wandb: - 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06444
wandb:   val_loss 1.06407
wandb: 
wandb: üöÄ View run breezy-sweep-674 at: https://wandb.ai/7shoe/domShift-extensive/runs/y28vivs2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184725-y28vivs2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0644 | Val Loss: 1.0641
2025-03-26 18:48:10,664 - wandb.wandb_agent - INFO - Cleaning up finished run: y28vivs2
2025-03-26 18:48:11,339 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:48:11,339 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:48:11,342 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.1
2025-03-26 18:48:16,354 - wandb.wandb_agent - INFO - Running runs: ['u6af9mhv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184817-u6af9mhv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-680
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u6af9mhv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: u6af9mhv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.5841 | Val Loss: 2.3021
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.2433 | Val Loss: 2.1949
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.2005 | Val Loss: 2.1395
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.0998 | Val Loss: 2.0115
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.9735 | Val Loss: 1.9027
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.8857 | Val Loss: 1.8412
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.8142 | Val Loss: 1.7411
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.7080 | Val Loss: 1.6394
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.6144 | Val Loss: 1.5548
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.52689
wandb:   val_loss 1.46346
wandb: 
wandb: üöÄ View run frosty-sweep-680 at: https://wandb.ai/7shoe/domShift-extensive/runs/u6af9mhv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184817-u6af9mhv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.5269 | Val Loss: 1.4635
2025-03-26 18:48:41,670 - wandb.wandb_agent - INFO - Cleaning up finished run: u6af9mhv
2025-03-26 18:48:42,473 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:48:42,473 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:48:42,475 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:48:47,487 - wandb.wandb_agent - INFO - Running runs: ['mf405q2n']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184847-mf405q2n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-686
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mf405q2n
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: mf405q2n
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.1524 | Val Loss: 1.0068
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0082 | Val Loss: 1.0257
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0543 | Val Loss: 1.0678
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0723 | Val Loss: 1.0753
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0748 | Val Loss: 1.0767
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0789 | Val Loss: 1.0806
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0819 | Val Loss: 1.0841
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0853 | Val Loss: 1.0871
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0884 | Val Loss: 1.0902
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09123
wandb:   val_loss 1.09264
wandb: 
wandb: üöÄ View run true-sweep-686 at: https://wandb.ai/7shoe/domShift-extensive/runs/mf405q2n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184847-mf405q2n/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0912 | Val Loss: 1.0926
2025-03-26 18:49:38,298 - wandb.wandb_agent - INFO - Cleaning up finished run: mf405q2n
2025-03-26 18:49:39,208 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:49:39,208 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:49:39,211 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:49:44,223 - wandb.wandb_agent - INFO - Running runs: ['b2w60kqx']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184944-b2w60kqx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-691
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b2w60kqx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: b2w60kqx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3041 | Val Loss: 1.1608
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1917 | Val Loss: 1.1751
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1425 | Val Loss: 1.1299
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1534 | Val Loss: 1.1739
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1807 | Val Loss: 1.1846
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1911 | Val Loss: 1.2050
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2049 | Val Loss: 1.2034
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2044 | Val Loss: 1.2060
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2066 | Val Loss: 1.2072
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20742
wandb:   val_loss 1.20742
wandb: 
wandb: üöÄ View run fresh-sweep-691 at: https://wandb.ai/7shoe/domShift-extensive/runs/b2w60kqx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184944-b2w60kqx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2074 | Val Loss: 1.2074
2025-03-26 18:50:34,892 - wandb.wandb_agent - INFO - Cleaning up finished run: b2w60kqx
2025-03-26 18:50:35,410 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:50:35,411 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:50:35,413 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:50:40,425 - wandb.wandb_agent - INFO - Running runs: ['tl4ht4pi']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185040-tl4ht4pi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-697
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tl4ht4pi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tl4ht4pi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2828 | Val Loss: 1.0945
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0574 | Val Loss: 1.0447
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0468 | Val Loss: 1.0446
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0432 | Val Loss: 1.0415
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0442 | Val Loss: 1.0478
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0512 | Val Loss: 1.0522
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0418 | Val Loss: 1.0411
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0707 | Val Loss: 1.0980
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1132 | Val Loss: 1.0858
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñá‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06471
wandb:   val_loss 1.04464
wandb: 
wandb: üöÄ View run proud-sweep-697 at: https://wandb.ai/7shoe/domShift-extensive/runs/tl4ht4pi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185040-tl4ht4pi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0647 | Val Loss: 1.0446
2025-03-26 18:51:26,082 - wandb.wandb_agent - INFO - Cleaning up finished run: tl4ht4pi
2025-03-26 18:51:27,009 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:51:27,010 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:51:27,012 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:51:32,025 - wandb.wandb_agent - INFO - Running runs: ['fcaar0i5']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185133-fcaar0i5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-702
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fcaar0i5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fcaar0i5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7125 | Val Loss: 1.6374
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5601 | Val Loss: 1.6484
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5040 | Val Loss: 1.4922
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5132 | Val Loss: 1.5477
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5525 | Val Loss: 1.3662
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1566 | Val Loss: 1.0178
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0809 | Val Loss: 1.0538
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0364 | Val Loss: 1.0319
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0238 | Val Loss: 1.0162
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÜ‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0141
wandb:   val_loss 1.01292
wandb: 
wandb: üöÄ View run distinctive-sweep-702 at: https://wandb.ai/7shoe/domShift-extensive/runs/fcaar0i5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185133-fcaar0i5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0141 | Val Loss: 1.0129
2025-03-26 18:52:42,983 - wandb.wandb_agent - INFO - Cleaning up finished run: fcaar0i5
2025-03-26 18:52:43,918 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:52:43,918 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:52:43,921 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:52:48,933 - wandb.wandb_agent - INFO - Running runs: ['f0w8hyri']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185249-f0w8hyri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-710
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f0w8hyri
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: f0w8hyri
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5224 | Val Loss: 1.5396
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4953 | Val Loss: 1.5366
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3976 | Val Loss: 1.3895
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3793 | Val Loss: 1.3780
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3793 | Val Loss: 1.3784
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3796 | Val Loss: 1.3787
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3797 | Val Loss: 1.3789
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3797 | Val Loss: 1.3789
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3798 | Val Loss: 1.3789
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37978
wandb:   val_loss 1.37895
wandb: 
wandb: üöÄ View run icy-sweep-710 at: https://wandb.ai/7shoe/domShift-extensive/runs/f0w8hyri
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185249-f0w8hyri/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3798 | Val Loss: 1.3789
2025-03-26 18:53:59,887 - wandb.wandb_agent - INFO - Cleaning up finished run: f0w8hyri
2025-03-26 18:54:00,429 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:54:00,429 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:54:00,432 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 18:54:05,440 - wandb.wandb_agent - INFO - Running runs: ['biwacpz1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185405-biwacpz1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-718
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/biwacpz1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: biwacpz1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.5522 | Val Loss: 5.1498
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 5.1572 | Val Loss: 4.8246
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 4.9344 | Val Loss: 4.6578
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 4.7434 | Val Loss: 4.5304
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 4.6328 | Val Loss: 4.5464
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 4.5803 | Val Loss: 4.3776
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 4.5042 | Val Loss: 4.2950
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 4.4580 | Val Loss: 4.2421
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 4.4172 | Val Loss: 4.2288
wandb: - 137.081 MB of 137.081 MB uploadedwandb: \ 137.081 MB of 137.081 MB uploadedwandb: | 137.101 MB of 137.101 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.38916
wandb:   val_loss 4.21173
wandb: 
wandb: üöÄ View run vocal-sweep-718 at: https://wandb.ai/7shoe/domShift-extensive/runs/biwacpz1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185405-biwacpz1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 4.3892 | Val Loss: 4.2117
2025-03-26 18:54:51,069 - wandb.wandb_agent - INFO - Cleaning up finished run: biwacpz1
2025-03-26 18:54:51,587 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:54:51,587 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:54:51,590 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimCLR --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:54:56,602 - wandb.wandb_agent - INFO - Running runs: ['dyoaex5r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185457-dyoaex5r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-723
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dyoaex5r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dyoaex5r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.9799 | Val Loss: 3.5723
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.5580 | Val Loss: 3.4661
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.4885 | Val Loss: 3.3993
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.4335 | Val Loss: 3.3655
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.3996 | Val Loss: 3.3308
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.3787 | Val Loss: 3.3109
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.3618 | Val Loss: 3.2936
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.3430 | Val Loss: 3.2789
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.3269 | Val Loss: 3.2721
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.31443
wandb:   val_loss 3.25756
wandb: 
wandb: üöÄ View run dainty-sweep-723 at: https://wandb.ai/7shoe/domShift-extensive/runs/dyoaex5r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185457-dyoaex5r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.3144 | Val Loss: 3.2576
2025-03-26 18:55:42,234 - wandb.wandb_agent - INFO - Cleaning up finished run: dyoaex5r
2025-03-26 18:55:42,852 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:55:42,853 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:55:42,856 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.4
2025-03-26 18:55:47,868 - wandb.wandb_agent - INFO - Running runs: ['0jzbb2xo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185548-0jzbb2xo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-728
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0jzbb2xo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0jzbb2xo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7672 | Val Loss: 1.6674
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4820 | Val Loss: 1.2798
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2663 | Val Loss: 1.2695
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2137 | Val Loss: 1.1273
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1323 | Val Loss: 1.1183
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1220 | Val Loss: 1.1516
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1314 | Val Loss: 1.0887
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0819 | Val Loss: 1.0930
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1049 | Val Loss: 1.1125
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10898
wandb:   val_loss 1.10382
wandb: 
wandb: üöÄ View run fearless-sweep-728 at: https://wandb.ai/7shoe/domShift-extensive/runs/0jzbb2xo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185548-0jzbb2xo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1090 | Val Loss: 1.1038
2025-03-26 18:56:28,462 - wandb.wandb_agent - INFO - Cleaning up finished run: 0jzbb2xo
2025-03-26 18:56:28,997 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:56:28,997 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:56:29,000 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:56:34,012 - wandb.wandb_agent - INFO - Running runs: ['vllbvo3d']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185634-vllbvo3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-733
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vllbvo3d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: vllbvo3d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3119 | Val Loss: 1.0044
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0095 | Val Loss: 1.0261
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0446 | Val Loss: 1.0524
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0424 | Val Loss: 1.0379
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0406 | Val Loss: 1.0414
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0386 | Val Loss: 1.0368
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0080 | Val Loss: 0.9989
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0197 | Val Loss: 1.0230
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0308 | Val Loss: 1.0462
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.02727
wandb:   val_loss 1.0256
wandb: 
wandb: üöÄ View run dazzling-sweep-733 at: https://wandb.ai/7shoe/domShift-extensive/runs/vllbvo3d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185634-vllbvo3d/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0273 | Val Loss: 1.0256
2025-03-26 18:57:19,626 - wandb.wandb_agent - INFO - Cleaning up finished run: vllbvo3d
2025-03-26 18:57:20,147 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:20,147 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:57:20,150 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.7000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:57:25,163 - wandb.wandb_agent - INFO - Running runs: ['zg0i84ti']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185725-zg0i84ti
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-739
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zg0i84ti
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zg0i84ti
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2437 | Val Loss: 1.3035
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2729 | Val Loss: 1.2626
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1992 | Val Loss: 1.1158
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0663 | Val Loss: 1.0248
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0145 | Val Loss: 1.0118
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0230 | Val Loss: 1.0397
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0572 | Val Loss: 1.0739
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0767 | Val Loss: 1.0764
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0763 | Val Loss: 1.0788
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08381
wandb:   val_loss 1.09073
wandb: 
wandb: üöÄ View run confused-sweep-739 at: https://wandb.ai/7shoe/domShift-extensive/runs/zg0i84ti
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185725-zg0i84ti/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0838 | Val Loss: 1.0907
2025-03-26 18:58:05,698 - wandb.wandb_agent - INFO - Cleaning up finished run: zg0i84ti
2025-03-26 18:58:06,189 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:58:06,189 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:58:06,192 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:58:11,207 - wandb.wandb_agent - INFO - Running runs: ['pavqh98n']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185811-pavqh98n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-745
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pavqh98n
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pavqh98n
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1962 | Val Loss: 1.1745
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2784 | Val Loss: 1.3405
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3428 | Val Loss: 1.3460
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3442 | Val Loss: 1.3462
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3448 | Val Loss: 1.3467
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3455 | Val Loss: 1.3471
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3461 | Val Loss: 1.3469
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3469 | Val Loss: 1.3483
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3482 | Val Loss: 1.3517
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34966
wandb:   val_loss 1.34943
wandb: 
wandb: üöÄ View run good-sweep-745 at: https://wandb.ai/7shoe/domShift-extensive/runs/pavqh98n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185811-pavqh98n/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3497 | Val Loss: 1.3494
2025-03-26 18:58:56,917 - wandb.wandb_agent - INFO - Cleaning up finished run: pavqh98n
2025-03-26 18:59:01,625 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:59:01,625 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:59:01,628 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:59:06,640 - wandb.wandb_agent - INFO - Running runs: ['xihck9dq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185906-xihck9dq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-749
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xihck9dq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: xihck9dq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2276 | Val Loss: 1.2543
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4140 | Val Loss: 1.4231
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2980 | Val Loss: 1.1792
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1510 | Val Loss: 1.1700
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1637 | Val Loss: 1.1368
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1898 | Val Loss: 1.1520
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1536 | Val Loss: 1.2695
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2485 | Val Loss: 1.2333
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2391 | Val Loss: 1.2186
wandb: - 34.376 MB of 34.376 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27308
wandb:   val_loss 1.19993
wandb: 
wandb: üöÄ View run glowing-sweep-749 at: https://wandb.ai/7shoe/domShift-extensive/runs/xihck9dq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185906-xihck9dq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2731 | Val Loss: 1.1999
2025-03-26 18:59:52,282 - wandb.wandb_agent - INFO - Cleaning up finished run: xihck9dq
2025-03-26 18:59:53,203 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:59:53,203 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:59:53,206 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:59:58,219 - wandb.wandb_agent - INFO - Running runs: ['laj8hazu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185959-laj8hazu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-754
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/laj8hazu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: laj8hazu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7699 | Val Loss: 1.5963
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3967 | Val Loss: 1.2830
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1367 | Val Loss: 1.0366
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0438 | Val Loss: 1.0550
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0621 | Val Loss: 1.0668
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0704 | Val Loss: 1.0736
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0804 | Val Loss: 1.0924
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1121 | Val Loss: 1.1356
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1595 | Val Loss: 1.1820
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.089 MB of 33.089 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19429
wandb:   val_loss 1.18246
wandb: 
wandb: üöÄ View run earnest-sweep-754 at: https://wandb.ai/7shoe/domShift-extensive/runs/laj8hazu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185959-laj8hazu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1943 | Val Loss: 1.1825
2025-03-26 19:00:43,846 - wandb.wandb_agent - INFO - Cleaning up finished run: laj8hazu
2025-03-26 19:00:45,092 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:45,093 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:00:45,096 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:00:50,108 - wandb.wandb_agent - INFO - Running runs: ['eijwa9oa']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190051-eijwa9oa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-760
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/eijwa9oa
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: eijwa9oa
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5387 | Val Loss: 1.4341
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4831 | Val Loss: 1.4802
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4674 | Val Loss: 1.4304
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4189 | Val Loss: 1.3846
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3477 | Val Loss: 1.2981
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2585 | Val Loss: 1.2153
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1926 | Val Loss: 1.1742
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1654 | Val Loss: 1.1548
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1432 | Val Loss: 1.1306
wandb: - 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12105
wandb:   val_loss 1.11185
wandb: 
wandb: üöÄ View run sunny-sweep-760 at: https://wandb.ai/7shoe/domShift-extensive/runs/eijwa9oa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190051-eijwa9oa/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1210 | Val Loss: 1.1119
2025-03-26 19:01:20,507 - wandb.wandb_agent - INFO - Cleaning up finished run: eijwa9oa
2025-03-26 19:01:21,063 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:01:21,063 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:01:21,066 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:01:26,078 - wandb.wandb_agent - INFO - Running runs: ['oz4n43x6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190126-oz4n43x6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-765
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/oz4n43x6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: oz4n43x6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7171 | Val Loss: 1.4692
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3821 | Val Loss: 1.3200
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3112 | Val Loss: 1.2942
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2805 | Val Loss: 1.2658
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2375 | Val Loss: 1.2061
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2107 | Val Loss: 1.2310
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2723 | Val Loss: 1.3236
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3853 | Val Loss: 1.4480
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4457 | Val Loss: 1.3894
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31685
wandb:   val_loss 1.25623
wandb: 
wandb: üöÄ View run zesty-sweep-765 at: https://wandb.ai/7shoe/domShift-extensive/runs/oz4n43x6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190126-oz4n43x6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3169 | Val Loss: 1.2562
2025-03-26 19:01:56,520 - wandb.wandb_agent - INFO - Cleaning up finished run: oz4n43x6
2025-03-26 19:01:57,070 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:01:57,070 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:01:57,073 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:02:02,085 - wandb.wandb_agent - INFO - Running runs: ['2ty6v7wa']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190203-2ty6v7wa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-769
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2ty6v7wa
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 2ty6v7wa
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 5.5161 | Val Loss: 4.8986
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 4.7096 | Val Loss: 4.2464
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 4.4525 | Val Loss: 4.1595
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 4.3425 | Val Loss: 4.0465
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 4.2472 | Val Loss: 3.9210
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 4.1571 | Val Loss: 3.8514
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 4.1173 | Val Loss: 3.8809
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 4.0487 | Val Loss: 3.7868
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 4.0242 | Val Loss: 3.7267
wandb: - 32.849 MB of 32.849 MB uploadedwandb: \ 32.849 MB of 32.849 MB uploadedwandb: | 32.942 MB of 32.942 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.9661
wandb:   val_loss 3.72111
wandb: 
wandb: üöÄ View run stellar-sweep-769 at: https://wandb.ai/7shoe/domShift-extensive/runs/2ty6v7wa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190203-2ty6v7wa/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 3.9661 | Val Loss: 3.7211
2025-03-26 19:02:47,713 - wandb.wandb_agent - INFO - Cleaning up finished run: 2ty6v7wa
2025-03-26 19:02:48,541 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:02:48,541 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:02:48,545 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 19:02:53,558 - wandb.wandb_agent - INFO - Running runs: ['j79ev1p6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190253-j79ev1p6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-775
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/j79ev1p6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: j79ev1p6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2838 | Val Loss: 1.1570
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0788 | Val Loss: 0.9731
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9556 | Val Loss: 0.9510
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9519 | Val Loss: 0.9527
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9509 | Val Loss: 0.9493
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9485 | Val Loss: 0.9475
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9462 | Val Loss: 0.9451
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9438 | Val Loss: 0.9428
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9417 | Val Loss: 0.9407
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93979
wandb:   val_loss 0.93912
wandb: 
wandb: üöÄ View run serene-sweep-775 at: https://wandb.ai/7shoe/domShift-extensive/runs/j79ev1p6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190253-j79ev1p6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9398 | Val Loss: 0.9391
2025-03-26 19:03:29,107 - wandb.wandb_agent - INFO - Cleaning up finished run: j79ev1p6
2025-03-26 19:03:29,646 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:03:29,646 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:03:29,649 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:03:34,661 - wandb.wandb_agent - INFO - Running runs: ['t3cig0qp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190334-t3cig0qp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-780
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/t3cig0qp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: t3cig0qp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.2931 | Val Loss: 4.7947
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.6181 | Val Loss: 4.2209
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 4.1819 | Val Loss: 3.9147
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.9491 | Val Loss: 3.7021
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.7458 | Val Loss: 3.5006
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.5869 | Val Loss: 3.3839
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.4903 | Val Loss: 3.2980
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.3999 | Val Loss: 3.2164
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.3359 | Val Loss: 3.1330
wandb: - 137.081 MB of 137.081 MB uploadedwandb: \ 137.081 MB of 137.081 MB uploadedwandb: | 137.101 MB of 137.101 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.26837
wandb:   val_loss 3.03849
wandb: 
wandb: üöÄ View run clear-sweep-780 at: https://wandb.ai/7shoe/domShift-extensive/runs/t3cig0qp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190334-t3cig0qp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.2684 | Val Loss: 3.0385
2025-03-26 19:04:20,267 - wandb.wandb_agent - INFO - Cleaning up finished run: t3cig0qp
2025-03-26 19:04:20,769 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:04:20,770 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:04:20,773 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:04:25,786 - wandb.wandb_agent - INFO - Running runs: ['uxa8i0q7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190426-uxa8i0q7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-784
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uxa8i0q7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: uxa8i0q7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3483 | Val Loss: 1.2648
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3128 | Val Loss: 1.3782
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3372 | Val Loss: 1.3435
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3154 | Val Loss: 1.3086
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3305 | Val Loss: 1.3330
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3336 | Val Loss: 1.3284
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3308 | Val Loss: 1.3304
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3293 | Val Loss: 1.3311
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3282 | Val Loss: 1.3306
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÜ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32729
wandb:   val_loss 1.32936
wandb: 
wandb: üöÄ View run crisp-sweep-784 at: https://wandb.ai/7shoe/domShift-extensive/runs/uxa8i0q7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190426-uxa8i0q7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3273 | Val Loss: 1.3294
2025-03-26 19:05:41,898 - wandb.wandb_agent - INFO - Cleaning up finished run: uxa8i0q7
2025-03-26 19:05:42,484 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:05:42,484 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:05:42,487 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:05:47,497 - wandb.wandb_agent - INFO - Running runs: ['3prp1u4z']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190547-3prp1u4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-793
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3prp1u4z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3prp1u4z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.0326 | Val Loss: 3.2229
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.8978 | Val Loss: 2.6281
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5765 | Val Loss: 2.4436
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4560 | Val Loss: 2.3569
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3459 | Val Loss: 2.2722
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2731 | Val Loss: 2.2033
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2290 | Val Loss: 2.1756
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1986 | Val Loss: 2.1506
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1748 | Val Loss: 2.1229
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.312 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.16556
wandb:   val_loss 2.11731
wandb: 
wandb: üöÄ View run fallen-sweep-793 at: https://wandb.ai/7shoe/domShift-extensive/runs/3prp1u4z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190547-3prp1u4z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1656 | Val Loss: 2.1173
2025-03-26 19:06:58,447 - wandb.wandb_agent - INFO - Cleaning up finished run: 3prp1u4z
2025-03-26 19:06:59,123 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:06:59,124 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:06:59,127 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:07:04,139 - wandb.wandb_agent - INFO - Running runs: ['kuriupi8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190704-kuriupi8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-800
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kuriupi8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: kuriupi8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9138 | Val Loss: 2.6271
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6132 | Val Loss: 2.5035
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2961 | Val Loss: 2.1137
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1309 | Val Loss: 2.1531
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1864 | Val Loss: 2.2205
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2587 | Val Loss: 2.2964
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3199 | Val Loss: 2.3407
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3767 | Val Loss: 2.4074
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4267 | Val Loss: 2.4408
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.469 MB uploadedwandb: | 137.311 MB of 137.469 MB uploadedwandb: / 137.469 MB of 137.469 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.44759
wandb:   val_loss 2.45061
wandb: 
wandb: üöÄ View run vivid-sweep-800 at: https://wandb.ai/7shoe/domShift-extensive/runs/kuriupi8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190704-kuriupi8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4476 | Val Loss: 2.4506
2025-03-26 19:08:30,420 - wandb.wandb_agent - INFO - Cleaning up finished run: kuriupi8
2025-03-26 19:08:31,036 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:08:31,036 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:08:31,039 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:08:36,050 - wandb.wandb_agent - INFO - Running runs: ['hoko6y8b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190836-hoko6y8b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-808
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hoko6y8b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hoko6y8b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.3506 | Val Loss: 2.6771
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7355 | Val Loss: 2.6958
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6791 | Val Loss: 2.7128
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7571 | Val Loss: 2.8034
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8358 | Val Loss: 2.8667
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8577 | Val Loss: 2.8343
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.8172 | Val Loss: 2.8028
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.7867 | Val Loss: 2.7743
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.7613 | Val Loss: 2.7524
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.7419
wandb:   val_loss 2.73565
wandb: 
wandb: üöÄ View run peachy-sweep-808 at: https://wandb.ai/7shoe/domShift-extensive/runs/hoko6y8b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190836-hoko6y8b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7419 | Val Loss: 2.7356
2025-03-26 19:09:11,543 - wandb.wandb_agent - INFO - Cleaning up finished run: hoko6y8b
2025-03-26 19:09:12,071 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:09:12,072 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:09:12,075 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 19:09:17,087 - wandb.wandb_agent - INFO - Running runs: ['iv73gvji']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190917-iv73gvji
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-813
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/iv73gvji
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: iv73gvji
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9641 | Val Loss: 2.7630
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7334 | Val Loss: 2.6190
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4988 | Val Loss: 2.3970
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.3362 | Val Loss: 2.2586
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1879 | Val Loss: 2.1152
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0226 | Val Loss: 1.9825
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.9977 | Val Loss: 2.0280
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0512 | Val Loss: 2.0746
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0879 | Val Loss: 2.1049
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.12544
wandb:   val_loss 2.15332
wandb: 
wandb: üöÄ View run fancy-sweep-813 at: https://wandb.ai/7shoe/domShift-extensive/runs/iv73gvji
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190917-iv73gvji/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1254 | Val Loss: 2.1533
2025-03-26 19:10:02,725 - wandb.wandb_agent - INFO - Cleaning up finished run: iv73gvji
2025-03-26 19:10:03,341 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:03,341 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:10:03,344 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 19:10:08,356 - wandb.wandb_agent - INFO - Running runs: ['ijx2w4w0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191009-ijx2w4w0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-818
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ijx2w4w0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ijx2w4w0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2529 | Val Loss: 1.1148
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1880 | Val Loss: 1.2302
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2394 | Val Loss: 1.2419
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2371 | Val Loss: 1.2347
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2314 | Val Loss: 1.2286
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2199 | Val Loss: 1.2079
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1961 | Val Loss: 1.1898
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1927 | Val Loss: 1.1990
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2089 | Val Loss: 1.2197
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22591
wandb:   val_loss 1.23194
wandb: 
wandb: üöÄ View run absurd-sweep-818 at: https://wandb.ai/7shoe/domShift-extensive/runs/ijx2w4w0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191009-ijx2w4w0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2259 | Val Loss: 1.2319
2025-03-26 19:11:24,484 - wandb.wandb_agent - INFO - Cleaning up finished run: ijx2w4w0
2025-03-26 19:11:24,975 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:11:24,975 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:11:24,978 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:11:29,990 - wandb.wandb_agent - INFO - Running runs: ['v7uh8oy1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191131-v7uh8oy1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-826
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/v7uh8oy1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: v7uh8oy1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4401 | Val Loss: 1.2260
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1385 | Val Loss: 1.0849
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0123 | Val Loss: 0.9072
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.8811 | Val Loss: 0.8899
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.8856 | Val Loss: 0.8743
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.8725 | Val Loss: 0.8729
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.8726 | Val Loss: 0.8730
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8727 | Val Loss: 0.8730
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8727 | Val Loss: 0.8729
wandb: - 32.875 MB of 32.875 MB uploadedwandb: \ 32.875 MB of 32.875 MB uploadedwandb: | 32.968 MB of 32.968 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.87265
wandb:   val_loss 0.87293
wandb: 
wandb: üöÄ View run atomic-sweep-826 at: https://wandb.ai/7shoe/domShift-extensive/runs/v7uh8oy1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191131-v7uh8oy1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8726 | Val Loss: 0.8729
2025-03-26 19:12:15,659 - wandb.wandb_agent - INFO - Cleaning up finished run: v7uh8oy1
2025-03-26 19:12:16,108 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:12:16,108 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:12:16,110 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:12:21,123 - wandb.wandb_agent - INFO - Running runs: ['lsinqyrj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191222-lsinqyrj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-833
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lsinqyrj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lsinqyrj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3410 | Val Loss: 1.0376
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0038 | Val Loss: 0.9776
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9550 | Val Loss: 0.9195
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8963 | Val Loss: 0.8757
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8613 | Val Loss: 0.8559
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8571 | Val Loss: 0.8580
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8605 | Val Loss: 0.8597
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8544 | Val Loss: 0.8523
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8482 | Val Loss: 0.8534
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.86802
wandb:   val_loss 0.8833
wandb: 
wandb: üöÄ View run gentle-sweep-833 at: https://wandb.ai/7shoe/domShift-extensive/runs/lsinqyrj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191222-lsinqyrj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8680 | Val Loss: 0.8833
2025-03-26 19:13:16,894 - wandb.wandb_agent - INFO - Cleaning up finished run: lsinqyrj
2025-03-26 19:13:17,567 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:13:17,567 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:13:17,571 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 19:13:22,583 - wandb.wandb_agent - INFO - Running runs: ['xggla41z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191323-xggla41z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-840
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xggla41z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xggla41z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.0952 | Val Loss: 2.5656
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.5611 | Val Loss: 2.4872
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.3988 | Val Loss: 2.2788
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.2054 | Val Loss: 2.1212
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.0693 | Val Loss: 2.0456
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.0924 | Val Loss: 2.1514
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.2049 | Val Loss: 2.2567
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.2909 | Val Loss: 2.3281
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.3614 | Val Loss: 2.3941
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.256 MB uploadedwandb: | 137.276 MB of 137.276 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.41436
wandb:   val_loss 2.41712
wandb: 
wandb: üöÄ View run still-sweep-840 at: https://wandb.ai/7shoe/domShift-extensive/runs/xggla41z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191323-xggla41z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.4144 | Val Loss: 2.4171
2025-03-26 19:14:08,227 - wandb.wandb_agent - INFO - Cleaning up finished run: xggla41z
2025-03-26 19:14:09,514 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:14:09,514 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:14:09,517 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 19:14:14,529 - wandb.wandb_agent - INFO - Running runs: ['uco3rlv2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191415-uco3rlv2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-845
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uco3rlv2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: uco3rlv2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.0807 | Val Loss: 2.4307
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.0348 | Val Loss: 1.9172
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.8948 | Val Loss: 1.8647
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.9526 | Val Loss: 2.0570
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.1438 | Val Loss: 2.2125
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.1443 | Val Loss: 2.1607
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.2538 | Val Loss: 2.3275
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.3885 | Val Loss: 2.4483
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.5358 | Val Loss: 2.6417
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.256 MB uploadedwandb: | 137.415 MB of 137.415 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ
wandb:   val_loss ‚ñÖ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.72067
wandb:   val_loss 2.75142
wandb: 
wandb: üöÄ View run floral-sweep-845 at: https://wandb.ai/7shoe/domShift-extensive/runs/uco3rlv2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191415-uco3rlv2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.7207 | Val Loss: 2.7514
2025-03-26 19:15:40,799 - wandb.wandb_agent - INFO - Cleaning up finished run: uco3rlv2
2025-03-26 19:15:41,434 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:15:41,434 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:15:41,437 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 19:15:46,449 - wandb.wandb_agent - INFO - Running runs: ['81rvhnrn']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191546-81rvhnrn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-854
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/81rvhnrn
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 81rvhnrn
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7211 | Val Loss: 1.5398
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2950 | Val Loss: 1.1500
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1590 | Val Loss: 1.1655
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1625 | Val Loss: 1.1587
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1614 | Val Loss: 1.1644
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1716 | Val Loss: 1.1832
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2047 | Val Loss: 1.2360
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2805 | Val Loss: 1.3369
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4083 | Val Loss: 1.5138
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.50624
wandb:   val_loss 1.52565
wandb: 
wandb: üöÄ View run rural-sweep-854 at: https://wandb.ai/7shoe/domShift-extensive/runs/81rvhnrn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191546-81rvhnrn/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5062 | Val Loss: 1.5256
2025-03-26 19:16:27,004 - wandb.wandb_agent - INFO - Cleaning up finished run: 81rvhnrn
2025-03-26 19:16:27,850 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:16:27,850 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:16:27,853 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:16:32,866 - wandb.wandb_agent - INFO - Running runs: ['6glogktj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191634-6glogktj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-860
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6glogktj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6glogktj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5492 | Val Loss: 1.3249
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1159 | Val Loss: 0.9054
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8465 | Val Loss: 0.8252
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8202 | Val Loss: 0.8132
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8155 | Val Loss: 0.8169
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8198 | Val Loss: 0.8219
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8239 | Val Loss: 0.8259
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8280 | Val Loss: 0.8301
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8299 | Val Loss: 0.8261
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.82763
wandb:   val_loss 0.83148
wandb: 
wandb: üöÄ View run twilight-sweep-860 at: https://wandb.ai/7shoe/domShift-extensive/runs/6glogktj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191634-6glogktj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8276 | Val Loss: 0.8315
2025-03-26 19:17:28,669 - wandb.wandb_agent - INFO - Cleaning up finished run: 6glogktj
2025-03-26 19:17:29,221 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:17:29,221 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:17:29,224 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 19:17:34,236 - wandb.wandb_agent - INFO - Running runs: ['i2znpxic']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191735-i2znpxic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-865
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/i2znpxic
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: i2znpxic
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9865 | Val Loss: 1.6659
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5340 | Val Loss: 1.3814
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3288 | Val Loss: 1.2733
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2149 | Val Loss: 1.1480
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1146 | Val Loss: 1.0778
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0620 | Val Loss: 1.0410
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0193 | Val Loss: 1.0271
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1392 | Val Loss: 1.2305
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2106 | Val Loss: 1.1031
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04212
wandb:   val_loss 1.0106
wandb: 
wandb: üöÄ View run zesty-sweep-865 at: https://wandb.ai/7shoe/domShift-extensive/runs/i2znpxic
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191735-i2znpxic/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0421 | Val Loss: 1.0106
2025-03-26 19:18:14,720 - wandb.wandb_agent - INFO - Cleaning up finished run: i2znpxic
2025-03-26 19:18:15,269 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:18:15,269 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:18:15,272 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:18:20,283 - wandb.wandb_agent - INFO - Running runs: ['u7cepk0r']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191820-u7cepk0r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-871
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u7cepk0r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u7cepk0r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5795 | Val Loss: 1.1738
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1498 | Val Loss: 1.1540
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1743 | Val Loss: 1.1951
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2218 | Val Loss: 1.2397
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2481 | Val Loss: 1.2595
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2634 | Val Loss: 1.2471
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1757 | Val Loss: 1.1044
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0858 | Val Loss: 1.0801
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0801 | Val Loss: 1.0797
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07958
wandb:   val_loss 1.07914
wandb: 
wandb: üöÄ View run classic-sweep-871 at: https://wandb.ai/7shoe/domShift-extensive/runs/u7cepk0r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191820-u7cepk0r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0796 | Val Loss: 1.0791
2025-03-26 19:19:05,945 - wandb.wandb_agent - INFO - Cleaning up finished run: u7cepk0r
2025-03-26 19:19:06,515 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:06,515 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:19:06,518 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 19:19:11,525 - wandb.wandb_agent - INFO - Running runs: ['h4gn1m45']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191911-h4gn1m45
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-877
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/h4gn1m45
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: h4gn1m45
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6951 | Val Loss: 1.3694
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3203 | Val Loss: 1.2891
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3232 | Val Loss: 1.3780
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4179 | Val Loss: 1.4447
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4288 | Val Loss: 1.4113
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3306 | Val Loss: 1.3112
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3146 | Val Loss: 1.3098
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3054 | Val Loss: 1.2983
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2892 | Val Loss: 1.2788
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27733
wandb:   val_loss 1.27081
wandb: 
wandb: üöÄ View run sunny-sweep-877 at: https://wandb.ai/7shoe/domShift-extensive/runs/h4gn1m45
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191911-h4gn1m45/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2773 | Val Loss: 1.2708
2025-03-26 19:19:52,104 - wandb.wandb_agent - INFO - Cleaning up finished run: h4gn1m45
2025-03-26 19:19:52,642 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:52,642 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:19:52,645 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:19:57,657 - wandb.wandb_agent - INFO - Running runs: ['y0qdnxri']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191957-y0qdnxri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-882
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y0qdnxri
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: y0qdnxri
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1846 | Val Loss: 1.0682
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1308 | Val Loss: 1.1699
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1811 | Val Loss: 1.1934
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2370 | Val Loss: 1.2099
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2237 | Val Loss: 1.2440
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2435 | Val Loss: 1.2398
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1618 | Val Loss: 1.0740
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1035 | Val Loss: 1.0990
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0972 | Val Loss: 1.0988
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñá‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09753
wandb:   val_loss 1.09834
wandb: 
wandb: üöÄ View run light-sweep-882 at: https://wandb.ai/7shoe/domShift-extensive/runs/y0qdnxri
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191957-y0qdnxri/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0975 | Val Loss: 1.0983
2025-03-26 19:20:38,665 - wandb.wandb_agent - INFO - Cleaning up finished run: y0qdnxri
2025-03-26 19:20:39,237 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:20:39,238 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:20:39,240 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 19:20:44,253 - wandb.wandb_agent - INFO - Running runs: ['fltnbm8d']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_192045-fltnbm8d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-887
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fltnbm8d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fltnbm8d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6362 | Val Loss: 1.5749
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5618 | Val Loss: 1.5649
