nohup: ignoring input
wandb: Starting wandb agent üïµÔ∏è
2025-03-26 17:04:41,664 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 17:04:41,988 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:04:41,989 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:04:41,991 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 17:04:47,004 - wandb.wandb_agent - INFO - Running runs: ['bpazs8db']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170447-bpazs8db
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bpazs8db
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bpazs8db
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.1578 | Val Loss: 3.0856
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.7518 | Val Loss: 2.2905
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.1389 | Val Loss: 2.0017
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.8032 | Val Loss: 1.6685
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5925 | Val Loss: 1.5587
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4624 | Val Loss: 1.4219
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3727 | Val Loss: 1.3540
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3152 | Val Loss: 1.3178
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2782 | Val Loss: 1.2830
wandb: - 10.263 MB of 10.263 MB uploadedwandb: \ 10.263 MB of 10.282 MB uploadedwandb: | 10.263 MB of 10.282 MB uploadedwandb: / 10.282 MB of 10.282 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23333
wandb:   val_loss 1.24356
wandb: 
wandb: üöÄ View run happy-sweep-5 at: https://wandb.ai/7shoe/domShift-extensive/runs/bpazs8db
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170447-bpazs8db/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2333 | Val Loss: 1.2436
2025-03-26 17:05:17,423 - wandb.wandb_agent - INFO - Cleaning up finished run: bpazs8db
2025-03-26 17:05:18,016 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:18,016 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:05:18,018 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 17:05:23,031 - wandb.wandb_agent - INFO - Running runs: ['2c7x5umm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170523-2c7x5umm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2c7x5umm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2c7x5umm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7641 | Val Loss: 2.2467
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1778 | Val Loss: 2.1647
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.1475 | Val Loss: 2.0714
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0666 | Val Loss: 2.0593
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0674 | Val Loss: 2.0818
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1089 | Val Loss: 2.1448
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1854 | Val Loss: 2.2328
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2736 | Val Loss: 2.3167
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3515 | Val Loss: 2.3862
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.4129
wandb:   val_loss 2.4396
wandb: 
wandb: üöÄ View run fragrant-sweep-10 at: https://wandb.ai/7shoe/domShift-extensive/runs/2c7x5umm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170523-2c7x5umm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4129 | Val Loss: 2.4396
2025-03-26 17:05:58,522 - wandb.wandb_agent - INFO - Cleaning up finished run: 2c7x5umm
2025-03-26 17:05:58,979 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:58,979 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:05:58,982 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:06:03,994 - wandb.wandb_agent - INFO - Running runs: ['a55xgje7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170605-a55xgje7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/a55xgje7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: a55xgje7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.8300 | Val Loss: 1.6665
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5927 | Val Loss: 1.5247
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5139 | Val Loss: 1.4984
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4737 | Val Loss: 1.4379
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4216 | Val Loss: 1.4134
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4159 | Val Loss: 1.4174
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4156 | Val Loss: 1.4115
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4106 | Val Loss: 1.4088
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4086 | Val Loss: 1.4082
wandb: - 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40552
wandb:   val_loss 1.4011
wandb: 
wandb: üöÄ View run silvery-sweep-17 at: https://wandb.ai/7shoe/domShift-extensive/runs/a55xgje7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170605-a55xgje7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4055 | Val Loss: 1.4011
2025-03-26 17:06:39,494 - wandb.wandb_agent - INFO - Cleaning up finished run: a55xgje7
2025-03-26 17:06:40,203 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:06:40,203 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:06:40,205 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:06:45,217 - wandb.wandb_agent - INFO - Running runs: ['ukexat5e']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170645-ukexat5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ukexat5e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ukexat5e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.5363 | Val Loss: 3.6879
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.3751 | Val Loss: 3.0152
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.8881 | Val Loss: 2.6492
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.6003 | Val Loss: 2.4283
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.3664 | Val Loss: 2.2773
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2038 | Val Loss: 2.0725
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0223 | Val Loss: 1.9341
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.8604 | Val Loss: 1.7509
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6926 | Val Loss: 1.5823
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.312 MB uploadedwandb: | 137.332 MB of 137.332 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.57766
wandb:   val_loss 1.45701
wandb: 
wandb: üöÄ View run expert-sweep-23 at: https://wandb.ai/7shoe/domShift-extensive/runs/ukexat5e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170645-ukexat5e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5777 | Val Loss: 1.4570
2025-03-26 17:07:15,639 - wandb.wandb_agent - INFO - Cleaning up finished run: ukexat5e
2025-03-26 17:07:16,305 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:16,305 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:07:16,308 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 17:07:21,320 - wandb.wandb_agent - INFO - Running runs: ['8xjz5856']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170722-8xjz5856
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8xjz5856
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 8xjz5856
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.0036 | Val Loss: 2.3581
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.0488 | Val Loss: 1.8120
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.7543 | Val Loss: 1.7165
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.7195 | Val Loss: 1.7222
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.7293 | Val Loss: 1.7341
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.7446 | Val Loss: 1.7551
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.7695 | Val Loss: 1.7833
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.7974 | Val Loss: 1.8107
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.8235 | Val Loss: 1.8356
wandb: - 32.851 MB of 32.851 MB uploadedwandb: \ 32.851 MB of 32.871 MB uploadedwandb: | 32.851 MB of 32.871 MB uploadedwandb: / 32.851 MB of 32.871 MB uploadedwandb: - 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.8471
wandb:   val_loss 1.85792
wandb: 
wandb: üöÄ View run super-sweep-29 at: https://wandb.ai/7shoe/domShift-extensive/runs/8xjz5856
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170722-8xjz5856/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.8471 | Val Loss: 1.8579
2025-03-26 17:07:51,767 - wandb.wandb_agent - INFO - Cleaning up finished run: 8xjz5856
2025-03-26 17:07:52,236 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:52,236 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:07:52,238 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:07:57,251 - wandb.wandb_agent - INFO - Running runs: ['xfyricu1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170758-xfyricu1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xfyricu1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: xfyricu1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4946 | Val Loss: 1.3991
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3741 | Val Loss: 1.3342
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3402 | Val Loss: 1.3045
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2584 | Val Loss: 1.2476
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2598 | Val Loss: 1.2611
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2644 | Val Loss: 1.2618
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2574 | Val Loss: 1.2486
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2613 | Val Loss: 1.2779
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2716 | Val Loss: 1.2551
wandb: - 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.24412
wandb:   val_loss 1.23613
wandb: 
wandb: üöÄ View run frosty-sweep-31 at: https://wandb.ai/7shoe/domShift-extensive/runs/xfyricu1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170758-xfyricu1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2441 | Val Loss: 1.2361
2025-03-26 17:08:32,754 - wandb.wandb_agent - INFO - Cleaning up finished run: xfyricu1
2025-03-26 17:08:33,219 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:08:33,219 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:08:33,222 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:08:38,234 - wandb.wandb_agent - INFO - Running runs: ['y0ish4k4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170839-y0ish4k4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y0ish4k4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: y0ish4k4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 3.5076 | Val Loss: 2.9107
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.8129 | Val Loss: 2.6904
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.6076 | Val Loss: 2.4896
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.4351 | Val Loss: 2.3860
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 2.3908 | Val Loss: 2.3863
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 2.3736 | Val Loss: 2.3406
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 2.3204 | Val Loss: 2.2928
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.2730 | Val Loss: 2.2488
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.2356 | Val Loss: 2.2224
wandb: - 32.868 MB of 32.868 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.21823
wandb:   val_loss 2.21316
wandb: 
wandb: üöÄ View run avid-sweep-38 at: https://wandb.ai/7shoe/domShift-extensive/runs/y0ish4k4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170839-y0ish4k4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.2182 | Val Loss: 2.2132
2025-03-26 17:09:08,696 - wandb.wandb_agent - INFO - Cleaning up finished run: y0ish4k4
2025-03-26 17:09:09,526 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:09,526 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:09:09,529 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 17:09:14,540 - wandb.wandb_agent - INFO - Running runs: ['ruaq06nd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170914-ruaq06nd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ruaq06nd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ruaq06nd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4254 | Val Loss: 1.2735
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2056 | Val Loss: 1.1341
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0926 | Val Loss: 1.0593
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0371 | Val Loss: 1.0036
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0000 | Val Loss: 1.0176
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0452 | Val Loss: 1.0836
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1184 | Val Loss: 1.1668
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2125 | Val Loss: 1.2620
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2744 | Val Loss: 1.2839
wandb: - 10.265 MB of 10.265 MB uploadedwandb: \ 10.265 MB of 10.285 MB uploadedwandb: | 10.265 MB of 10.285 MB uploadedwandb: / 10.285 MB of 10.285 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ
wandb:   val_loss ‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.292
wandb:   val_loss 1.30462
wandb: 
wandb: üöÄ View run radiant-sweep-43 at: https://wandb.ai/7shoe/domShift-extensive/runs/ruaq06nd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170914-ruaq06nd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2920 | Val Loss: 1.3046
2025-03-26 17:09:44,971 - wandb.wandb_agent - INFO - Cleaning up finished run: ruaq06nd
2025-03-26 17:09:45,868 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:45,869 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:09:45,871 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:09:50,883 - wandb.wandb_agent - INFO - Running runs: ['pcym9r58']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170950-pcym9r58
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-47
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pcym9r58
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pcym9r58
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5569 | Val Loss: 1.4602
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4402 | Val Loss: 1.4414
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3105 | Val Loss: 1.2051
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2232 | Val Loss: 1.2274
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2664 | Val Loss: 1.3319
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3240 | Val Loss: 1.3082
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3004 | Val Loss: 1.2935
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2916 | Val Loss: 1.2921
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2970 | Val Loss: 1.3037
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñá‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32513
wandb:   val_loss 1.35833
wandb: 
wandb: üöÄ View run effortless-sweep-47 at: https://wandb.ai/7shoe/domShift-extensive/runs/pcym9r58
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170950-pcym9r58/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3251 | Val Loss: 1.3583
2025-03-26 17:10:41,578 - wandb.wandb_agent - INFO - Cleaning up finished run: pcym9r58
2025-03-26 17:10:52,462 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:10:52,462 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:10:52,465 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:10:57,476 - wandb.wandb_agent - INFO - Running runs: ['5683lp1c']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171058-5683lp1c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5683lp1c
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 5683lp1c
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1861 | Val Loss: 1.1386
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1779 | Val Loss: 1.2190
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2423 | Val Loss: 1.3023
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3710 | Val Loss: 1.3973
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3188 | Val Loss: 1.2396
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1524 | Val Loss: 1.0872
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0760 | Val Loss: 1.1023
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1174 | Val Loss: 1.1364
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1378 | Val Loss: 1.1493
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15052
wandb:   val_loss 1.16197
wandb: 
wandb: üöÄ View run wandering-sweep-55 at: https://wandb.ai/7shoe/domShift-extensive/runs/5683lp1c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171058-5683lp1c/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1505 | Val Loss: 1.1620
2025-03-26 17:11:32,984 - wandb.wandb_agent - INFO - Cleaning up finished run: 5683lp1c
2025-03-26 17:11:33,468 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:11:33,468 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:11:33,470 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:11:38,482 - wandb.wandb_agent - INFO - Running runs: ['oiungnmk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171139-oiungnmk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/oiungnmk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: oiungnmk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5241 | Val Loss: 1.1856
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1306 | Val Loss: 1.1146
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1272 | Val Loss: 1.1496
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1375 | Val Loss: 1.1313
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1247 | Val Loss: 1.0910
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0259 | Val Loss: 0.9411
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9782 | Val Loss: 0.9998
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0246 | Val Loss: 1.0441
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0486 | Val Loss: 1.0501
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04969
wandb:   val_loss 1.04895
wandb: 
wandb: üöÄ View run vivid-sweep-59 at: https://wandb.ai/7shoe/domShift-extensive/runs/oiungnmk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171139-oiungnmk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0497 | Val Loss: 1.0490
2025-03-26 17:12:49,508 - wandb.wandb_agent - INFO - Cleaning up finished run: oiungnmk
2025-03-26 17:12:50,169 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:50,169 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:12:50,172 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:12:55,186 - wandb.wandb_agent - INFO - Running runs: ['7tf4tflr']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171255-7tf4tflr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7tf4tflr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7tf4tflr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.5416 | Val Loss: 3.7205
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.5430 | Val Loss: 3.1619
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9700 | Val Loss: 2.6764
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.6247 | Val Loss: 2.4181
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.2845 | Val Loss: 2.1051
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0340 | Val Loss: 1.8951
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8514 | Val Loss: 1.7356
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6970 | Val Loss: 1.6304
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6354 | Val Loss: 1.5358
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.312 MB uploadedwandb: | 137.332 MB of 137.332 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.50743
wandb:   val_loss 1.46626
wandb: 
wandb: üöÄ View run amber-sweep-70 at: https://wandb.ai/7shoe/domShift-extensive/runs/7tf4tflr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171255-7tf4tflr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5074 | Val Loss: 1.4663
2025-03-26 17:13:25,694 - wandb.wandb_agent - INFO - Cleaning up finished run: 7tf4tflr
2025-03-26 17:13:26,233 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:26,233 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:13:26,236 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:13:31,247 - wandb.wandb_agent - INFO - Running runs: ['842l2d9w']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171332-842l2d9w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-74
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/842l2d9w
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 842l2d9w
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.9527 | Val Loss: 3.7656
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.7052 | Val Loss: 3.6435
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 3.5879 | Val Loss: 3.5015
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 3.4294 | Val Loss: 3.3273
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 3.2684 | Val Loss: 3.2188
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 3.1971 | Val Loss: 3.1687
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 3.1470 | Val Loss: 3.1330
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 3.1364 | Val Loss: 3.1412
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 3.1400 | Val Loss: 3.1350
wandb: - 32.851 MB of 32.871 MB uploadedwandb: \ 32.851 MB of 32.871 MB uploadedwandb: | 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.12209
wandb:   val_loss 3.10676
wandb: 
wandb: üöÄ View run peach-sweep-74 at: https://wandb.ai/7shoe/domShift-extensive/runs/842l2d9w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171332-842l2d9w/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 3.1221 | Val Loss: 3.1068
2025-03-26 17:13:56,616 - wandb.wandb_agent - INFO - Cleaning up finished run: 842l2d9w
2025-03-26 17:13:57,131 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:57,132 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:13:57,134 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:14:02,146 - wandb.wandb_agent - INFO - Running runs: ['g89pkxqd']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171402-g89pkxqd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/g89pkxqd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: g89pkxqd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.1136 | Val Loss: 1.9377
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.9474 | Val Loss: 1.8811
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.8236 | Val Loss: 1.6861
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6461 | Val Loss: 1.5757
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5598 | Val Loss: 1.5144
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5060 | Val Loss: 1.4756
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4676 | Val Loss: 1.4331
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4197 | Val Loss: 1.3845
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3708 | Val Loss: 1.3352
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.876 MB uploadedwandb: | 32.856 MB of 32.876 MB uploadedwandb: / 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31758
wandb:   val_loss 1.27953
wandb: 
wandb: üöÄ View run unique-sweep-80 at: https://wandb.ai/7shoe/domShift-extensive/runs/g89pkxqd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171402-g89pkxqd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3176 | Val Loss: 1.2795
2025-03-26 17:14:32,558 - wandb.wandb_agent - INFO - Cleaning up finished run: g89pkxqd
2025-03-26 17:14:33,301 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:14:33,301 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:14:33,304 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:14:38,318 - wandb.wandb_agent - INFO - Running runs: ['dtguec0g']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171438-dtguec0g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-84
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dtguec0g
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dtguec0g
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.1798 | Val Loss: 1.9668
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.8782 | Val Loss: 1.7733
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7039 | Val Loss: 1.6042
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5193 | Val Loss: 1.4049
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3428 | Val Loss: 1.2593
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2240 | Val Loss: 1.1766
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1517 | Val Loss: 1.1216
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1119 | Val Loss: 1.0977
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0922 | Val Loss: 1.0789
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06239
wandb:   val_loss 1.02855
wandb: 
wandb: üöÄ View run eager-sweep-84 at: https://wandb.ai/7shoe/domShift-extensive/runs/dtguec0g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171438-dtguec0g/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0624 | Val Loss: 1.0286
2025-03-26 17:15:08,751 - wandb.wandb_agent - INFO - Cleaning up finished run: dtguec0g
2025-03-26 17:15:09,271 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:15:09,271 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:15:09,273 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:15:14,285 - wandb.wandb_agent - INFO - Running runs: ['ihr9s271']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171515-ihr9s271
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-89
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ihr9s271
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ihr9s271
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4411 | Val Loss: 1.4498
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4752 | Val Loss: 1.4773
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4465 | Val Loss: 1.4202
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4329 | Val Loss: 1.4133
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4130 | Val Loss: 1.4354
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4770 | Val Loss: 1.5208
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4579 | Val Loss: 1.4292
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4971 | Val Loss: 1.5771
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5980 | Val Loss: 1.5594
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÖ
wandb:   val_loss ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñà‚ñá‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.50701
wandb:   val_loss 1.47789
wandb: 
wandb: üöÄ View run peach-sweep-89 at: https://wandb.ai/7shoe/domShift-extensive/runs/ihr9s271
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171515-ihr9s271/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5070 | Val Loss: 1.4779
2025-03-26 17:16:10,118 - wandb.wandb_agent - INFO - Cleaning up finished run: ihr9s271
2025-03-26 17:16:10,614 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:10,614 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:16:10,617 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:16:15,630 - wandb.wandb_agent - INFO - Running runs: ['yw27nz7b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171615-yw27nz7b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-96
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yw27nz7b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: yw27nz7b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7529 | Val Loss: 1.5628
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4491 | Val Loss: 1.3606
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3409 | Val Loss: 1.3420
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3626 | Val Loss: 1.4062
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4154 | Val Loss: 1.3862
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3608 | Val Loss: 1.3669
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3502 | Val Loss: 1.3678
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4038 | Val Loss: 1.4177
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3801 | Val Loss: 1.3519
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.876 MB uploadedwandb: | 32.856 MB of 32.876 MB uploadedwandb: / 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.35153
wandb:   val_loss 1.36666
wandb: 
wandb: üöÄ View run tough-sweep-96 at: https://wandb.ai/7shoe/domShift-extensive/runs/yw27nz7b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171615-yw27nz7b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3515 | Val Loss: 1.3667
2025-03-26 17:16:46,024 - wandb.wandb_agent - INFO - Cleaning up finished run: yw27nz7b
2025-03-26 17:16:47,432 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:47,432 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:16:47,434 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:16:52,446 - wandb.wandb_agent - INFO - Running runs: ['cutrw8ck']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171652-cutrw8ck
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-101
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cutrw8ck
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cutrw8ck
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 0.9997 | Val Loss: 0.7959
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.8214 | Val Loss: 0.8261
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8115 | Val Loss: 0.8138
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8289 | Val Loss: 0.8396
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8344 | Val Loss: 0.8271
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8282 | Val Loss: 0.8297
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8297 | Val Loss: 0.8283
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8280 | Val Loss: 0.8289
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8295 | Val Loss: 0.8285
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.82867
wandb:   val_loss 0.8294
wandb: 
wandb: üöÄ View run cool-sweep-101 at: https://wandb.ai/7shoe/domShift-extensive/runs/cutrw8ck
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171652-cutrw8ck/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8287 | Val Loss: 0.8294
2025-03-26 17:17:22,893 - wandb.wandb_agent - INFO - Cleaning up finished run: cutrw8ck
2025-03-26 17:17:23,372 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:17:23,372 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:17:23,376 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:17:28,389 - wandb.wandb_agent - INFO - Running runs: ['dtkm6v48']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171728-dtkm6v48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-106
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dtkm6v48
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dtkm6v48
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4403 | Val Loss: 1.3081
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2118 | Val Loss: 1.1945
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1785 | Val Loss: 1.1839
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1388 | Val Loss: 1.0649
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9681 | Val Loss: 0.8910
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8864 | Val Loss: 0.8764
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8740 | Val Loss: 0.8709
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8679 | Val Loss: 0.8642
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8613 | Val Loss: 0.8580
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.8568
wandb:   val_loss 0.85565
wandb: 
wandb: üöÄ View run fresh-sweep-106 at: https://wandb.ai/7shoe/domShift-extensive/runs/dtkm6v48
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171728-dtkm6v48/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8568 | Val Loss: 0.8556
2025-03-26 17:18:03,888 - wandb.wandb_agent - INFO - Cleaning up finished run: dtkm6v48
2025-03-26 17:18:04,466 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:18:04,466 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:18:04,469 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:18:09,481 - wandb.wandb_agent - INFO - Running runs: ['vyck69vg']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171809-vyck69vg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-110
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vyck69vg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vyck69vg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6401 | Val Loss: 1.5055
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4403 | Val Loss: 1.4047
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4280 | Val Loss: 1.4563
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4430 | Val Loss: 1.4291
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3670 | Val Loss: 1.3274
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2983 | Val Loss: 1.2202
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1748 | Val Loss: 1.1644
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1678 | Val Loss: 1.1751
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1612 | Val Loss: 1.1176
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09295
wandb:   val_loss 1.0986
wandb: 
wandb: üöÄ View run jolly-sweep-110 at: https://wandb.ai/7shoe/domShift-extensive/runs/vyck69vg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171809-vyck69vg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0929 | Val Loss: 1.0986
2025-03-26 17:18:50,060 - wandb.wandb_agent - INFO - Cleaning up finished run: vyck69vg
2025-03-26 17:18:50,630 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:18:50,630 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:18:50,633 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:18:55,645 - wandb.wandb_agent - INFO - Running runs: ['hbb73212']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171855-hbb73212
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-115
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hbb73212
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: hbb73212
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4618 | Val Loss: 1.3919
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3828 | Val Loss: 1.3033
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1539 | Val Loss: 1.0010
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0026 | Val Loss: 1.0193
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0080 | Val Loss: 0.9729
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0427 | Val Loss: 1.2247
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4147 | Val Loss: 1.4837
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3545 | Val Loss: 1.2380
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1159 | Val Loss: 1.0194
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñá‚ñÜ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0252
wandb:   val_loss 1.04837
wandb: 
wandb: üöÄ View run azure-sweep-115 at: https://wandb.ai/7shoe/domShift-extensive/runs/hbb73212
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171855-hbb73212/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0252 | Val Loss: 1.0484
2025-03-26 17:19:26,095 - wandb.wandb_agent - INFO - Cleaning up finished run: hbb73212
2025-03-26 17:19:26,630 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:19:26,630 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:19:26,634 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:19:31,645 - wandb.wandb_agent - INFO - Running runs: ['7je299if']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171933-7je299if
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-120
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7je299if
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7je299if
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6541 | Val Loss: 1.4256
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3520 | Val Loss: 1.2814
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2388 | Val Loss: 1.1825
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1151 | Val Loss: 1.0030
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9168 | Val Loss: 0.8427
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.8164 | Val Loss: 0.7957
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.7907 | Val Loss: 0.7841
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.7791 | Val Loss: 0.7704
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.7622 | Val Loss: 0.7495
wandb: - 32.853 MB of 32.873 MB uploadedwandb: \ 32.853 MB of 32.873 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.73983
wandb:   val_loss 0.72831
wandb: 
wandb: üöÄ View run feasible-sweep-120 at: https://wandb.ai/7shoe/domShift-extensive/runs/7je299if
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171933-7je299if/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.7398 | Val Loss: 0.7283
2025-03-26 17:20:02,041 - wandb.wandb_agent - INFO - Cleaning up finished run: 7je299if
2025-03-26 17:20:02,711 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:02,711 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:20:02,713 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:20:07,726 - wandb.wandb_agent - INFO - Running runs: ['p32opkld']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172009-p32opkld
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-124
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/p32opkld
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: p32opkld
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5066 | Val Loss: 1.5671
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4358 | Val Loss: 1.1739
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1637 | Val Loss: 1.1726
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1663 | Val Loss: 1.1644
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1627 | Val Loss: 1.1629
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1620 | Val Loss: 1.1627
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1620 | Val Loss: 1.1628
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1621 | Val Loss: 1.1628
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1621 | Val Loss: 1.1629
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.946 MB uploadedwandb: | 32.853 MB of 32.946 MB uploadedwandb: / 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1622
wandb:   val_loss 1.16296
wandb: 
wandb: üöÄ View run playful-sweep-124 at: https://wandb.ai/7shoe/domShift-extensive/runs/p32opkld
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172009-p32opkld/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1622 | Val Loss: 1.1630
2025-03-26 17:20:53,344 - wandb.wandb_agent - INFO - Cleaning up finished run: p32opkld
2025-03-26 17:20:54,799 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:54,799 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:20:54,801 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:20:59,814 - wandb.wandb_agent - INFO - Running runs: ['7ctqs118']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172101-7ctqs118
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-130
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7ctqs118
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7ctqs118
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.2900 | Val Loss: 3.2423
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.1445 | Val Loss: 3.0439
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.9894 | Val Loss: 2.9306
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.8538 | Val Loss: 2.7666
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.6875 | Val Loss: 2.5436
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.3798 | Val Loss: 2.2170
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.1543 | Val Loss: 2.0955
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.0685 | Val Loss: 2.0454
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.0380 | Val Loss: 2.0316
wandb: - 32.851 MB of 32.851 MB uploadedwandb: \ 32.851 MB of 32.871 MB uploadedwandb: | 32.851 MB of 32.871 MB uploadedwandb: / 32.871 MB of 32.871 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.03321
wandb:   val_loss 2.03406
wandb: 
wandb: üöÄ View run genial-sweep-130 at: https://wandb.ai/7shoe/domShift-extensive/runs/7ctqs118
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172101-7ctqs118/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.0332 | Val Loss: 2.0341
2025-03-26 17:21:35,307 - wandb.wandb_agent - INFO - Cleaning up finished run: 7ctqs118
2025-03-26 17:21:35,910 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:21:35,910 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:21:35,913 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:21:40,925 - wandb.wandb_agent - INFO - Running runs: ['yyzl2qk1']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172140-yyzl2qk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-133
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yyzl2qk1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yyzl2qk1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3068 | Val Loss: 1.1768
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2387 | Val Loss: 1.2206
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2177 | Val Loss: 1.2169
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1974 | Val Loss: 1.2041
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1974 | Val Loss: 1.1579
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1726 | Val Loss: 1.1638
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1723 | Val Loss: 1.1712
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1555 | Val Loss: 1.1349
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1264 | Val Loss: 1.1277
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.014 MB of 33.014 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñÖ‚ñà‚ñà‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14264
wandb:   val_loss 1.15918
wandb: 
wandb: üöÄ View run devoted-sweep-133 at: https://wandb.ai/7shoe/domShift-extensive/runs/yyzl2qk1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172140-yyzl2qk1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1426 | Val Loss: 1.1592
2025-03-26 17:22:21,463 - wandb.wandb_agent - INFO - Cleaning up finished run: yyzl2qk1
2025-03-26 17:22:21,962 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:22:21,962 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:22:21,964 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:22:26,977 - wandb.wandb_agent - INFO - Running runs: ['0hksktts']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172226-0hksktts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-139
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0hksktts
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0hksktts
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4833 | Val Loss: 1.2833
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1645 | Val Loss: 1.1159
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0610 | Val Loss: 1.0228
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0088 | Val Loss: 1.0001
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9926 | Val Loss: 0.9856
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9817 | Val Loss: 0.9774
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9779 | Val Loss: 0.9789
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9799 | Val Loss: 0.9802
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9815 | Val Loss: 0.9719
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93285
wandb:   val_loss 0.91765
wandb: 
wandb: üöÄ View run celestial-sweep-139 at: https://wandb.ai/7shoe/domShift-extensive/runs/0hksktts
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172226-0hksktts/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9328 | Val Loss: 0.9177
2025-03-26 17:23:17,722 - wandb.wandb_agent - INFO - Cleaning up finished run: 0hksktts
2025-03-26 17:23:18,407 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:23:18,407 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:23:18,410 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:23:23,422 - wandb.wandb_agent - INFO - Running runs: ['vhou6cth']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172324-vhou6cth
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-146
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vhou6cth
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vhou6cth
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3338 | Val Loss: 1.0581
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9342 | Val Loss: 0.8135
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8015 | Val Loss: 0.8054
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8116 | Val Loss: 0.8176
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8255 | Val Loss: 0.8327
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8508 | Val Loss: 0.8184
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8552 | Val Loss: 0.8885
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9051 | Val Loss: 0.9101
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9150 | Val Loss: 0.9198
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92272
wandb:   val_loss 0.9259
wandb: 
wandb: üöÄ View run dark-sweep-146 at: https://wandb.ai/7shoe/domShift-extensive/runs/vhou6cth
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172324-vhou6cth/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9227 | Val Loss: 0.9259
2025-03-26 17:24:19,201 - wandb.wandb_agent - INFO - Cleaning up finished run: vhou6cth
2025-03-26 17:24:19,732 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:19,732 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:24:19,735 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:24:24,747 - wandb.wandb_agent - INFO - Running runs: ['0amvzjmq']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172424-0amvzjmq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-152
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0amvzjmq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0amvzjmq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9397 | Val Loss: 2.3656
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.2150 | Val Loss: 2.1923
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.2288 | Val Loss: 2.2669
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.3122 | Val Loss: 2.3620
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.4124 | Val Loss: 2.4627
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.4997 | Val Loss: 2.5310
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.5496 | Val Loss: 2.5640
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.5690 | Val Loss: 2.5706
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.5648 | Val Loss: 2.5557
wandb: - 32.969 MB of 32.969 MB uploadedwandb: \ 32.969 MB of 33.062 MB uploadedwandb: | 32.969 MB of 33.062 MB uploadedwandb: / 33.062 MB of 33.062 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.53879
wandb:   val_loss 2.52192
wandb: 
wandb: üöÄ View run misunderstood-sweep-152 at: https://wandb.ai/7shoe/domShift-extensive/runs/0amvzjmq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172424-0amvzjmq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.5388 | Val Loss: 2.5219
2025-03-26 17:25:10,387 - wandb.wandb_agent - INFO - Cleaning up finished run: 0amvzjmq
2025-03-26 17:25:11,322 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:25:11,323 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:25:11,325 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:25:16,337 - wandb.wandb_agent - INFO - Running runs: ['pbpxr3lw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172517-pbpxr3lw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-159
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pbpxr3lw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: pbpxr3lw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9726 | Val Loss: 1.8194
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.7452 | Val Loss: 1.5216
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2768 | Val Loss: 1.2465
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2586 | Val Loss: 1.2456
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1976 | Val Loss: 1.2122
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2566 | Val Loss: 1.2941
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3003 | Val Loss: 1.3133
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3122 | Val Loss: 1.3037
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3130 | Val Loss: 1.3037
wandb: - 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3106
wandb:   val_loss 1.34058
wandb: 
wandb: üöÄ View run peachy-sweep-159 at: https://wandb.ai/7shoe/domShift-extensive/runs/pbpxr3lw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172517-pbpxr3lw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3106 | Val Loss: 1.3406
2025-03-26 17:25:51,788 - wandb.wandb_agent - INFO - Cleaning up finished run: pbpxr3lw
2025-03-26 17:25:52,435 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:25:52,436 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:25:52,438 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:25:57,451 - wandb.wandb_agent - INFO - Running runs: ['xnql7d52']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172557-xnql7d52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-164
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xnql7d52
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: xnql7d52
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.8204 | Val Loss: 1.7002
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6119 | Val Loss: 1.4833
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4878 | Val Loss: 1.4884
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4326 | Val Loss: 1.3609
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3508 | Val Loss: 1.3518
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3526 | Val Loss: 1.3489
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3474 | Val Loss: 1.3464
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3478 | Val Loss: 1.3481
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3507 | Val Loss: 1.3562
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37123
wandb:   val_loss 1.37452
wandb: 
wandb: üöÄ View run valiant-sweep-164 at: https://wandb.ai/7shoe/domShift-extensive/runs/xnql7d52
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172557-xnql7d52/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3712 | Val Loss: 1.3745
2025-03-26 17:26:32,943 - wandb.wandb_agent - INFO - Cleaning up finished run: xnql7d52
2025-03-26 17:26:33,460 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:26:33,460 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:26:33,464 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:26:38,477 - wandb.wandb_agent - INFO - Running runs: ['hueolysj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172638-hueolysj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-170
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hueolysj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hueolysj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4733 | Val Loss: 1.3555
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3428 | Val Loss: 1.3157
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2896 | Val Loss: 1.2715
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2225 | Val Loss: 1.1289
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0469 | Val Loss: 1.0207
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0307 | Val Loss: 1.0288
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0309 | Val Loss: 1.0325
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0333 | Val Loss: 1.0343
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0351 | Val Loss: 1.0359
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03641
wandb:   val_loss 1.0371
wandb: 
wandb: üöÄ View run different-sweep-170 at: https://wandb.ai/7shoe/domShift-extensive/runs/hueolysj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172638-hueolysj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0364 | Val Loss: 1.0371
2025-03-26 17:27:13,965 - wandb.wandb_agent - INFO - Cleaning up finished run: hueolysj
2025-03-26 17:27:14,558 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:27:14,558 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:27:14,561 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:27:19,574 - wandb.wandb_agent - INFO - Running runs: ['4khun6oj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172720-4khun6oj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-174
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4khun6oj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4khun6oj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2098 | Val Loss: 1.1322
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1120 | Val Loss: 1.1424
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2122 | Val Loss: 1.2746
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2329 | Val Loss: 1.1160
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0522 | Val Loss: 1.0344
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0214 | Val Loss: 1.0106
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0078 | Val Loss: 1.0097
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0509 | Val Loss: 1.0836
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1018 | Val Loss: 1.1144
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.946 MB of 32.946 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÑ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11258
wandb:   val_loss 1.1095
wandb: 
wandb: üöÄ View run young-sweep-174 at: https://wandb.ai/7shoe/domShift-extensive/runs/4khun6oj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172720-4khun6oj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1126 | Val Loss: 1.1095
2025-03-26 17:28:05,167 - wandb.wandb_agent - INFO - Cleaning up finished run: 4khun6oj
2025-03-26 17:28:05,976 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:28:05,976 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:28:05,979 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:28:10,990 - wandb.wandb_agent - INFO - Running runs: ['q4y5b4qw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172811-q4y5b4qw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-180
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/q4y5b4qw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: q4y5b4qw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3972 | Val Loss: 1.2395
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2399 | Val Loss: 1.2462
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2181 | Val Loss: 1.2296
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2255 | Val Loss: 1.2114
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2212 | Val Loss: 1.2207
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2194 | Val Loss: 1.2253
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2201 | Val Loss: 1.2236
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2208 | Val Loss: 1.2211
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2210 | Val Loss: 1.2198
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22107
wandb:   val_loss 1.21919
wandb: 
wandb: üöÄ View run soft-sweep-180 at: https://wandb.ai/7shoe/domShift-extensive/runs/q4y5b4qw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172811-q4y5b4qw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2211 | Val Loss: 1.2192
2025-03-26 17:29:11,880 - wandb.wandb_agent - INFO - Cleaning up finished run: q4y5b4qw
2025-03-26 17:29:12,419 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:29:12,419 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:29:12,421 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:29:17,433 - wandb.wandb_agent - INFO - Running runs: ['obg8kb6h']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172917-obg8kb6h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-189
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/obg8kb6h
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: obg8kb6h
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5196 | Val Loss: 1.3972
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4172 | Val Loss: 1.4618
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4457 | Val Loss: 1.4072
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3555 | Val Loss: 1.3114
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2900 | Val Loss: 1.2556
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2166 | Val Loss: 1.1728
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1429 | Val Loss: 1.1147
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1069 | Val Loss: 1.1003
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0974 | Val Loss: 1.0939
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09552
wandb:   val_loss 1.0965
wandb: 
wandb: üöÄ View run vibrant-sweep-189 at: https://wandb.ai/7shoe/domShift-extensive/runs/obg8kb6h
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172917-obg8kb6h/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0955 | Val Loss: 1.0965
2025-03-26 17:29:47,875 - wandb.wandb_agent - INFO - Cleaning up finished run: obg8kb6h
2025-03-26 17:29:49,210 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:29:49,210 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:29:49,212 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:29:54,225 - wandb.wandb_agent - INFO - Running runs: ['f6k41e7a']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172955-f6k41e7a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-191
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f6k41e7a
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: f6k41e7a
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4419 | Val Loss: 1.1180
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1369 | Val Loss: 1.1220
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1246 | Val Loss: 1.1423
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1277 | Val Loss: 1.1280
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1250 | Val Loss: 1.1241
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1250 | Val Loss: 1.1266
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1250 | Val Loss: 1.1248
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1248 | Val Loss: 1.1259
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1246 | Val Loss: 1.1250
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12462
wandb:   val_loss 1.12486
wandb: 
wandb: üöÄ View run tough-sweep-191 at: https://wandb.ai/7shoe/domShift-extensive/runs/f6k41e7a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172955-f6k41e7a/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1246 | Val Loss: 1.1249
2025-03-26 17:31:05,521 - wandb.wandb_agent - INFO - Cleaning up finished run: f6k41e7a
2025-03-26 17:31:06,070 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:31:06,070 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:31:06,072 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:31:11,084 - wandb.wandb_agent - INFO - Running runs: ['bfhwanix']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173110-bfhwanix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-199
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bfhwanix
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bfhwanix
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.0975 | Val Loss: 2.7441
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.4450 | Val Loss: 2.2883
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.2827 | Val Loss: 2.2845
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.2719 | Val Loss: 2.2702
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.2721 | Val Loss: 2.2733
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.2752 | Val Loss: 2.2783
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.2816 | Val Loss: 2.2873
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.2965 | Val Loss: 2.3076
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3192 | Val Loss: 2.3315
wandb: - 32.969 MB of 32.969 MB uploadedwandb: \ 32.969 MB of 32.969 MB uploadedwandb: | 33.062 MB of 33.062 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.34081
wandb:   val_loss 2.34967
wandb: 
wandb: üöÄ View run cool-sweep-199 at: https://wandb.ai/7shoe/domShift-extensive/runs/bfhwanix
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173110-bfhwanix/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.3408 | Val Loss: 2.3497
2025-03-26 17:32:01,745 - wandb.wandb_agent - INFO - Cleaning up finished run: bfhwanix
2025-03-26 17:32:02,299 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:32:02,299 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:32:02,303 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:32:07,315 - wandb.wandb_agent - INFO - Running runs: ['vp3j08q6']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173207-vp3j08q6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-205
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vp3j08q6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: vp3j08q6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2808 | Val Loss: 1.1681
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1981 | Val Loss: 1.2279
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2419 | Val Loss: 1.2579
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2726 | Val Loss: 1.2839
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2949 | Val Loss: 1.3057
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3130 | Val Loss: 1.3184
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3231 | Val Loss: 1.3304
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3398 | Val Loss: 1.3503
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3601 | Val Loss: 1.3672
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37277
wandb:   val_loss 1.38352
wandb: 
wandb: üöÄ View run silver-sweep-205 at: https://wandb.ai/7shoe/domShift-extensive/runs/vp3j08q6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173207-vp3j08q6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3728 | Val Loss: 1.3835
2025-03-26 17:33:08,164 - wandb.wandb_agent - INFO - Cleaning up finished run: vp3j08q6
2025-03-26 17:33:09,313 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:33:09,313 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:33:09,316 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:33:14,328 - wandb.wandb_agent - INFO - Running runs: ['7nypkirk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173314-7nypkirk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-212
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7nypkirk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7nypkirk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4429 | Val Loss: 1.1946
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1515 | Val Loss: 1.1615
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1669 | Val Loss: 1.1793
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1435 | Val Loss: 1.1625
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1695 | Val Loss: 1.1477
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1667 | Val Loss: 1.1872
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1837 | Val Loss: 1.1928
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1891 | Val Loss: 1.1774
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1778 | Val Loss: 1.1865
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÅ‚ñá‚ñà‚ñÖ‚ñá‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17224
wandb:   val_loss 1.1597
wandb: 
wandb: üöÄ View run soft-sweep-212 at: https://wandb.ai/7shoe/domShift-extensive/runs/7nypkirk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173314-7nypkirk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1722 | Val Loss: 1.1597
2025-03-26 17:34:05,086 - wandb.wandb_agent - INFO - Cleaning up finished run: 7nypkirk
2025-03-26 17:34:05,849 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:34:05,849 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:34:05,852 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 17:34:10,866 - wandb.wandb_agent - INFO - Running runs: ['5c0opi05']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173412-5c0opi05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-217
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5c0opi05
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5c0opi05
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5875 | Val Loss: 1.2667
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2847 | Val Loss: 1.3959
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4609 | Val Loss: 1.5234
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4504 | Val Loss: 1.3144
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1896 | Val Loss: 1.1202
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1557 | Val Loss: 1.2160
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2954 | Val Loss: 1.3755
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4056 | Val Loss: 1.4256
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4375 | Val Loss: 1.4471
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ
wandb:   val_loss ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4579
wandb:   val_loss 1.46464
wandb: 
wandb: üöÄ View run apricot-sweep-217 at: https://wandb.ai/7shoe/domShift-extensive/runs/5c0opi05
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173412-5c0opi05/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4579 | Val Loss: 1.4646
2025-03-26 17:35:16,779 - wandb.wandb_agent - INFO - Cleaning up finished run: 5c0opi05
2025-03-26 17:35:17,618 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:17,619 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:35:17,621 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:35:22,633 - wandb.wandb_agent - INFO - Running runs: ['336mww72']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173522-336mww72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-226
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/336mww72
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 336mww72
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2645 | Val Loss: 1.0174
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9890 | Val Loss: 0.9404
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9388 | Val Loss: 0.9239
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9337 | Val Loss: 0.9315
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9301 | Val Loss: 0.9390
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9328 | Val Loss: 0.9314
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9331 | Val Loss: 0.9300
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9322 | Val Loss: 0.9320
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9318 | Val Loss: 0.9332
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93176
wandb:   val_loss 0.93342
wandb: 
wandb: üöÄ View run exalted-sweep-226 at: https://wandb.ai/7shoe/domShift-extensive/runs/336mww72
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173522-336mww72/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9318 | Val Loss: 0.9334
2025-03-26 17:36:38,779 - wandb.wandb_agent - INFO - Cleaning up finished run: 336mww72
2025-03-26 17:36:39,715 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:36:39,715 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:36:39,718 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:36:44,730 - wandb.wandb_agent - INFO - Running runs: ['wf1m4n9h']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173644-wf1m4n9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-234
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wf1m4n9h
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: wf1m4n9h
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6319 | Val Loss: 1.4813
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6188 | Val Loss: 1.6632
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5171 | Val Loss: 1.4811
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5473 | Val Loss: 1.5257
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4454 | Val Loss: 1.5354
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5826 | Val Loss: 1.6414
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.6469 | Val Loss: 1.6089
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.6113 | Val Loss: 1.6196
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.6269 | Val Loss: 1.6278
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñá‚ñÉ‚ñÖ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñÅ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.62764
wandb:   val_loss 1.6283
wandb: 
wandb: üöÄ View run whole-sweep-234 at: https://wandb.ai/7shoe/domShift-extensive/runs/wf1m4n9h
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173644-wf1m4n9h/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.6276 | Val Loss: 1.6283
2025-03-26 17:37:30,440 - wandb.wandb_agent - INFO - Cleaning up finished run: wf1m4n9h
2025-03-26 17:37:30,952 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:37:30,952 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:37:30,955 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:37:35,967 - wandb.wandb_agent - INFO - Running runs: ['dwlov2ty']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173735-dwlov2ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-240
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dwlov2ty
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: dwlov2ty
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5842 | Val Loss: 1.3926
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2706 | Val Loss: 1.2037
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1734 | Val Loss: 1.1597
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1452 | Val Loss: 1.1270
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1216 | Val Loss: 1.1166
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1233 | Val Loss: 1.1370
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1470 | Val Loss: 1.1508
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1435 | Val Loss: 1.1373
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1426 | Val Loss: 1.1488
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15109
wandb:   val_loss 1.15036
wandb: 
wandb: üöÄ View run lyric-sweep-240 at: https://wandb.ai/7shoe/domShift-extensive/runs/dwlov2ty
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173735-dwlov2ty/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1511 | Val Loss: 1.1504
2025-03-26 17:38:21,629 - wandb.wandb_agent - INFO - Cleaning up finished run: dwlov2ty
2025-03-26 17:38:22,182 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:38:22,182 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:38:22,185 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:38:27,197 - wandb.wandb_agent - INFO - Running runs: ['9llxkrvh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173828-9llxkrvh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-245
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9llxkrvh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9llxkrvh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3048 | Val Loss: 1.3993
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3339 | Val Loss: 1.2640
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2949 | Val Loss: 1.3294
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3726 | Val Loss: 1.4159
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4563 | Val Loss: 1.4870
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4799 | Val Loss: 1.3554
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2189 | Val Loss: 1.1343
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0998 | Val Loss: 1.0705
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0598 | Val Loss: 1.0549
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05575
wandb:   val_loss 1.05743
wandb: 
wandb: üöÄ View run autumn-sweep-245 at: https://wandb.ai/7shoe/domShift-extensive/runs/9llxkrvh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173828-9llxkrvh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0557 | Val Loss: 1.0574
2025-03-26 17:39:07,715 - wandb.wandb_agent - INFO - Cleaning up finished run: 9llxkrvh
2025-03-26 17:39:08,251 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:08,252 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:39:08,255 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:39:13,267 - wandb.wandb_agent - INFO - Running runs: ['77gl5nfu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173914-77gl5nfu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-252
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/77gl5nfu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 77gl5nfu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4990 | Val Loss: 1.3431
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3115 | Val Loss: 1.2790
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2606 | Val Loss: 1.2357
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2240 | Val Loss: 1.2072
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1999 | Val Loss: 1.1876
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1819 | Val Loss: 1.1685
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1570 | Val Loss: 1.1389
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1313 | Val Loss: 1.1210
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1261 | Val Loss: 1.1367
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1562
wandb:   val_loss 1.17739
wandb: 
wandb: üöÄ View run fiery-sweep-252 at: https://wandb.ai/7shoe/domShift-extensive/runs/77gl5nfu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173914-77gl5nfu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1562 | Val Loss: 1.1774
2025-03-26 17:39:43,651 - wandb.wandb_agent - INFO - Cleaning up finished run: 77gl5nfu
2025-03-26 17:39:44,370 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:44,370 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:39:44,373 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:39:49,384 - wandb.wandb_agent - INFO - Running runs: ['efled2ed']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173949-efled2ed
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-255
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/efled2ed
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: efled2ed
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.7360 | Val Loss: 3.5269
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.1890 | Val Loss: 2.7790
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.6738 | Val Loss: 2.6824
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.7301 | Val Loss: 2.7650
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8038 | Val Loss: 2.8535
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.8588 | Val Loss: 2.8537
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.8564 | Val Loss: 2.8739
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.8918 | Val Loss: 2.9128
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.9233 | Val Loss: 2.9364
wandb: - 10.261 MB of 10.261 MB uploadedwandb: \ 10.261 MB of 10.261 MB uploadedwandb: | 10.281 MB of 10.281 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.94612
wandb:   val_loss 2.95754
wandb: 
wandb: üöÄ View run lunar-sweep-255 at: https://wandb.ai/7shoe/domShift-extensive/runs/efled2ed
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173949-efled2ed/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.9461 | Val Loss: 2.9575
2025-03-26 17:40:14,716 - wandb.wandb_agent - INFO - Cleaning up finished run: efled2ed
2025-03-26 17:40:15,323 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:40:15,323 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:40:15,326 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 17:40:20,338 - wandb.wandb_agent - INFO - Running runs: ['0ymp2th8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174020-0ymp2th8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-260
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0ymp2th8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 0ymp2th8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0414 | Val Loss: 0.9344
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9838 | Val Loss: 0.9744
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9878 | Val Loss: 1.0267
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0163 | Val Loss: 1.0187
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0256 | Val Loss: 1.0350
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0485 | Val Loss: 1.0610
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0719 | Val Loss: 1.0837
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0952 | Val Loss: 1.1088
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1007 | Val Loss: 1.0950
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11581
wandb:   val_loss 1.14674
wandb: 
wandb: üöÄ View run northern-sweep-260 at: https://wandb.ai/7shoe/domShift-extensive/runs/0ymp2th8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174020-0ymp2th8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1158 | Val Loss: 1.1467
2025-03-26 17:41:21,151 - wandb.wandb_agent - INFO - Cleaning up finished run: 0ymp2th8
2025-03-26 17:41:21,662 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:41:21,663 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:41:21,665 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:41:26,678 - wandb.wandb_agent - INFO - Running runs: ['ownuczr4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174128-ownuczr4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-267
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ownuczr4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ownuczr4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.4917 | Val Loss: 1.2369
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.1526 | Val Loss: 1.0808
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 0.9872 | Val Loss: 0.9001
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 0.8691 | Val Loss: 0.8566
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 0.8404 | Val Loss: 0.8051
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 0.7864 | Val Loss: 0.7704
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 0.7646 | Val Loss: 0.7603
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 0.7595 | Val Loss: 0.7578
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 0.7575 | Val Loss: 0.7562
wandb: - 32.850 MB of 32.850 MB uploadedwandb: \ 32.850 MB of 32.850 MB uploadedwandb: | 32.869 MB of 32.869 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.75616
wandb:   val_loss 0.75567
wandb: 
wandb: üöÄ View run royal-sweep-267 at: https://wandb.ai/7shoe/domShift-extensive/runs/ownuczr4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174128-ownuczr4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 0.7562 | Val Loss: 0.7557
2025-03-26 17:42:02,185 - wandb.wandb_agent - INFO - Cleaning up finished run: ownuczr4
2025-03-26 17:42:03,312 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:42:03,313 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:42:03,316 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:42:08,328 - wandb.wandb_agent - INFO - Running runs: ['f1z0vplh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174208-f1z0vplh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-272
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f1z0vplh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: f1z0vplh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5634 | Val Loss: 1.3741
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3387 | Val Loss: 1.3099
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3327 | Val Loss: 1.3641
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3709 | Val Loss: 1.3638
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3366 | Val Loss: 1.3043
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3381 | Val Loss: 1.3426
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3006 | Val Loss: 1.2717
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2662 | Val Loss: 1.2694
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2714 | Val Loss: 1.2719
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27266
wandb:   val_loss 1.27457
wandb: 
wandb: üöÄ View run pleasant-sweep-272 at: https://wandb.ai/7shoe/domShift-extensive/runs/f1z0vplh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174208-f1z0vplh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2727 | Val Loss: 1.2746
2025-03-26 17:42:53,950 - wandb.wandb_agent - INFO - Cleaning up finished run: f1z0vplh
2025-03-26 17:42:54,703 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:42:54,703 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:42:54,706 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 17:42:59,717 - wandb.wandb_agent - INFO - Running runs: ['gxzgkvq8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174300-gxzgkvq8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-277
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gxzgkvq8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gxzgkvq8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3531 | Val Loss: 1.0727
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1951 | Val Loss: 1.1627
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0665 | Val Loss: 1.0524
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0474 | Val Loss: 1.0482
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0471 | Val Loss: 1.0482
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0472 | Val Loss: 1.0482
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0471 | Val Loss: 1.0482
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0471 | Val Loss: 1.0481
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0471 | Val Loss: 1.0481
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04701
wandb:   val_loss 1.04803
wandb: 
wandb: üöÄ View run clean-sweep-277 at: https://wandb.ai/7shoe/domShift-extensive/runs/gxzgkvq8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174300-gxzgkvq8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0470 | Val Loss: 1.0480
2025-03-26 17:43:45,350 - wandb.wandb_agent - INFO - Cleaning up finished run: gxzgkvq8
2025-03-26 17:43:46,092 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:43:46,092 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:43:46,095 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:43:51,107 - wandb.wandb_agent - INFO - Running runs: ['4r6ryk9q']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174351-4r6ryk9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-283
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4r6ryk9q
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 4r6ryk9q
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9965 | Val Loss: 1.7703
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7494 | Val Loss: 1.7087
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5518 | Val Loss: 1.4270
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4462 | Val Loss: 1.4746
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4817 | Val Loss: 1.4570
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4122 | Val Loss: 1.3697
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3502 | Val Loss: 1.3274
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3024 | Val Loss: 1.2772
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2920 | Val Loss: 1.3084
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30545
wandb:   val_loss 1.29795
wandb: 
wandb: üöÄ View run cool-sweep-283 at: https://wandb.ai/7shoe/domShift-extensive/runs/4r6ryk9q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174351-4r6ryk9q/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3055 | Val Loss: 1.2979
2025-03-26 17:44:36,702 - wandb.wandb_agent - INFO - Cleaning up finished run: 4r6ryk9q
2025-03-26 17:44:37,518 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:44:37,518 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.8
2025-03-26 17:44:37,521 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.8
2025-03-26 17:44:42,533 - wandb.wandb_agent - INFO - Running runs: ['tfao5w9p']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174443-tfao5w9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-288
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tfao5w9p
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tfao5w9p
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 5.3398 | Val Loss: 5.1459
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 5.1498 | Val Loss: 5.0798
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 5.1088 | Val Loss: 5.0592
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 5.0923 | Val Loss: 5.0494
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 5.0824 | Val Loss: 5.0386
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 5.0796 | Val Loss: 5.0382
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 5.0743 | Val Loss: 5.0345
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 5.0727 | Val Loss: 5.0287
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 5.0691 | Val Loss: 5.0288
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 33.058 MB of 33.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.06501
wandb:   val_loss 5.02055
wandb: 
wandb: üöÄ View run leafy-sweep-288 at: https://wandb.ai/7shoe/domShift-extensive/runs/tfao5w9p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174443-tfao5w9p/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 5.0650 | Val Loss: 5.0206
2025-03-26 17:45:33,229 - wandb.wandb_agent - INFO - Cleaning up finished run: tfao5w9p
2025-03-26 17:45:34,268 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:45:34,268 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:45:34,271 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:45:39,284 - wandb.wandb_agent - INFO - Running runs: ['drzej2io']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174539-drzej2io
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-295
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/drzej2io
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: drzej2io
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3244 | Val Loss: 1.3524
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3138 | Val Loss: 1.2920
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1603 | Val Loss: 1.0397
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0387 | Val Loss: 1.0495
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0483 | Val Loss: 1.0453
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0422 | Val Loss: 1.0409
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0394 | Val Loss: 1.0391
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0380 | Val Loss: 1.0381
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0373 | Val Loss: 1.0375
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03684
wandb:   val_loss 1.0372
wandb: 
wandb: üöÄ View run firm-sweep-295 at: https://wandb.ai/7shoe/domShift-extensive/runs/drzej2io
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174539-drzej2io/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0368 | Val Loss: 1.0372
2025-03-26 17:46:30,018 - wandb.wandb_agent - INFO - Cleaning up finished run: drzej2io
2025-03-26 17:46:30,912 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:46:30,912 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:46:30,915 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:46:35,927 - wandb.wandb_agent - INFO - Running runs: ['tfcoo7i2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174635-tfcoo7i2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-302
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tfcoo7i2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tfcoo7i2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6826 | Val Loss: 1.4277
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3409 | Val Loss: 1.2531
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1990 | Val Loss: 1.1413
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1158 | Val Loss: 1.0907
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0861 | Val Loss: 1.0839
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0810 | Val Loss: 1.0823
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0821 | Val Loss: 1.0817
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0786 | Val Loss: 1.0770
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0758 | Val Loss: 1.0770
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07605
wandb:   val_loss 1.07607
wandb: 
wandb: üöÄ View run sandy-sweep-302 at: https://wandb.ai/7shoe/domShift-extensive/runs/tfcoo7i2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174635-tfcoo7i2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0761 | Val Loss: 1.0761
2025-03-26 17:47:16,484 - wandb.wandb_agent - INFO - Cleaning up finished run: tfcoo7i2
2025-03-26 17:47:17,074 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:47:17,074 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:47:17,077 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:47:22,090 - wandb.wandb_agent - INFO - Running runs: ['sa146gum']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174723-sa146gum
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-308
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sa146gum
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sa146gum
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7115 | Val Loss: 1.3133
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1844 | Val Loss: 1.1702
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2009 | Val Loss: 1.2137
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2164 | Val Loss: 1.2169
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2184 | Val Loss: 1.2198
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2209 | Val Loss: 1.2217
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2230 | Val Loss: 1.2243
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2266 | Val Loss: 1.2290
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2326 | Val Loss: 1.2373
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.24303
wandb:   val_loss 1.24959
wandb: 
wandb: üöÄ View run trim-sweep-308 at: https://wandb.ai/7shoe/domShift-extensive/runs/sa146gum
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174723-sa146gum/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2430 | Val Loss: 1.2496
2025-03-26 17:48:17,949 - wandb.wandb_agent - INFO - Cleaning up finished run: sa146gum
2025-03-26 17:48:18,437 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:48:18,437 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:48:18,440 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:48:23,452 - wandb.wandb_agent - INFO - Running runs: ['qw3tbw8s']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174823-qw3tbw8s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-314
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qw3tbw8s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qw3tbw8s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9535 | Val Loss: 1.7170
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6263 | Val Loss: 1.5633
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5172 | Val Loss: 1.4538
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4162 | Val Loss: 1.3789
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3638 | Val Loss: 1.3566
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3718 | Val Loss: 1.3903
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4114 | Val Loss: 1.4311
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4461 | Val Loss: 1.4579
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4603 | Val Loss: 1.4587
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.45719
wandb:   val_loss 1.45376
wandb: 
wandb: üöÄ View run exalted-sweep-314 at: https://wandb.ai/7shoe/domShift-extensive/runs/qw3tbw8s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174823-qw3tbw8s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4572 | Val Loss: 1.4538
2025-03-26 17:49:03,977 - wandb.wandb_agent - INFO - Cleaning up finished run: qw3tbw8s
2025-03-26 17:49:04,530 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:04,530 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:49:04,533 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:49:09,540 - wandb.wandb_agent - INFO - Running runs: ['ybj0gvst']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174909-ybj0gvst
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-318
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ybj0gvst
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ybj0gvst
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4331 | Val Loss: 1.3383
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3642 | Val Loss: 1.4045
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3913 | Val Loss: 1.3494
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3292 | Val Loss: 1.3125
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3116 | Val Loss: 1.3107
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3117 | Val Loss: 1.3122
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3137 | Val Loss: 1.3147
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3164 | Val Loss: 1.3176
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3190 | Val Loss: 1.3202
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32161
wandb:   val_loss 1.32289
wandb: 
wandb: üöÄ View run mild-sweep-318 at: https://wandb.ai/7shoe/domShift-extensive/runs/ybj0gvst
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174909-ybj0gvst/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3216 | Val Loss: 1.3229
2025-03-26 17:50:10,463 - wandb.wandb_agent - INFO - Cleaning up finished run: ybj0gvst
2025-03-26 17:50:11,143 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:50:11,143 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:50:11,146 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.5
2025-03-26 17:50:16,157 - wandb.wandb_agent - INFO - Running runs: ['zfeuio14']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175016-zfeuio14
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-326
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zfeuio14
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: zfeuio14
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6647 | Val Loss: 1.5677
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5358 | Val Loss: 1.5350
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4776 | Val Loss: 1.4105
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4002 | Val Loss: 1.4006
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4103 | Val Loss: 1.4186
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4273 | Val Loss: 1.4389
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4516 | Val Loss: 1.4630
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4729 | Val Loss: 1.4819
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4874 | Val Loss: 1.4915
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.014 MB uploadedwandb: | 32.995 MB of 33.014 MB uploadedwandb: / 33.014 MB of 33.014 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.49387
wandb:   val_loss 1.49582
wandb: 
wandb: üöÄ View run sparkling-sweep-326 at: https://wandb.ai/7shoe/domShift-extensive/runs/zfeuio14
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175016-zfeuio14/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4939 | Val Loss: 1.4958
2025-03-26 17:50:46,583 - wandb.wandb_agent - INFO - Cleaning up finished run: zfeuio14
2025-03-26 17:50:47,207 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:50:47,207 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:50:47,210 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:50:52,222 - wandb.wandb_agent - INFO - Running runs: ['gcmzagm5']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175053-gcmzagm5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-332
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gcmzagm5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gcmzagm5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.3574 | Val Loss: 3.0472
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.9939 | Val Loss: 2.9522
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 3.0070 | Val Loss: 3.0621
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 3.1033 | Val Loss: 3.1257
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 3.1366 | Val Loss: 3.1406
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 3.1459 | Val Loss: 3.1464
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 3.1481 | Val Loss: 3.1427
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 3.1359 | Val Loss: 3.1260
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 3.1224 | Val Loss: 3.1176
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.256 MB uploadedwandb: | 137.276 MB of 137.276 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.11999
wandb:   val_loss 3.12183
wandb: 
wandb: üöÄ View run wobbly-sweep-332 at: https://wandb.ai/7shoe/domShift-extensive/runs/gcmzagm5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175053-gcmzagm5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 3.1200 | Val Loss: 3.1218
2025-03-26 17:51:37,933 - wandb.wandb_agent - INFO - Cleaning up finished run: gcmzagm5
2025-03-26 17:51:52,484 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:51:52,484 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:51:52,488 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:51:57,503 - wandb.wandb_agent - INFO - Running runs: ['zkdnx3qp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175158-zkdnx3qp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-337
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zkdnx3qp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: zkdnx3qp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.5247 | Val Loss: 1.4118
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.5067 | Val Loss: 1.5838
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.5811 | Val Loss: 1.5884
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.6222 | Val Loss: 1.6571
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.6364 | Val Loss: 1.5636
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.4590 | Val Loss: 1.4355
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.4821 | Val Loss: 1.5416
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.5496 | Val Loss: 1.4585
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.4913 | Val Loss: 1.4505
wandb: - 137.027 MB of 137.027 MB uploadedwandb: \ 137.027 MB of 137.027 MB uploadedwandb: | 137.047 MB of 137.047 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.39322
wandb:   val_loss 1.38834
wandb: 
wandb: üöÄ View run sweet-sweep-337 at: https://wandb.ai/7shoe/domShift-extensive/runs/zkdnx3qp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175158-zkdnx3qp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.3932 | Val Loss: 1.3883
2025-03-26 17:52:43,206 - wandb.wandb_agent - INFO - Cleaning up finished run: zkdnx3qp
2025-03-26 17:52:43,771 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:52:43,772 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:52:43,774 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:52:48,786 - wandb.wandb_agent - INFO - Running runs: ['ybghbvp4']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175248-ybghbvp4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-341
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ybghbvp4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ybghbvp4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3838 | Val Loss: 1.3233
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1046 | Val Loss: 1.1751
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1745 | Val Loss: 1.1806
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1639 | Val Loss: 1.1512
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1508 | Val Loss: 1.2261
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3070 | Val Loss: 1.4261
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2151 | Val Loss: 1.0608
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1528 | Val Loss: 1.2245
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2133 | Val Loss: 1.2200
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÇ‚ñÖ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08689
wandb:   val_loss 0.99407
wandb: 
wandb: üöÄ View run dashing-sweep-341 at: https://wandb.ai/7shoe/domShift-extensive/runs/ybghbvp4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175248-ybghbvp4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0869 | Val Loss: 0.9941
2025-03-26 17:53:34,440 - wandb.wandb_agent - INFO - Cleaning up finished run: ybghbvp4
2025-03-26 17:53:35,028 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:53:35,028 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:53:35,032 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 17:53:40,040 - wandb.wandb_agent - INFO - Running runs: ['sqdiwi44']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175340-sqdiwi44
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-348
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sqdiwi44
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: sqdiwi44
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0490 | Val Loss: 0.9099
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9272 | Val Loss: 0.9120
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9836 | Val Loss: 1.0039
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9860 | Val Loss: 1.0418
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1157 | Val Loss: 1.1758
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1785 | Val Loss: 1.1888
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2066 | Val Loss: 1.2545
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3525 | Val Loss: 1.4070
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3949 | Val Loss: 1.3746
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37923
wandb:   val_loss 1.38224
wandb: 
wandb: üöÄ View run revived-sweep-348 at: https://wandb.ai/7shoe/domShift-extensive/runs/sqdiwi44
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175340-sqdiwi44/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3792 | Val Loss: 1.3822
2025-03-26 17:54:25,697 - wandb.wandb_agent - INFO - Cleaning up finished run: sqdiwi44
2025-03-26 17:54:26,233 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:54:26,234 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 17:54:26,236 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:54:31,249 - wandb.wandb_agent - INFO - Running runs: ['bcvgp41c']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175431-bcvgp41c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-354
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bcvgp41c
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bcvgp41c
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.1909 | Val Loss: 2.9483
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7904 | Val Loss: 2.8627
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.8300 | Val Loss: 2.8097
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7552 | Val Loss: 2.7019
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.6392 | Val Loss: 2.5472
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4465 | Val Loss: 2.3227
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2393 | Val Loss: 2.1486
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0872 | Val Loss: 2.0261
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9972 | Val Loss: 1.9841
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.99616
wandb:   val_loss 2.03066
wandb: 
wandb: üöÄ View run chocolate-sweep-354 at: https://wandb.ai/7shoe/domShift-extensive/runs/bcvgp41c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175431-bcvgp41c/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9962 | Val Loss: 2.0307
2025-03-26 17:55:01,659 - wandb.wandb_agent - INFO - Cleaning up finished run: bcvgp41c
2025-03-26 17:55:02,162 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:55:02,162 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:55:02,165 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:55:07,177 - wandb.wandb_agent - INFO - Running runs: ['u3su1mln']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175508-u3su1mln
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-358
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u3su1mln
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u3su1mln
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2254 | Val Loss: 1.1419
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0874 | Val Loss: 1.0982
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1011 | Val Loss: 1.0700
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0506 | Val Loss: 1.0528
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0733 | Val Loss: 1.0765
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0628 | Val Loss: 1.0576
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0692 | Val Loss: 1.0702
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0621 | Val Loss: 1.0634
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0686 | Val Loss: 1.0623
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06285
wandb:   val_loss 1.07023
wandb: 
wandb: üöÄ View run rich-sweep-358 at: https://wandb.ai/7shoe/domShift-extensive/runs/u3su1mln
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175508-u3su1mln/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0628 | Val Loss: 1.0702
2025-03-26 17:56:23,286 - wandb.wandb_agent - INFO - Cleaning up finished run: u3su1mln
2025-03-26 17:56:23,783 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:56:23,783 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:56:23,787 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.5
2025-03-26 17:56:28,799 - wandb.wandb_agent - INFO - Running runs: ['cqme91cb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175630-cqme91cb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-368
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cqme91cb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cqme91cb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.1825 | Val Loss: 1.0569
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.0432 | Val Loss: 1.0344
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.0235 | Val Loss: 1.0045
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.0003 | Val Loss: 0.9982
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 0.9951 | Val Loss: 0.9944
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 0.9954 | Val Loss: 0.9955
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 0.9962 | Val Loss: 0.9961
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 0.9963 | Val Loss: 0.9961
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 0.9964 | Val Loss: 0.9962
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 33.059 MB of 33.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99642
wandb:   val_loss 0.99631
wandb: 
wandb: üöÄ View run giddy-sweep-368 at: https://wandb.ai/7shoe/domShift-extensive/runs/cqme91cb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175630-cqme91cb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 0.9964 | Val Loss: 0.9963
2025-03-26 17:57:19,483 - wandb.wandb_agent - INFO - Cleaning up finished run: cqme91cb
2025-03-26 17:57:20,183 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:57:20,183 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:57:20,186 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:57:25,198 - wandb.wandb_agent - INFO - Running runs: ['xku93a9b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175726-xku93a9b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-373
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xku93a9b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xku93a9b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8595 | Val Loss: 2.4049
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1996 | Val Loss: 1.9708
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.8972 | Val Loss: 1.8385
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8316 | Val Loss: 1.8230
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8248 | Val Loss: 1.8222
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8228 | Val Loss: 1.8173
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8045 | Val Loss: 1.7607
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.7378 | Val Loss: 1.7084
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.7006 | Val Loss: 1.6957
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.70127
wandb:   val_loss 1.70318
wandb: 
wandb: üöÄ View run graceful-sweep-373 at: https://wandb.ai/7shoe/domShift-extensive/runs/xku93a9b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175726-xku93a9b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7013 | Val Loss: 1.7032
2025-03-26 17:58:00,683 - wandb.wandb_agent - INFO - Cleaning up finished run: xku93a9b
2025-03-26 17:58:01,273 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:58:01,273 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:58:01,275 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:58:06,288 - wandb.wandb_agent - INFO - Running runs: ['00bke0pi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175807-00bke0pi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-377
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/00bke0pi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 00bke0pi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4603 | Val Loss: 1.2663
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2540 | Val Loss: 1.2610
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3262 | Val Loss: 1.3462
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3030 | Val Loss: 1.2584
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2508 | Val Loss: 1.2511
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2498 | Val Loss: 1.2446
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2346 | Val Loss: 1.2250
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2236 | Val Loss: 1.2236
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2261 | Val Loss: 1.2280
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22946
wandb:   val_loss 1.22997
wandb: 
wandb: üöÄ View run exalted-sweep-377 at: https://wandb.ai/7shoe/domShift-extensive/runs/00bke0pi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175807-00bke0pi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2295 | Val Loss: 1.2300
2025-03-26 17:58:46,890 - wandb.wandb_agent - INFO - Cleaning up finished run: 00bke0pi
2025-03-26 17:58:47,438 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:58:47,438 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:58:47,441 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:58:52,454 - wandb.wandb_agent - INFO - Running runs: ['gcua3zq9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175853-gcua3zq9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-383
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gcua3zq9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gcua3zq9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.8981 | Val Loss: 1.7503
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6764 | Val Loss: 1.5454
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4393 | Val Loss: 1.3603
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2541 | Val Loss: 1.2272
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2121 | Val Loss: 1.2059
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2097 | Val Loss: 1.2151
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2216 | Val Loss: 1.2292
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2379 | Val Loss: 1.2478
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2583 | Val Loss: 1.2686
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 33.064 MB uploadedwandb: | 32.971 MB of 33.064 MB uploadedwandb: / 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27734
wandb:   val_loss 1.28943
wandb: 
wandb: üöÄ View run avid-sweep-383 at: https://wandb.ai/7shoe/domShift-extensive/runs/gcua3zq9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175853-gcua3zq9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2773 | Val Loss: 1.2894
2025-03-26 17:59:43,204 - wandb.wandb_agent - INFO - Cleaning up finished run: gcua3zq9
2025-03-26 17:59:43,826 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:59:43,826 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:59:43,829 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:59:48,840 - wandb.wandb_agent - INFO - Running runs: ['bh7geury']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175948-bh7geury
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-389
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bh7geury
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: bh7geury
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2630 | Val Loss: 1.3178
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2761 | Val Loss: 1.2291
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2199 | Val Loss: 1.2107
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1837 | Val Loss: 1.1706
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1548 | Val Loss: 1.1541
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1522 | Val Loss: 1.1565
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0557 | Val Loss: 1.0318
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0260 | Val Loss: 1.0258
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0265 | Val Loss: 1.0269
wandb: - 34.332 MB of 34.332 MB uploadedwandb: \ 34.332 MB of 34.332 MB uploadedwandb: | 34.376 MB of 34.376 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.02682
wandb:   val_loss 1.02738
wandb: 
wandb: üöÄ View run devout-sweep-389 at: https://wandb.ai/7shoe/domShift-extensive/runs/bh7geury
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175948-bh7geury/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0268 | Val Loss: 1.0274
2025-03-26 18:00:34,505 - wandb.wandb_agent - INFO - Cleaning up finished run: bh7geury
2025-03-26 18:00:35,189 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:35,189 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:00:35,191 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:00:40,204 - wandb.wandb_agent - INFO - Running runs: ['83hbjxzg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180041-83hbjxzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-395
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/83hbjxzg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 83hbjxzg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.3027 | Val Loss: 1.1143
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.1195 | Val Loss: 1.1190
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.1007 | Val Loss: 1.0894
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.0848 | Val Loss: 1.0808
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.0785 | Val Loss: 1.0765
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0769 | Val Loss: 1.0783
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0792 | Val Loss: 1.0803
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0811 | Val Loss: 1.0821
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.0827 | Val Loss: 1.0836
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 33.059 MB uploadedwandb: | 32.965 MB of 33.059 MB uploadedwandb: / 33.059 MB of 33.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08416
wandb:   val_loss 1.08489
wandb: 
wandb: üöÄ View run curious-sweep-395 at: https://wandb.ai/7shoe/domShift-extensive/runs/83hbjxzg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180041-83hbjxzg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.0842 | Val Loss: 1.0849
2025-03-26 18:01:30,885 - wandb.wandb_agent - INFO - Cleaning up finished run: 83hbjxzg
2025-03-26 18:01:31,586 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:01:31,586 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:01:31,589 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:01:36,601 - wandb.wandb_agent - INFO - Running runs: ['i9i82xpu']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180136-i9i82xpu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-399
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/i9i82xpu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: i9i82xpu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3397 | Val Loss: 1.3131
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5293 | Val Loss: 1.6277
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5322 | Val Loss: 1.5138
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5156 | Val Loss: 1.4846
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4328 | Val Loss: 1.2755
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1768 | Val Loss: 1.1990
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2349 | Val Loss: 1.2657
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3406 | Val Loss: 1.4669
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2842 | Val Loss: 1.1665
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñà‚ñà‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14671
wandb:   val_loss 1.11135
wandb: 
wandb: üöÄ View run misty-sweep-399 at: https://wandb.ai/7shoe/domShift-extensive/runs/i9i82xpu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180136-i9i82xpu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1467 | Val Loss: 1.1113
2025-03-26 18:02:52,701 - wandb.wandb_agent - INFO - Cleaning up finished run: i9i82xpu
2025-03-26 18:02:53,191 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:02:53,191 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:02:53,193 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:02:58,206 - wandb.wandb_agent - INFO - Running runs: ['ft0etj1z']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180259-ft0etj1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-408
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ft0etj1z
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ft0etj1z
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5034 | Val Loss: 1.2581
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2852 | Val Loss: 1.3070
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3110 | Val Loss: 1.3106
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3039 | Val Loss: 1.3012
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3038 | Val Loss: 1.3056
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3058 | Val Loss: 1.3108
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3098 | Val Loss: 1.3082
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3120 | Val Loss: 1.3152
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3187 | Val Loss: 1.3257
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32606
wandb:   val_loss 1.33303
wandb: 
wandb: üöÄ View run neat-sweep-408 at: https://wandb.ai/7shoe/domShift-extensive/runs/ft0etj1z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180259-ft0etj1z/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3261 | Val Loss: 1.3330
2025-03-26 18:03:38,742 - wandb.wandb_agent - INFO - Cleaning up finished run: ft0etj1z
2025-03-26 18:03:39,331 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:03:39,331 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:03:39,334 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:03:44,346 - wandb.wandb_agent - INFO - Running runs: ['qrrp1b4u']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180344-qrrp1b4u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-413
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qrrp1b4u
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qrrp1b4u
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7257 | Val Loss: 1.6312
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5106 | Val Loss: 1.3462
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3898 | Val Loss: 1.4602
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4972 | Val Loss: 1.5502
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6102 | Val Loss: 1.6273
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6809 | Val Loss: 1.6921
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5739 | Val Loss: 1.5325
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4959 | Val Loss: 1.4690
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4363 | Val Loss: 1.4089
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40446
wandb:   val_loss 1.39806
wandb: 
wandb: üöÄ View run eager-sweep-413 at: https://wandb.ai/7shoe/domShift-extensive/runs/qrrp1b4u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180344-qrrp1b4u/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4045 | Val Loss: 1.3981
2025-03-26 18:04:24,861 - wandb.wandb_agent - INFO - Cleaning up finished run: qrrp1b4u
2025-03-26 18:04:25,488 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:04:25,488 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:04:25,491 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 18:04:30,503 - wandb.wandb_agent - INFO - Running runs: ['nf8kh90k']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180431-nf8kh90k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-418
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nf8kh90k
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: nf8kh90k
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5684 | Val Loss: 1.3170
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2531 | Val Loss: 1.2483
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2524 | Val Loss: 1.2310
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2206 | Val Loss: 1.2160
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2277 | Val Loss: 1.2416
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2498 | Val Loss: 1.2533
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2527 | Val Loss: 1.2479
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2354 | Val Loss: 1.1777
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1402 | Val Loss: 1.1330
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11591
wandb:   val_loss 1.0804
wandb: 
wandb: üöÄ View run distinctive-sweep-418 at: https://wandb.ai/7shoe/domShift-extensive/runs/nf8kh90k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180431-nf8kh90k/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1159 | Val Loss: 1.0804
2025-03-26 18:05:00,955 - wandb.wandb_agent - INFO - Cleaning up finished run: nf8kh90k
2025-03-26 18:05:01,690 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:05:01,690 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:05:01,692 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:05:06,705 - wandb.wandb_agent - INFO - Running runs: ['3cmy1jfv']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180506-3cmy1jfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-423
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3cmy1jfv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3cmy1jfv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2812 | Val Loss: 1.1890
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2116 | Val Loss: 1.2078
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2077 | Val Loss: 1.2159
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2099 | Val Loss: 1.2078
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2098 | Val Loss: 1.2136
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2132 | Val Loss: 1.2143
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2131 | Val Loss: 1.2135
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2123 | Val Loss: 1.2129
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2117 | Val Loss: 1.2123
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.2108
wandb:   val_loss 1.2116
wandb: 
wandb: üöÄ View run playful-sweep-423 at: https://wandb.ai/7shoe/domShift-extensive/runs/3cmy1jfv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180506-3cmy1jfv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2108 | Val Loss: 1.2116
2025-03-26 18:05:52,370 - wandb.wandb_agent - INFO - Cleaning up finished run: 3cmy1jfv
2025-03-26 18:05:52,915 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:05:52,915 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:05:52,918 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:05:57,930 - wandb.wandb_agent - INFO - Running runs: ['dgb10n7e']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180558-dgb10n7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-429
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dgb10n7e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: dgb10n7e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9739 | Val Loss: 1.5406
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3492 | Val Loss: 1.1992
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1588 | Val Loss: 1.1516
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1634 | Val Loss: 1.1757
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1915 | Val Loss: 1.2073
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2277 | Val Loss: 1.2595
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2962 | Val Loss: 1.3186
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3045 | Val Loss: 1.3034
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2739 | Val Loss: 1.2470
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25139
wandb:   val_loss 1.25587
wandb: 
wandb: üöÄ View run robust-sweep-429 at: https://wandb.ai/7shoe/domShift-extensive/runs/dgb10n7e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180558-dgb10n7e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2514 | Val Loss: 1.2559
2025-03-26 18:06:43,559 - wandb.wandb_agent - INFO - Cleaning up finished run: dgb10n7e
2025-03-26 18:06:44,163 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:44,163 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:06:44,166 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 18:06:49,178 - wandb.wandb_agent - INFO - Running runs: ['yrz0muh8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180650-yrz0muh8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-435
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yrz0muh8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yrz0muh8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4541 | Val Loss: 1.0907
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0792 | Val Loss: 1.1804
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1963 | Val Loss: 1.1668
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1680 | Val Loss: 1.1818
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2011 | Val Loss: 1.2171
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2269 | Val Loss: 1.2310
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2279 | Val Loss: 1.2267
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2296 | Val Loss: 1.2323
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2329 | Val Loss: 1.2336
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23349
wandb:   val_loss 1.23337
wandb: 
wandb: üöÄ View run mild-sweep-435 at: https://wandb.ai/7shoe/domShift-extensive/runs/yrz0muh8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180650-yrz0muh8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2335 | Val Loss: 1.2334
2025-03-26 18:07:44,962 - wandb.wandb_agent - INFO - Cleaning up finished run: yrz0muh8
2025-03-26 18:07:45,473 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:07:45,473 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:07:45,476 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.5
2025-03-26 18:07:50,488 - wandb.wandb_agent - INFO - Running runs: ['b4qvjwik']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180750-b4qvjwik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-442
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b4qvjwik
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: b4qvjwik
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6102 | Val Loss: 1.3167
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1455 | Val Loss: 0.9839
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9495 | Val Loss: 0.9354
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9288 | Val Loss: 0.9170
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9145 | Val Loss: 0.9121
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9089 | Val Loss: 0.8999
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.8931 | Val Loss: 0.8820
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8833 | Val Loss: 0.8841
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8836 | Val Loss: 0.8822
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.88239
wandb:   val_loss 0.88218
wandb: 
wandb: üöÄ View run different-sweep-442 at: https://wandb.ai/7shoe/domShift-extensive/runs/b4qvjwik
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180750-b4qvjwik/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8824 | Val Loss: 0.8822
2025-03-26 18:08:25,995 - wandb.wandb_agent - INFO - Cleaning up finished run: b4qvjwik
2025-03-26 18:08:26,531 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:08:26,531 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:08:26,534 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:08:31,545 - wandb.wandb_agent - INFO - Running runs: ['7srqcsvo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180832-7srqcsvo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-448
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7srqcsvo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7srqcsvo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3896 | Val Loss: 1.1869
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1834 | Val Loss: 1.1837
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2054 | Val Loss: 1.2357
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2931 | Val Loss: 1.3589
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3895 | Val Loss: 1.3727
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3138 | Val Loss: 1.2526
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2333 | Val Loss: 1.2164
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2088 | Val Loss: 1.2010
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1888 | Val Loss: 1.1864
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1883
wandb:   val_loss 1.18862
wandb: 
wandb: üöÄ View run faithful-sweep-448 at: https://wandb.ai/7shoe/domShift-extensive/runs/7srqcsvo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180832-7srqcsvo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1883 | Val Loss: 1.1886
2025-03-26 18:09:22,273 - wandb.wandb_agent - INFO - Cleaning up finished run: 7srqcsvo
2025-03-26 18:09:23,063 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:09:23,064 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:09:23,067 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:09:28,079 - wandb.wandb_agent - INFO - Running runs: ['iqa1wnl3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180929-iqa1wnl3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-452
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/iqa1wnl3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: iqa1wnl3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5122 | Val Loss: 1.4049
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3518 | Val Loss: 1.2786
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2742 | Val Loss: 1.2317
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1456 | Val Loss: 1.0768
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1194 | Val Loss: 1.1882
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1813 | Val Loss: 1.1045
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0381 | Val Loss: 1.0267
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0499 | Val Loss: 1.0683
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0719 | Val Loss: 1.0814
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08853
wandb:   val_loss 1.09013
wandb: 
wandb: üöÄ View run crisp-sweep-452 at: https://wandb.ai/7shoe/domShift-extensive/runs/iqa1wnl3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180929-iqa1wnl3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0885 | Val Loss: 1.0901
2025-03-26 18:10:08,687 - wandb.wandb_agent - INFO - Cleaning up finished run: iqa1wnl3
2025-03-26 18:10:09,543 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:10:09,543 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:10:09,546 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:10:14,558 - wandb.wandb_agent - INFO - Running runs: ['aq35n0yq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181014-aq35n0yq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-458
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/aq35n0yq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: aq35n0yq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.8876 | Val Loss: 2.6535
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.6237 | Val Loss: 2.6253
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.6293 | Val Loss: 2.5778
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.5318 | Val Loss: 2.4971
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.4789 | Val Loss: 2.4643
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.4498 | Val Loss: 2.4299
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.4085 | Val Loss: 2.3881
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.3728 | Val Loss: 2.3574
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3468 | Val Loss: 2.3368
wandb: - 32.969 MB of 32.969 MB uploadedwandb: \ 32.969 MB of 32.969 MB uploadedwandb: | 32.988 MB of 32.988 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.33095
wandb:   val_loss 2.32576
wandb: 
wandb: üöÄ View run restful-sweep-458 at: https://wandb.ai/7shoe/domShift-extensive/runs/aq35n0yq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181014-aq35n0yq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.3309 | Val Loss: 2.3258
2025-03-26 18:10:55,281 - wandb.wandb_agent - INFO - Cleaning up finished run: aq35n0yq
2025-03-26 18:10:55,834 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:10:55,834 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:10:55,837 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:11:00,849 - wandb.wandb_agent - INFO - Running runs: ['cjh17kyp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181102-cjh17kyp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-462
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cjh17kyp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: cjh17kyp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 2.6742 | Val Loss: 2.1548
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.9618 | Val Loss: 1.8530
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.8330 | Val Loss: 1.8278
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.8472 | Val Loss: 1.8702
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.9000 | Val Loss: 1.9279
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.9516 | Val Loss: 1.9722
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.9895 | Val Loss: 2.0046
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.0175 | Val Loss: 2.0289
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.0388 | Val Loss: 2.0476
wandb: - 32.941 MB of 32.941 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.05541
wandb:   val_loss 2.06228
wandb: 
wandb: üöÄ View run cosmic-sweep-462 at: https://wandb.ai/7shoe/domShift-extensive/runs/cjh17kyp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181102-cjh17kyp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.0554 | Val Loss: 2.0623
2025-03-26 18:11:46,521 - wandb.wandb_agent - INFO - Cleaning up finished run: cjh17kyp
2025-03-26 18:11:47,457 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:11:47,457 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:11:47,460 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:11:52,472 - wandb.wandb_agent - INFO - Running runs: ['3s7bjjih']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181153-3s7bjjih
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-468
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3s7bjjih
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3s7bjjih
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5871 | Val Loss: 1.4271
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3478 | Val Loss: 1.3719
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4150 | Val Loss: 1.4637
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6172 | Val Loss: 1.7174
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4250 | Val Loss: 1.1676
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0921 | Val Loss: 1.0643
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0744 | Val Loss: 1.0863
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0798 | Val Loss: 1.0791
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0788 | Val Loss: 1.0785
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb: - 137.472 MB of 137.472 MB uploadedwandb: \ 137.472 MB of 137.472 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07841
wandb:   val_loss 1.0784
wandb: 
wandb: üöÄ View run effortless-sweep-468 at: https://wandb.ai/7shoe/domShift-extensive/runs/3s7bjjih
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181153-3s7bjjih/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0784 | Val Loss: 1.0784
2025-03-26 18:13:54,215 - wandb.wandb_agent - INFO - Cleaning up finished run: 3s7bjjih
2025-03-26 18:13:55,825 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:13:55,826 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:13:55,828 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.7000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:14:00,840 - wandb.wandb_agent - INFO - Running runs: ['4w78byi2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181400-4w78byi2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-480
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4w78byi2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4w78byi2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.8766 | Val Loss: 5.5020
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 5.4730 | Val Loss: 5.3435
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 5.3272 | Val Loss: 5.2368
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 5.2384 | Val Loss: 5.1758
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 5.1992 | Val Loss: 5.1431
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 5.1717 | Val Loss: 5.1223
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 5.1379 | Val Loss: 5.0961
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 5.1171 | Val Loss: 5.0760
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 5.0964 | Val Loss: 5.0661
wandb: - 137.312 MB of 137.312 MB uploadedwandb: \ 137.312 MB of 137.312 MB uploadedwandb: | 137.332 MB of 137.332 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.08825
wandb:   val_loss 5.04499
wandb: 
wandb: üöÄ View run tough-sweep-480 at: https://wandb.ai/7shoe/domShift-extensive/runs/4w78byi2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181400-4w78byi2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 5.0882 | Val Loss: 5.0450
2025-03-26 18:14:31,291 - wandb.wandb_agent - INFO - Cleaning up finished run: 4w78byi2
2025-03-26 18:14:31,978 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:14:31,978 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:14:31,981 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:14:36,994 - wandb.wandb_agent - INFO - Running runs: ['qx10rfwi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181438-qx10rfwi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-484
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qx10rfwi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qx10rfwi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.7697 | Val Loss: 1.7070
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.5559 | Val Loss: 1.3043
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.3082 | Val Loss: 1.3543
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.3916 | Val Loss: 1.3947
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.3667 | Val Loss: 1.3377
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.3258 | Val Loss: 1.3299
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.3496 | Val Loss: 1.3634
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.3607 | Val Loss: 1.3487
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.3377 | Val Loss: 1.3282
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32048
wandb:   val_loss 1.31201
wandb: 
wandb: üöÄ View run fluent-sweep-484 at: https://wandb.ai/7shoe/domShift-extensive/runs/qx10rfwi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181438-qx10rfwi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.3205 | Val Loss: 1.3120
2025-03-26 18:15:53,103 - wandb.wandb_agent - INFO - Cleaning up finished run: qx10rfwi
2025-03-26 18:15:53,728 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:15:53,729 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:15:53,732 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:15:58,740 - wandb.wandb_agent - INFO - Running runs: ['qfhrvk8p']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181558-qfhrvk8p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-491
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qfhrvk8p
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qfhrvk8p
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6878 | Val Loss: 1.7123
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6698 | Val Loss: 1.5335
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4855 | Val Loss: 1.4421
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3970 | Val Loss: 1.3569
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3365 | Val Loss: 1.3246
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3375 | Val Loss: 1.3546
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3638 | Val Loss: 1.3728
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3751 | Val Loss: 1.3769
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3889 | Val Loss: 1.4023
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.313 MB of 137.333 MB uploadedwandb: - 137.313 MB of 137.333 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40346
wandb:   val_loss 1.41453
wandb: 
wandb: üöÄ View run genial-sweep-491 at: https://wandb.ai/7shoe/domShift-extensive/runs/qfhrvk8p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181558-qfhrvk8p/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4035 | Val Loss: 1.4145
2025-03-26 18:17:50,316 - wandb.wandb_agent - INFO - Cleaning up finished run: qfhrvk8p
2025-03-26 18:17:50,921 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:17:50,921 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:17:50,924 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:17:55,935 - wandb.wandb_agent - INFO - Running runs: ['h8y55jh5']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181757-h8y55jh5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-502
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/h8y55jh5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: h8y55jh5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6602 | Val Loss: 1.4398
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3861 | Val Loss: 1.3775
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3968 | Val Loss: 1.4163
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4497 | Val Loss: 1.5000
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5388 | Val Loss: 1.5548
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4749 | Val Loss: 1.2656
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1825 | Val Loss: 1.1283
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1186 | Val Loss: 1.1341
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1440 | Val Loss: 1.1486
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14102
wandb:   val_loss 1.1273
wandb: 
wandb: üöÄ View run desert-sweep-502 at: https://wandb.ai/7shoe/domShift-extensive/runs/h8y55jh5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181757-h8y55jh5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1410 | Val Loss: 1.1273
2025-03-26 18:18:51,755 - wandb.wandb_agent - INFO - Cleaning up finished run: h8y55jh5
2025-03-26 18:18:52,360 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:18:52,360 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:18:52,363 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:18:57,376 - wandb.wandb_agent - INFO - Running runs: ['6982h0na']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181857-6982h0na
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-507
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6982h0na
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6982h0na
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5222 | Val Loss: 1.2605
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0963 | Val Loss: 1.0172
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9818 | Val Loss: 0.9597
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9532 | Val Loss: 0.9456
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9441 | Val Loss: 0.9445
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9453 | Val Loss: 0.9441
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9420 | Val Loss: 0.9352
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9307 | Val Loss: 0.9257
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9282 | Val Loss: 0.9303
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.93004
wandb:   val_loss 0.92565
wandb: 
wandb: üöÄ View run fearless-sweep-507 at: https://wandb.ai/7shoe/domShift-extensive/runs/6982h0na
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181857-6982h0na/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9300 | Val Loss: 0.9257
2025-03-26 18:19:27,770 - wandb.wandb_agent - INFO - Cleaning up finished run: 6982h0na
2025-03-26 18:19:28,539 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:19:28,539 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:19:28,542 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:19:33,554 - wandb.wandb_agent - INFO - Running runs: ['f6nn9r76']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181933-f6nn9r76
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-511
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f6nn9r76
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: f6nn9r76
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3374 | Val Loss: 1.0057
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9757 | Val Loss: 0.9870
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9738 | Val Loss: 0.9503
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9456 | Val Loss: 0.9528
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9484 | Val Loss: 0.9458
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9489 | Val Loss: 0.9494
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9487 | Val Loss: 0.9494
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9493 | Val Loss: 0.9493
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9497 | Val Loss: 0.9500
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.94978
wandb:   val_loss 0.95028
wandb: 
wandb: üöÄ View run cerulean-sweep-511 at: https://wandb.ai/7shoe/domShift-extensive/runs/f6nn9r76
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181933-f6nn9r76/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9498 | Val Loss: 0.9503
2025-03-26 18:20:09,092 - wandb.wandb_agent - INFO - Cleaning up finished run: f6nn9r76
2025-03-26 18:20:09,893 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:20:09,893 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:20:09,896 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:20:14,909 - wandb.wandb_agent - INFO - Running runs: ['dyj8s4qj']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182014-dyj8s4qj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-514
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dyj8s4qj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dyj8s4qj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.3705 | Val Loss: 2.1234
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.1246 | Val Loss: 2.2541
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.3462 | Val Loss: 2.4273
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.4788 | Val Loss: 2.5202
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.5201 | Val Loss: 2.5272
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.5335 | Val Loss: 2.4989
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.3098 | Val Loss: 1.9903
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.9644 | Val Loss: 1.9635
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.9476 | Val Loss: 1.9339
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 32.988 MB of 32.988 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.93214
wandb:   val_loss 1.93552
wandb: 
wandb: üöÄ View run cerulean-sweep-514 at: https://wandb.ai/7shoe/domShift-extensive/runs/dyj8s4qj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182014-dyj8s4qj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.9321 | Val Loss: 1.9355
2025-03-26 18:20:55,443 - wandb.wandb_agent - INFO - Cleaning up finished run: dyj8s4qj
2025-03-26 18:20:56,109 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:20:56,110 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.9
2025-03-26 18:20:56,112 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.9
2025-03-26 18:21:01,124 - wandb.wandb_agent - INFO - Running runs: ['0fe1w6en']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182102-0fe1w6en
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-519
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/0fe1w6en
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 0fe1w6en
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9725 | Val Loss: 2.8903
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.8805 | Val Loss: 2.8602
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8647 | Val Loss: 2.8612
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.8450 | Val Loss: 2.8258
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8045 | Val Loss: 2.7840
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.7709 | Val Loss: 2.7667
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.7812 | Val Loss: 2.7990
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.8120 | Val Loss: 2.8227
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.8288 | Val Loss: 2.8335
wandb: - 10.262 MB of 10.262 MB uploadedwandb: \ 10.262 MB of 10.262 MB uploadedwandb: | 10.291 MB of 10.291 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.83327
wandb:   val_loss 2.83178
wandb: 
wandb: üöÄ View run dauntless-sweep-519 at: https://wandb.ai/7shoe/domShift-extensive/runs/0fe1w6en
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182102-0fe1w6en/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.8333 | Val Loss: 2.8318
2025-03-26 18:21:46,760 - wandb.wandb_agent - INFO - Cleaning up finished run: 0fe1w6en
2025-03-26 18:21:47,767 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:21:47,767 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:21:47,770 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:21:52,781 - wandb.wandb_agent - INFO - Running runs: ['nfmnpva5']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182152-nfmnpva5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-525
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nfmnpva5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: nfmnpva5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0434 | Val Loss: 2.7718
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7669 | Val Loss: 2.6973
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6168 | Val Loss: 2.5120
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2414 | Val Loss: 2.1323
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8136 | Val Loss: 1.8529
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.9526 | Val Loss: 1.9988
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0119 | Val Loss: 2.0240
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0335 | Val Loss: 2.0415
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0470 | Val Loss: 2.0505
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.04986
wandb:   val_loss 2.0496
wandb: 
wandb: üöÄ View run usual-sweep-525 at: https://wandb.ai/7shoe/domShift-extensive/runs/nfmnpva5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182152-nfmnpva5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.0499 | Val Loss: 2.0496
2025-03-26 18:24:04,930 - wandb.wandb_agent - INFO - Cleaning up finished run: nfmnpva5
2025-03-26 18:24:05,557 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:24:05,557 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:24:05,560 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:24:10,572 - wandb.wandb_agent - INFO - Running runs: ['a5qtkbmu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182410-a5qtkbmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-537
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/a5qtkbmu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: a5qtkbmu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4455 | Val Loss: 1.4093
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5258 | Val Loss: 1.6331
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5594 | Val Loss: 1.4613
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4635 | Val Loss: 1.4737
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4893 | Val Loss: 1.4827
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4569 | Val Loss: 1.4342
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4194 | Val Loss: 1.4018
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3769 | Val Loss: 1.3624
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3680 | Val Loss: 1.3680
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3658
wandb:   val_loss 1.36336
wandb: 
wandb: üöÄ View run exalted-sweep-537 at: https://wandb.ai/7shoe/domShift-extensive/runs/a5qtkbmu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182410-a5qtkbmu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3658 | Val Loss: 1.3634
2025-03-26 18:25:16,557 - wandb.wandb_agent - INFO - Cleaning up finished run: a5qtkbmu
2025-03-26 18:25:17,044 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:25:17,044 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:25:17,047 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:25:22,059 - wandb.wandb_agent - INFO - Running runs: ['01q2lb68']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182522-01q2lb68
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-543
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/01q2lb68
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 01q2lb68
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6920 | Val Loss: 1.3916
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3016 | Val Loss: 1.1846
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1356 | Val Loss: 1.0805
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0720 | Val Loss: 1.0696
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0751 | Val Loss: 1.0817
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0912 | Val Loss: 1.1021
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1095 | Val Loss: 1.1167
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1190 | Val Loss: 1.1202
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1188 | Val Loss: 1.1167
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.102 MB uploadedwandb: | 137.082 MB of 137.102 MB uploadedwandb: / 137.082 MB of 137.102 MB uploadedwandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11495
wandb:   val_loss 1.11281
wandb: 
wandb: üöÄ View run vibrant-sweep-543 at: https://wandb.ai/7shoe/domShift-extensive/runs/01q2lb68
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182522-01q2lb68/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1149 | Val Loss: 1.1128
2025-03-26 18:25:52,507 - wandb.wandb_agent - INFO - Cleaning up finished run: 01q2lb68
2025-03-26 18:25:53,044 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:25:53,044 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimCLR
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:25:53,048 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimCLR --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:25:58,059 - wandb.wandb_agent - INFO - Running runs: ['za1isl3a']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182557-za1isl3a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-547
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/za1isl3a
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: za1isl3a
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.9987 | Val Loss: 3.5596
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.5532 | Val Loss: 3.4388
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.4721 | Val Loss: 3.3875
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.4231 | Val Loss: 3.3505
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.3920 | Val Loss: 3.3322
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.3726 | Val Loss: 3.3248
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.3592 | Val Loss: 3.3129
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.3446 | Val Loss: 3.2949
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.3305 | Val Loss: 3.2782
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.31984
wandb:   val_loss 3.26854
wandb: 
wandb: üöÄ View run brisk-sweep-547 at: https://wandb.ai/7shoe/domShift-extensive/runs/za1isl3a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182557-za1isl3a/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.3198 | Val Loss: 3.2685
2025-03-26 18:26:43,679 - wandb.wandb_agent - INFO - Cleaning up finished run: za1isl3a
2025-03-26 18:26:44,617 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:26:44,617 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:26:44,620 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:26:49,632 - wandb.wandb_agent - INFO - Running runs: ['ig5znsue']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182649-ig5znsue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-551
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ig5znsue
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ig5znsue
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2298 | Val Loss: 1.1044
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1545 | Val Loss: 1.1270
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1153 | Val Loss: 1.1514
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1571 | Val Loss: 1.1204
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1356 | Val Loss: 1.1636
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1433 | Val Loss: 1.1379
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1524 | Val Loss: 1.1300
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1442 | Val Loss: 1.1758
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1429 | Val Loss: 1.1488
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16424
wandb:   val_loss 1.16954
wandb: 
wandb: üöÄ View run sleek-sweep-551 at: https://wandb.ai/7shoe/domShift-extensive/runs/ig5znsue
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182649-ig5znsue/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1642 | Val Loss: 1.1695
2025-03-26 18:28:05,708 - wandb.wandb_agent - INFO - Cleaning up finished run: ig5znsue
2025-03-26 18:28:06,332 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:28:06,332 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:28:06,335 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:28:11,348 - wandb.wandb_agent - INFO - Running runs: ['z2w9nb41']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182812-z2w9nb41
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-560
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/z2w9nb41
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: z2w9nb41
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3426 | Val Loss: 1.2647
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3137 | Val Loss: 1.3788
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3410 | Val Loss: 1.3521
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4476 | Val Loss: 1.4851
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4261 | Val Loss: 1.4128
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4213 | Val Loss: 1.4290
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4286 | Val Loss: 1.4263
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4282 | Val Loss: 1.4335
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4386 | Val Loss: 1.4473
wandb: - 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.45272
wandb:   val_loss 1.45574
wandb: 
wandb: üöÄ View run sleek-sweep-560 at: https://wandb.ai/7shoe/domShift-extensive/runs/z2w9nb41
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182812-z2w9nb41/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4527 | Val Loss: 1.4557
2025-03-26 18:29:22,420 - wandb.wandb_agent - INFO - Cleaning up finished run: z2w9nb41
2025-03-26 18:29:23,877 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:29:23,877 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:29:23,879 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 18:29:28,892 - wandb.wandb_agent - INFO - Running runs: ['knl94mdy']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182929-knl94mdy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-566
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/knl94mdy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: knl94mdy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4764 | Val Loss: 1.2915
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2898 | Val Loss: 1.2670
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1594 | Val Loss: 1.1002
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0818 | Val Loss: 1.0790
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0932 | Val Loss: 1.1017
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1011 | Val Loss: 1.0980
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0917 | Val Loss: 1.0853
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0825 | Val Loss: 1.0805
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0804 | Val Loss: 1.0806
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08042
wandb:   val_loss 1.07793
wandb: 
wandb: üöÄ View run hearty-sweep-566 at: https://wandb.ai/7shoe/domShift-extensive/runs/knl94mdy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182929-knl94mdy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0804 | Val Loss: 1.0779
2025-03-26 18:30:08,807 - wandb.wandb_agent - INFO - Cleaning up finished run: knl94mdy
2025-03-26 18:30:09,328 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:09,329 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:30:09,331 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 18:30:14,340 - wandb.wandb_agent - INFO - Running runs: ['31i8rvyz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183015-31i8rvyz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-570
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/31i8rvyz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 31i8rvyz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.5821 | Val Loss: 1.4609
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.4522 | Val Loss: 1.3983
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.3162 | Val Loss: 1.2414
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.1921 | Val Loss: 1.1195
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.0905 | Val Loss: 1.0587
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.0472 | Val Loss: 1.0322
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.0276 | Val Loss: 1.0240
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.0217 | Val Loss: 1.0223
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.0286 | Val Loss: 1.0376
wandb: - 137.027 MB of 137.027 MB uploadedwandb: \ 137.027 MB of 137.047 MB uploadedwandb: | 137.027 MB of 137.047 MB uploadedwandb: / 137.047 MB of 137.047 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04373
wandb:   val_loss 1.05001
wandb: 
wandb: üöÄ View run gentle-sweep-570 at: https://wandb.ai/7shoe/domShift-extensive/runs/31i8rvyz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183015-31i8rvyz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.0437 | Val Loss: 1.0500
2025-03-26 18:30:44,811 - wandb.wandb_agent - INFO - Cleaning up finished run: 31i8rvyz
2025-03-26 18:30:45,494 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:45,494 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:30:45,497 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:30:50,509 - wandb.wandb_agent - INFO - Running runs: ['omv7tzpy']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183050-omv7tzpy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-575
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/omv7tzpy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: omv7tzpy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3410 | Val Loss: 1.1773
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1556 | Val Loss: 1.1771
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1860 | Val Loss: 1.1766
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1309 | Val Loss: 1.0996
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0858 | Val Loss: 1.0776
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0652 | Val Loss: 1.0640
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0621 | Val Loss: 1.0650
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0631 | Val Loss: 1.0663
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0623 | Val Loss: 1.0638
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06042
wandb:   val_loss 1.06259
wandb: 
wandb: üöÄ View run autumn-sweep-575 at: https://wandb.ai/7shoe/domShift-extensive/runs/omv7tzpy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183050-omv7tzpy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0604 | Val Loss: 1.0626
2025-03-26 18:31:31,004 - wandb.wandb_agent - INFO - Cleaning up finished run: omv7tzpy
2025-03-26 18:31:31,945 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:31:31,945 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:31:31,948 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:31:36,961 - wandb.wandb_agent - INFO - Running runs: ['p6ouyuwl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183138-p6ouyuwl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-580
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/p6ouyuwl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: p6ouyuwl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2929 | Val Loss: 1.1050
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1026 | Val Loss: 1.1017
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1007 | Val Loss: 1.1012
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1005 | Val Loss: 1.1009
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1004 | Val Loss: 1.1008
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1004 | Val Loss: 1.1008
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1004 | Val Loss: 1.1008
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1004 | Val Loss: 1.1008
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1003 | Val Loss: 1.1008
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10032
wandb:   val_loss 1.10069
wandb: 
wandb: üöÄ View run unique-sweep-580 at: https://wandb.ai/7shoe/domShift-extensive/runs/p6ouyuwl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183138-p6ouyuwl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1003 | Val Loss: 1.1007
2025-03-26 18:32:52,976 - wandb.wandb_agent - INFO - Cleaning up finished run: p6ouyuwl
2025-03-26 18:32:53,505 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:32:53,505 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:32:53,508 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:32:58,520 - wandb.wandb_agent - INFO - Running runs: ['9ljwsyx2']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183258-9ljwsyx2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-586
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9ljwsyx2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9ljwsyx2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2836 | Val Loss: 1.0203
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9956 | Val Loss: 0.9813
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9884 | Val Loss: 0.9824
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9754 | Val Loss: 0.9693
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9563 | Val Loss: 0.9368
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9316 | Val Loss: 0.9212
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9063 | Val Loss: 0.8887
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8868 | Val Loss: 0.9193
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9420 | Val Loss: 0.9569
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95341
wandb:   val_loss 0.94494
wandb: 
wandb: üöÄ View run fancy-sweep-586 at: https://wandb.ai/7shoe/domShift-extensive/runs/9ljwsyx2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183258-9ljwsyx2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9534 | Val Loss: 0.9449
2025-03-26 18:33:49,207 - wandb.wandb_agent - INFO - Cleaning up finished run: 9ljwsyx2
2025-03-26 18:33:49,813 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:33:49,813 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:33:49,816 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 18:33:54,828 - wandb.wandb_agent - INFO - Running runs: ['u86e9cb3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183356-u86e9cb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-592
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u86e9cb3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: u86e9cb3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 2.1764 | Val Loss: 2.2121
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.1013 | Val Loss: 1.9677
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.8903 | Val Loss: 1.7757
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.7254 | Val Loss: 1.6554
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.6086 | Val Loss: 1.5442
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.5074 | Val Loss: 1.4554
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.4363 | Val Loss: 1.4054
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.3967 | Val Loss: 1.3850
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.3854 | Val Loss: 1.3826
wandb: - 137.027 MB of 137.027 MB uploadedwandb: \ 137.027 MB of 137.047 MB uploadedwandb: | 137.027 MB of 137.047 MB uploadedwandb: / 137.047 MB of 137.047 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.38542
wandb:   val_loss 1.38615
wandb: 
wandb: üöÄ View run olive-sweep-592 at: https://wandb.ai/7shoe/domShift-extensive/runs/u86e9cb3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183356-u86e9cb3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.3854 | Val Loss: 1.3861
2025-03-26 18:34:25,244 - wandb.wandb_agent - INFO - Cleaning up finished run: u86e9cb3
2025-03-26 18:34:25,761 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:34:25,761 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimCLR
	model_class: cnn
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.9
2025-03-26 18:34:25,764 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimCLR --model_class=cnn --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.9
2025-03-26 18:34:30,776 - wandb.wandb_agent - INFO - Running runs: ['m703rxjt']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183432-m703rxjt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-596
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m703rxjt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: m703rxjt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 6.1432 | Val Loss: 5.8237
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 5.9230 | Val Loss: 5.5854
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 5.7044 | Val Loss: 5.4206
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 5.5676 | Val Loss: 5.3144
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 5.4941 | Val Loss: 5.2752
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 5.4479 | Val Loss: 5.2326
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 5.4202 | Val Loss: 5.2045
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 5.3948 | Val Loss: 5.1919
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 5.3749 | Val Loss: 5.1786
wandb: - 7.907 MB of 7.907 MB uploadedwandb: \ 7.907 MB of 7.907 MB uploadedwandb: | 7.926 MB of 7.926 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.37863
wandb:   val_loss 5.16015
wandb: 
wandb: üöÄ View run unique-sweep-596 at: https://wandb.ai/7shoe/domShift-extensive/runs/m703rxjt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183432-m703rxjt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 5.3786 | Val Loss: 5.1602
2025-03-26 18:34:56,098 - wandb.wandb_agent - INFO - Cleaning up finished run: m703rxjt
2025-03-26 18:34:56,627 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:34:56,627 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:34:56,630 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:35:01,640 - wandb.wandb_agent - INFO - Running runs: ['qeju1co0']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183501-qeju1co0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-599
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qeju1co0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: qeju1co0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4451 | Val Loss: 1.4078
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4226 | Val Loss: 1.4669
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3791 | Val Loss: 1.2430
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1506 | Val Loss: 1.0925
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0762 | Val Loss: 1.0614
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0487 | Val Loss: 1.0238
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0131 | Val Loss: 1.0268
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0433 | Val Loss: 1.0532
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0537 | Val Loss: 1.0502
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñá‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05392
wandb:   val_loss 1.05304
wandb: 
wandb: üöÄ View run gallant-sweep-599 at: https://wandb.ai/7shoe/domShift-extensive/runs/qeju1co0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183501-qeju1co0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0539 | Val Loss: 1.0530
2025-03-26 18:35:37,115 - wandb.wandb_agent - INFO - Cleaning up finished run: qeju1co0
2025-03-26 18:35:37,955 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:35:37,955 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:35:37,959 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:35:42,971 - wandb.wandb_agent - INFO - Running runs: ['3stv5sgh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183544-3stv5sgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-604
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3stv5sgh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3stv5sgh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.3237 | Val Loss: 1.4422
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.3623 | Val Loss: 1.3607
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.3532 | Val Loss: 1.3536
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.3383 | Val Loss: 1.3326
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.3329 | Val Loss: 1.3338
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.3330 | Val Loss: 1.3336
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.3331 | Val Loss: 1.3337
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.3331 | Val Loss: 1.3338
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.3331 | Val Loss: 1.3338
wandb: - 32.850 MB of 32.850 MB uploadedwandb: \ 32.850 MB of 32.850 MB uploadedwandb: | 32.943 MB of 32.943 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3331
wandb:   val_loss 1.33376
wandb: 
wandb: üöÄ View run efficient-sweep-604 at: https://wandb.ai/7shoe/domShift-extensive/runs/3stv5sgh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183544-3stv5sgh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.3331 | Val Loss: 1.3338
2025-03-26 18:36:28,529 - wandb.wandb_agent - INFO - Cleaning up finished run: 3stv5sgh
2025-03-26 18:36:29,092 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:36:29,092 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:36:29,095 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimCLR --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 18:36:34,107 - wandb.wandb_agent - INFO - Running runs: ['jzmpnxdw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183635-jzmpnxdw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-610
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jzmpnxdw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jzmpnxdw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 4.0993 | Val Loss: 3.3695
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.8698 | Val Loss: 2.3577
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.0405 | Val Loss: 1.7686
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.6211 | Val Loss: 1.5062
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.4250 | Val Loss: 1.3276
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.3087 | Val Loss: 1.2497
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.2117 | Val Loss: 1.1706
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.1748 | Val Loss: 1.1323
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.1261 | Val Loss: 1.0811
wandb: - 137.258 MB of 137.258 MB uploadedwandb: \ 137.258 MB of 137.277 MB uploadedwandb: | 137.258 MB of 137.277 MB uploadedwandb: / 137.277 MB of 137.277 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07274
wandb:   val_loss 1.06184
wandb: 
wandb: üöÄ View run playful-sweep-610 at: https://wandb.ai/7shoe/domShift-extensive/runs/jzmpnxdw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183635-jzmpnxdw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.0727 | Val Loss: 1.0618
2025-03-26 18:37:14,682 - wandb.wandb_agent - INFO - Cleaning up finished run: jzmpnxdw
2025-03-26 18:37:15,358 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:37:15,359 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:37:15,361 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:37:20,374 - wandb.wandb_agent - INFO - Running runs: ['1keq5u18']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183721-1keq5u18
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-614
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1keq5u18
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1keq5u18
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3027 | Val Loss: 1.1756
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1259 | Val Loss: 1.1696
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1259 | Val Loss: 1.0903
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0535 | Val Loss: 1.0378
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0706 | Val Loss: 1.0759
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0728 | Val Loss: 1.0750
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0755 | Val Loss: 1.0737
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0745 | Val Loss: 1.0755
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0750 | Val Loss: 1.0752
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07559
wandb:   val_loss 1.07534
wandb: 
wandb: üöÄ View run vital-sweep-614 at: https://wandb.ai/7shoe/domShift-extensive/runs/1keq5u18
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183721-1keq5u18/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0756 | Val Loss: 1.0753
2025-03-26 18:38:31,350 - wandb.wandb_agent - INFO - Cleaning up finished run: 1keq5u18
2025-03-26 18:38:32,127 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:38:32,127 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:38:32,130 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:38:37,142 - wandb.wandb_agent - INFO - Running runs: ['tieymf6l']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183838-tieymf6l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-623
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tieymf6l
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tieymf6l
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8133 | Val Loss: 1.4489
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3118 | Val Loss: 1.1468
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1433 | Val Loss: 1.1869
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2051 | Val Loss: 1.1941
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1837 | Val Loss: 1.1764
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1687 | Val Loss: 1.1636
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1644 | Val Loss: 1.1637
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1625 | Val Loss: 1.1669
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1764 | Val Loss: 1.2055
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20495
wandb:   val_loss 1.17687
wandb: 
wandb: üöÄ View run eager-sweep-623 at: https://wandb.ai/7shoe/domShift-extensive/runs/tieymf6l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183838-tieymf6l/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2049 | Val Loss: 1.1769
2025-03-26 18:39:32,920 - wandb.wandb_agent - INFO - Cleaning up finished run: tieymf6l
2025-03-26 18:39:33,574 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:39:33,575 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:39:33,578 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:39:38,591 - wandb.wandb_agent - INFO - Running runs: ['t405knwb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183939-t405knwb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-630
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/t405knwb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: t405knwb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7815 | Val Loss: 2.3604
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2585 | Val Loss: 2.2111
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2072 | Val Loss: 2.1992
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1904 | Val Loss: 2.1731
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1461 | Val Loss: 2.1163
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0994 | Val Loss: 2.0868
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0917 | Val Loss: 2.0957
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1026 | Val Loss: 2.1089
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1193 | Val Loss: 2.1305
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.1446
wandb:   val_loss 2.15892
wandb: 
wandb: üöÄ View run logical-sweep-630 at: https://wandb.ai/7shoe/domShift-extensive/runs/t405knwb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183939-t405knwb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1446 | Val Loss: 2.1589
2025-03-26 18:40:39,454 - wandb.wandb_agent - INFO - Cleaning up finished run: t405knwb
2025-03-26 18:40:40,323 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:40:40,324 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:40:40,326 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:40:45,339 - wandb.wandb_agent - INFO - Running runs: ['2g9pbtx9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184046-2g9pbtx9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-636
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2g9pbtx9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 2g9pbtx9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5820 | Val Loss: 1.3150
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1945 | Val Loss: 1.1668
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0578 | Val Loss: 1.0234
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9785 | Val Loss: 0.9549
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9654 | Val Loss: 0.9808
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0302 | Val Loss: 1.1104
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1897 | Val Loss: 1.1654
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1140 | Val Loss: 1.1054
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1127 | Val Loss: 1.1299
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13379
wandb:   val_loss 1.14656
wandb: 
wandb: üöÄ View run comic-sweep-636 at: https://wandb.ai/7shoe/domShift-extensive/runs/2g9pbtx9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184046-2g9pbtx9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1338 | Val Loss: 1.1466
2025-03-26 18:41:31,001 - wandb.wandb_agent - INFO - Cleaning up finished run: 2g9pbtx9
2025-03-26 18:41:31,447 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:41:31,447 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:41:31,450 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:41:36,461 - wandb.wandb_agent - INFO - Running runs: ['1zl6b05c']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184137-1zl6b05c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-641
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1zl6b05c
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1zl6b05c
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6272 | Val Loss: 1.6122
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6180 | Val Loss: 1.4239
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3306 | Val Loss: 1.2627
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2491 | Val Loss: 1.2514
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2565 | Val Loss: 1.2622
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2666 | Val Loss: 1.2650
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2634 | Val Loss: 1.2619
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2632 | Val Loss: 1.2636
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2645 | Val Loss: 1.2646
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.26524
wandb:   val_loss 1.26513
wandb: 
wandb: üöÄ View run visionary-sweep-641 at: https://wandb.ai/7shoe/domShift-extensive/runs/1zl6b05c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184137-1zl6b05c/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2652 | Val Loss: 1.2651
2025-03-26 18:42:32,261 - wandb.wandb_agent - INFO - Cleaning up finished run: 1zl6b05c
2025-03-26 18:42:33,023 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:42:33,023 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:42:33,026 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:42:38,038 - wandb.wandb_agent - INFO - Running runs: ['nxv2ftf8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184238-nxv2ftf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-646
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nxv2ftf8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: nxv2ftf8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6794 | Val Loss: 1.2115
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1390 | Val Loss: 1.1268
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1004 | Val Loss: 1.1262
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1555 | Val Loss: 1.1577
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1576 | Val Loss: 1.1541
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1425 | Val Loss: 1.1327
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1307 | Val Loss: 1.1160
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1170 | Val Loss: 1.1323
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1486 | Val Loss: 1.1631
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16783
wandb:   val_loss 1.16275
wandb: 
wandb: üöÄ View run smooth-sweep-646 at: https://wandb.ai/7shoe/domShift-extensive/runs/nxv2ftf8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184238-nxv2ftf8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1678 | Val Loss: 1.1628
2025-03-26 18:43:23,641 - wandb.wandb_agent - INFO - Cleaning up finished run: nxv2ftf8
2025-03-26 18:43:24,249 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:43:24,249 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:43:24,252 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.2
2025-03-26 18:43:29,264 - wandb.wandb_agent - INFO - Running runs: ['e4f8dv50']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184330-e4f8dv50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-652
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/e4f8dv50
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: e4f8dv50
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 3.5183 | Val Loss: 3.1615
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 3.0247 | Val Loss: 2.8395
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.7777 | Val Loss: 2.6598
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.6165 | Val Loss: 2.5073
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 2.4652 | Val Loss: 2.3874
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 2.3621 | Val Loss: 2.3095
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 2.2887 | Val Loss: 2.2487
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.2468 | Val Loss: 2.2479
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.2591 | Val Loss: 2.2700
wandb: - 32.868 MB of 32.868 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.28206
wandb:   val_loss 2.29186
wandb: 
wandb: üöÄ View run copper-sweep-652 at: https://wandb.ai/7shoe/domShift-extensive/runs/e4f8dv50
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184330-e4f8dv50/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.2821 | Val Loss: 2.2919
2025-03-26 18:43:54,618 - wandb.wandb_agent - INFO - Cleaning up finished run: e4f8dv50
2025-03-26 18:43:55,054 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:43:55,054 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:43:55,057 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:44:00,069 - wandb.wandb_agent - INFO - Running runs: ['4rdh3lz6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184401-4rdh3lz6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-655
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4rdh3lz6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4rdh3lz6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.6602 | Val Loss: 1.4349
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.4365 | Val Loss: 1.4228
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.3562 | Val Loss: 1.2556
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.1935 | Val Loss: 1.1463
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.1246 | Val Loss: 1.0866
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0804 | Val Loss: 1.0756
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0746 | Val Loss: 1.0783
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0861 | Val Loss: 1.0954
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.1023 | Val Loss: 1.0937
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09117
wandb:   val_loss 1.09832
wandb: 
wandb: üöÄ View run icy-sweep-655 at: https://wandb.ai/7shoe/domShift-extensive/runs/4rdh3lz6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184401-4rdh3lz6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.0912 | Val Loss: 1.0983
2025-03-26 18:44:40,708 - wandb.wandb_agent - INFO - Cleaning up finished run: 4rdh3lz6
2025-03-26 18:44:41,349 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:44:41,349 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:44:41,352 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:44:46,364 - wandb.wandb_agent - INFO - Running runs: ['8nj8a4bk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184447-8nj8a4bk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-660
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8nj8a4bk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 8nj8a4bk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.2646 | Val Loss: 1.3383
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.4112 | Val Loss: 1.4315
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.3595 | Val Loss: 1.0577
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.0239 | Val Loss: 0.9731
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 0.9765 | Val Loss: 0.9787
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 0.9809 | Val Loss: 0.9785
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 0.9807 | Val Loss: 0.9810
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 0.9812 | Val Loss: 0.9822
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 0.9819 | Val Loss: 0.9827
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.9826
wandb:   val_loss 0.98115
wandb: 
wandb: üöÄ View run quiet-sweep-660 at: https://wandb.ai/7shoe/domShift-extensive/runs/8nj8a4bk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184447-8nj8a4bk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 0.9826 | Val Loss: 0.9811
2025-03-26 18:45:57,385 - wandb.wandb_agent - INFO - Cleaning up finished run: 8nj8a4bk
2025-03-26 18:45:57,892 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:45:57,892 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:45:57,896 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:46:02,907 - wandb.wandb_agent - INFO - Running runs: ['o38e43cy']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184603-o38e43cy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-667
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o38e43cy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o38e43cy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1978 | Val Loss: 1.1434
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3026 | Val Loss: 1.2380
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2132 | Val Loss: 1.1740
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1594 | Val Loss: 1.1555
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1868 | Val Loss: 1.1999
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1656 | Val Loss: 1.1963
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2003 | Val Loss: 1.1276
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2072 | Val Loss: 1.2994
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3006 | Val Loss: 1.3047
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñà
wandb:   val_loss ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3033
wandb:   val_loss 1.30518
wandb: 
wandb: üöÄ View run colorful-sweep-667 at: https://wandb.ai/7shoe/domShift-extensive/runs/o38e43cy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184603-o38e43cy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3033 | Val Loss: 1.3052
2025-03-26 18:46:58,710 - wandb.wandb_agent - INFO - Cleaning up finished run: o38e43cy
2025-03-26 18:46:59,218 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:46:59,218 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:46:59,221 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimCLR --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:47:04,233 - wandb.wandb_agent - INFO - Running runs: ['tkyolffc']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184705-tkyolffc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-673
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tkyolffc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tkyolffc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 4.0289 | Val Loss: 3.4330
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 3.1522 | Val Loss: 2.9093
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.8148 | Val Loss: 2.6568
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.6095 | Val Loss: 2.5821
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.5024 | Val Loss: 2.4115
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.4377 | Val Loss: 2.4275
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.3952 | Val Loss: 2.3438
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.3390 | Val Loss: 2.3036
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.3172 | Val Loss: 2.3054
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.014 MB uploadedwandb: | 32.995 MB of 33.014 MB uploadedwandb: / 33.014 MB of 33.014 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.29664
wandb:   val_loss 2.254
wandb: 
wandb: üöÄ View run fiery-sweep-673 at: https://wandb.ai/7shoe/domShift-extensive/runs/tkyolffc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184705-tkyolffc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.2966 | Val Loss: 2.2540
2025-03-26 18:47:44,801 - wandb.wandb_agent - INFO - Cleaning up finished run: tkyolffc
2025-03-26 18:47:45,355 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:47:45,355 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:47:45,358 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:47:50,370 - wandb.wandb_agent - INFO - Running runs: ['43grmn0v']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184751-43grmn0v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-678
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/43grmn0v
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 43grmn0v
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9245 | Val Loss: 1.8964
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6023 | Val Loss: 1.3486
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2788 | Val Loss: 1.2001
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2065 | Val Loss: 1.3003
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4063 | Val Loss: 1.4490
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4467 | Val Loss: 1.5092
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3421 | Val Loss: 1.1609
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1298 | Val Loss: 1.0953
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0718 | Val Loss: 1.0654
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06007
wandb:   val_loss 1.05895
wandb: 
wandb: üöÄ View run fancy-sweep-678 at: https://wandb.ai/7shoe/domShift-extensive/runs/43grmn0v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184751-43grmn0v/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0601 | Val Loss: 1.0590
2025-03-26 18:48:30,909 - wandb.wandb_agent - INFO - Cleaning up finished run: 43grmn0v
2025-03-26 18:48:31,714 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:48:31,715 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:48:31,717 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:48:36,730 - wandb.wandb_agent - INFO - Running runs: ['jnru4i9k']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184836-jnru4i9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-682
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jnru4i9k
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: jnru4i9k
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5096 | Val Loss: 1.4782
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4854 | Val Loss: 1.4746
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4533 | Val Loss: 1.4218
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4039 | Val Loss: 1.3821
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3714 | Val Loss: 1.3609
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3589 | Val Loss: 1.3562
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3555 | Val Loss: 1.3538
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3550 | Val Loss: 1.3551
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3588 | Val Loss: 1.3641
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36981
wandb:   val_loss 1.36399
wandb: 
wandb: üöÄ View run sweepy-sweep-682 at: https://wandb.ai/7shoe/domShift-extensive/runs/jnru4i9k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184836-jnru4i9k/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3698 | Val Loss: 1.3640
2025-03-26 18:49:12,224 - wandb.wandb_agent - INFO - Cleaning up finished run: jnru4i9k
2025-03-26 18:49:13,420 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:49:13,420 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:49:13,426 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:49:18,440 - wandb.wandb_agent - INFO - Running runs: ['5y1ofm0f']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184918-5y1ofm0f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-688
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5y1ofm0f
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5y1ofm0f
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5760 | Val Loss: 1.2761
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1853 | Val Loss: 1.1155
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1078 | Val Loss: 1.1181
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1477 | Val Loss: 1.1763
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1929 | Val Loss: 1.2016
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2050 | Val Loss: 1.2095
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2213 | Val Loss: 1.2328
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2456 | Val Loss: 1.2499
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2515 | Val Loss: 1.2544
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25007
wandb:   val_loss 1.24959
wandb: 
wandb: üöÄ View run solar-sweep-688 at: https://wandb.ai/7shoe/domShift-extensive/runs/5y1ofm0f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184918-5y1ofm0f/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2501 | Val Loss: 1.2496
2025-03-26 18:50:04,110 - wandb.wandb_agent - INFO - Cleaning up finished run: 5y1ofm0f
2025-03-26 18:50:05,170 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:50:05,170 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:50:05,174 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:50:10,187 - wandb.wandb_agent - INFO - Running runs: ['glnrkwb7']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185010-glnrkwb7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-693
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/glnrkwb7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: glnrkwb7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5888 | Val Loss: 1.3014
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1299 | Val Loss: 1.0171
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0382 | Val Loss: 1.0372
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0358 | Val Loss: 1.0345
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0358 | Val Loss: 1.0355
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0357 | Val Loss: 1.0354
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0357 | Val Loss: 1.0354
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0357 | Val Loss: 1.0353
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0356 | Val Loss: 1.0353
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0356
wandb:   val_loss 1.03516
wandb: 
wandb: üöÄ View run sweepy-sweep-693 at: https://wandb.ai/7shoe/domShift-extensive/runs/glnrkwb7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185010-glnrkwb7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0356 | Val Loss: 1.0352
2025-03-26 18:51:21,175 - wandb.wandb_agent - INFO - Cleaning up finished run: glnrkwb7
2025-03-26 18:51:21,749 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:51:21,749 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:51:21,752 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:51:26,766 - wandb.wandb_agent - INFO - Running runs: ['u52ltsgb']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185126-u52ltsgb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-701
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u52ltsgb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u52ltsgb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7984 | Val Loss: 1.3543
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2710 | Val Loss: 1.3477
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4409 | Val Loss: 1.4927
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5094 | Val Loss: 1.5335
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5350 | Val Loss: 1.3870
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1273 | Val Loss: 1.0473
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1087 | Val Loss: 1.1582
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1616 | Val Loss: 1.1138
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1298 | Val Loss: 1.1291
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 33.088 MB uploadedwandb: | 32.995 MB of 33.088 MB uploadedwandb: / 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15559
wandb:   val_loss 1.18759
wandb: 
wandb: üöÄ View run major-sweep-701 at: https://wandb.ai/7shoe/domShift-extensive/runs/u52ltsgb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185126-u52ltsgb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1556 | Val Loss: 1.1876
2025-03-26 18:52:12,399 - wandb.wandb_agent - INFO - Cleaning up finished run: u52ltsgb
2025-03-26 18:52:12,988 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:52:12,988 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:52:12,990 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:52:18,003 - wandb.wandb_agent - INFO - Running runs: ['l9ljt6ci']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185217-l9ljt6ci
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-706
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l9ljt6ci
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: l9ljt6ci
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0631 | Val Loss: 1.0163
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0063 | Val Loss: 1.0004
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0132 | Val Loss: 1.0163
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0118 | Val Loss: 1.0114
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0112 | Val Loss: 1.0113
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0113 | Val Loss: 1.0113
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0115 | Val Loss: 1.0115
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0117 | Val Loss: 1.0121
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0120 | Val Loss: 1.0129
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0118
wandb:   val_loss 1.01261
wandb: 
wandb: üöÄ View run misunderstood-sweep-706 at: https://wandb.ai/7shoe/domShift-extensive/runs/l9ljt6ci
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185217-l9ljt6ci/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0118 | Val Loss: 1.0126
2025-03-26 18:53:34,066 - wandb.wandb_agent - INFO - Cleaning up finished run: l9ljt6ci
2025-03-26 18:53:34,977 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:53:34,977 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:53:34,979 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:53:39,992 - wandb.wandb_agent - INFO - Running runs: ['iqwzuwff']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185339-iqwzuwff
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-715
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/iqwzuwff
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: iqwzuwff
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7667 | Val Loss: 1.4622
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3441 | Val Loss: 1.2495
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2239 | Val Loss: 1.2030
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2114 | Val Loss: 1.2241
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2423 | Val Loss: 1.2722
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3011 | Val Loss: 1.3388
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3550 | Val Loss: 1.3595
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3498 | Val Loss: 1.3274
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3061 | Val Loss: 1.2792
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.2568
wandb:   val_loss 1.2314
wandb: 
wandb: üöÄ View run denim-sweep-715 at: https://wandb.ai/7shoe/domShift-extensive/runs/iqwzuwff
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185339-iqwzuwff/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2568 | Val Loss: 1.2314
2025-03-26 18:54:10,416 - wandb.wandb_agent - INFO - Cleaning up finished run: iqwzuwff
2025-03-26 18:54:11,625 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:54:11,625 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:54:11,628 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:54:16,640 - wandb.wandb_agent - INFO - Running runs: ['sprdc29g']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185416-sprdc29g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-719
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sprdc29g
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sprdc29g
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3361 | Val Loss: 1.3139
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3173 | Val Loss: 1.3772
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5546 | Val Loss: 1.5509
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5003 | Val Loss: 1.4440
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4528 | Val Loss: 1.4663
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4677 | Val Loss: 1.4731
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4702 | Val Loss: 1.4727
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4688 | Val Loss: 1.4689
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4558 | Val Loss: 1.4558
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.44967
wandb:   val_loss 1.44993
wandb: 
wandb: üöÄ View run still-sweep-719 at: https://wandb.ai/7shoe/domShift-extensive/runs/sprdc29g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185416-sprdc29g/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4497 | Val Loss: 1.4499
2025-03-26 18:55:02,272 - wandb.wandb_agent - INFO - Cleaning up finished run: sprdc29g
2025-03-26 18:55:03,200 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:55:03,201 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.9
2025-03-26 18:55:03,204 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.9
2025-03-26 18:55:08,216 - wandb.wandb_agent - INFO - Running runs: ['l9kwayw6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185509-l9kwayw6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-725
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l9kwayw6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: l9kwayw6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8097 | Val Loss: 2.7680
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6445 | Val Loss: 2.6576
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5641 | Val Loss: 2.2827
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1952 | Val Loss: 2.1596
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1543 | Val Loss: 2.1547
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1559 | Val Loss: 2.1593
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1606 | Val Loss: 2.1628
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1635 | Val Loss: 2.1660
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1665 | Val Loss: 2.1687
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.773 MB uploadedwandb: | 43.729 MB of 43.773 MB uploadedwandb: / 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.16891
wandb:   val_loss 2.17119
wandb: 
wandb: üöÄ View run lucky-sweep-725 at: https://wandb.ai/7shoe/domShift-extensive/runs/l9kwayw6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185509-l9kwayw6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1689 | Val Loss: 2.1712
2025-03-26 18:55:53,922 - wandb.wandb_agent - INFO - Cleaning up finished run: l9kwayw6
2025-03-26 18:56:14,002 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 18:56:18,257 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:56:18,257 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:56:18,260 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:56:23,273 - wandb.wandb_agent - INFO - Running runs: ['xlkhtk9m']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185624-xlkhtk9m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-731
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/xlkhtk9m
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: xlkhtk9m
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0189 | Val Loss: 1.9095
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.9068 | Val Loss: 1.9655
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.9205 | Val Loss: 1.7685
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6439 | Val Loss: 1.5324
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4815 | Val Loss: 1.4853
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4881 | Val Loss: 1.4905
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4998 | Val Loss: 1.5134
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5232 | Val Loss: 1.5311
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5307 | Val Loss: 1.5298
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñá‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.52943
wandb:   val_loss 1.53146
wandb: 
wandb: üöÄ View run lively-sweep-731 at: https://wandb.ai/7shoe/domShift-extensive/runs/xlkhtk9m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185624-xlkhtk9m/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5294 | Val Loss: 1.5315
2025-03-26 18:57:03,821 - wandb.wandb_agent - INFO - Cleaning up finished run: xlkhtk9m
2025-03-26 18:57:04,437 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:04,437 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:57:04,440 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:57:09,452 - wandb.wandb_agent - INFO - Running runs: ['88xpxigq']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185709-88xpxigq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-736
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/88xpxigq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 88xpxigq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.7562 | Val Loss: 2.3378
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1067 | Val Loss: 1.9191
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7859 | Val Loss: 1.6998
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6279 | Val Loss: 1.5854
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5465 | Val Loss: 1.5395
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5067 | Val Loss: 1.4932
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4818 | Val Loss: 1.4967
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4950 | Val Loss: 1.5167
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5175 | Val Loss: 1.5384
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.53932
wandb:   val_loss 1.56004
wandb: 
wandb: üöÄ View run upbeat-sweep-736 at: https://wandb.ai/7shoe/domShift-extensive/runs/88xpxigq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185709-88xpxigq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5393 | Val Loss: 1.5600
2025-03-26 18:57:39,846 - wandb.wandb_agent - INFO - Cleaning up finished run: 88xpxigq
2025-03-26 18:57:40,446 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:40,447 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:57:40,450 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 18:57:45,464 - wandb.wandb_agent - INFO - Running runs: ['lt2ayl7a']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185746-lt2ayl7a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-740
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lt2ayl7a
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lt2ayl7a
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7669 | Val Loss: 1.5707
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5094 | Val Loss: 1.4561
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4338 | Val Loss: 1.4160
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4145 | Val Loss: 1.4118
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4113 | Val Loss: 1.4107
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4108 | Val Loss: 1.4108
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4112 | Val Loss: 1.4093
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4094 | Val Loss: 1.4089
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4096 | Val Loss: 1.4104
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.41225
wandb:   val_loss 1.41515
wandb: 
wandb: üöÄ View run dazzling-sweep-740 at: https://wandb.ai/7shoe/domShift-extensive/runs/lt2ayl7a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185746-lt2ayl7a/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4122 | Val Loss: 1.4151
2025-03-26 18:58:41,267 - wandb.wandb_agent - INFO - Cleaning up finished run: lt2ayl7a
2025-03-26 18:58:41,915 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:58:41,915 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:58:41,919 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:58:46,931 - wandb.wandb_agent - INFO - Running runs: ['423c7sxb']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185846-423c7sxb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-746
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/423c7sxb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 423c7sxb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5512 | Val Loss: 1.4194
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4626 | Val Loss: 1.5562
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5907 | Val Loss: 1.5342
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5027 | Val Loss: 1.4288
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3769 | Val Loss: 1.3708
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4451 | Val Loss: 1.3930
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0272 | Val Loss: 0.9177
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9483 | Val Loss: 0.9811
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0033 | Val Loss: 1.0132
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.99261
wandb:   val_loss 0.97512
wandb: 
wandb: üöÄ View run misunderstood-sweep-746 at: https://wandb.ai/7shoe/domShift-extensive/runs/423c7sxb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185846-423c7sxb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9926 | Val Loss: 0.9751
2025-03-26 18:59:37,660 - wandb.wandb_agent - INFO - Cleaning up finished run: 423c7sxb
2025-03-26 18:59:38,382 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:59:38,383 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:59:38,385 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:59:43,398 - wandb.wandb_agent - INFO - Running runs: ['en89thl5']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185944-en89thl5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-752
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/en89thl5
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: en89thl5
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2429 | Val Loss: 1.1193
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1012 | Val Loss: 1.0864
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0600 | Val Loss: 1.0068
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9638 | Val Loss: 0.9434
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9387 | Val Loss: 0.9345
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9369 | Val Loss: 0.9369
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9353 | Val Loss: 0.9339
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9312 | Val Loss: 0.9281
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9299 | Val Loss: 0.9369
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95365
wandb:   val_loss 0.976
wandb: 
wandb: üöÄ View run dutiful-sweep-752 at: https://wandb.ai/7shoe/domShift-extensive/runs/en89thl5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185944-en89thl5/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9536 | Val Loss: 0.9760
2025-03-26 19:00:39,174 - wandb.wandb_agent - INFO - Cleaning up finished run: en89thl5
2025-03-26 19:00:40,100 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:40,101 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:00:40,103 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 19:00:45,116 - wandb.wandb_agent - INFO - Running runs: ['c4jhcnwk']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190046-c4jhcnwk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-759
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/c4jhcnwk
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: c4jhcnwk
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9797 | Val Loss: 1.7618
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7135 | Val Loss: 1.6507
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6226 | Val Loss: 1.5994
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6045 | Val Loss: 1.6055
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6174 | Val Loss: 1.6321
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6419 | Val Loss: 1.6435
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6360 | Val Loss: 1.6139
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5911 | Val Loss: 1.5484
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5149 | Val Loss: 1.4711
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.45098
wandb:   val_loss 1.41987
wandb: 
wandb: üöÄ View run visionary-sweep-759 at: https://wandb.ai/7shoe/domShift-extensive/runs/c4jhcnwk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190046-c4jhcnwk/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4510 | Val Loss: 1.4199
2025-03-26 19:01:15,504 - wandb.wandb_agent - INFO - Cleaning up finished run: c4jhcnwk
2025-03-26 19:01:16,072 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:01:16,073 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:01:16,076 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:01:21,088 - wandb.wandb_agent - INFO - Running runs: ['skvh6zab']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190120-skvh6zab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-764
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/skvh6zab
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: skvh6zab
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6835 | Val Loss: 1.5412
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4578 | Val Loss: 1.3724
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3262 | Val Loss: 1.2892
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2859 | Val Loss: 1.2731
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2513 | Val Loss: 1.2254
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1961 | Val Loss: 1.1457
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0827 | Val Loss: 1.0158
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0004 | Val Loss: 1.0068
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0288 | Val Loss: 1.0524
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07988
wandb:   val_loss 1.11369
wandb: 
wandb: üöÄ View run polished-sweep-764 at: https://wandb.ai/7shoe/domShift-extensive/runs/skvh6zab
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190120-skvh6zab/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0799 | Val Loss: 1.1137
2025-03-26 19:01:51,578 - wandb.wandb_agent - INFO - Cleaning up finished run: skvh6zab
2025-03-26 19:01:52,177 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:01:52,177 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:01:52,180 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimCLR --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 19:01:57,192 - wandb.wandb_agent - INFO - Running runs: ['nhzwup85']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190158-nhzwup85
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-768
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nhzwup85
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: nhzwup85
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.1354 | Val Loss: 4.2759
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.1383 | Val Loss: 3.5854
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.6075 | Val Loss: 3.2870
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.1835 | Val Loss: 2.8993
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.9319 | Val Loss: 2.7267
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.7637 | Val Loss: 2.5079
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6412 | Val Loss: 2.3202
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4597 | Val Loss: 2.1842
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3511 | Val Loss: 2.0762
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.24157
wandb:   val_loss 1.99971
wandb: 
wandb: üöÄ View run electric-sweep-768 at: https://wandb.ai/7shoe/domShift-extensive/runs/nhzwup85
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190158-nhzwup85/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2416 | Val Loss: 1.9997
2025-03-26 19:02:42,831 - wandb.wandb_agent - INFO - Cleaning up finished run: nhzwup85
2025-03-26 19:02:43,444 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:02:43,444 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:02:43,447 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:02:48,459 - wandb.wandb_agent - INFO - Running runs: ['gfp13jak']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190248-gfp13jak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-774
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gfp13jak
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: gfp13jak
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3390 | Val Loss: 1.1001
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0226 | Val Loss: 0.9898
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9716 | Val Loss: 0.9516
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9909 | Val Loss: 1.0572
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1157 | Val Loss: 1.1933
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1899 | Val Loss: 1.1715
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1502 | Val Loss: 1.1452
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1652 | Val Loss: 1.1976
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2304 | Val Loss: 1.2588
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ
wandb:   val_loss ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22427
wandb:   val_loss 1.14702
wandb: 
wandb: üöÄ View run bumbling-sweep-774 at: https://wandb.ai/7shoe/domShift-extensive/runs/gfp13jak
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190248-gfp13jak/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2243 | Val Loss: 1.1470
2025-03-26 19:03:34,142 - wandb.wandb_agent - INFO - Cleaning up finished run: gfp13jak
2025-03-26 19:03:34,676 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:03:34,677 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:03:34,679 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 19:03:39,692 - wandb.wandb_agent - INFO - Running runs: ['ph3j5djl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190340-ph3j5djl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-781
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ph3j5djl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ph3j5djl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9681 | Val Loss: 2.6651
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5074 | Val Loss: 2.4187
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4183 | Val Loss: 2.4322
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4507 | Val Loss: 2.4684
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4847 | Val Loss: 2.4992
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5125 | Val Loss: 2.5248
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5372 | Val Loss: 2.5491
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5613 | Val Loss: 2.5732
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5852 | Val Loss: 2.5969
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.60867
wandb:   val_loss 2.62022
wandb: 
wandb: üöÄ View run chocolate-sweep-781 at: https://wandb.ai/7shoe/domShift-extensive/runs/ph3j5djl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190340-ph3j5djl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.6087 | Val Loss: 2.6202
2025-03-26 19:04:40,558 - wandb.wandb_agent - INFO - Cleaning up finished run: ph3j5djl
2025-03-26 19:04:41,153 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:04:41,153 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:04:41,156 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 19:04:46,169 - wandb.wandb_agent - INFO - Running runs: ['u69g9dyu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190446-u69g9dyu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-788
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u69g9dyu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u69g9dyu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1601 | Val Loss: 0.9607
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9379 | Val Loss: 0.8764
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8732 | Val Loss: 0.8834
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8847 | Val Loss: 0.8857
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8855 | Val Loss: 0.8854
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8851 | Val Loss: 0.8856
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8849 | Val Loss: 0.8848
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8848 | Val Loss: 0.8849
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8849 | Val Loss: 0.8853
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.88492
wandb:   val_loss 0.8848
wandb: 
wandb: üöÄ View run vague-sweep-788 at: https://wandb.ai/7shoe/domShift-extensive/runs/u69g9dyu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190446-u69g9dyu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8849 | Val Loss: 0.8848
2025-03-26 19:05:36,910 - wandb.wandb_agent - INFO - Cleaning up finished run: u69g9dyu
2025-03-26 19:05:37,521 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:05:37,521 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:05:37,524 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 19:05:42,536 - wandb.wandb_agent - INFO - Running runs: ['pqhc2g12']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190543-pqhc2g12
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-791
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pqhc2g12
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pqhc2g12
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6935 | Val Loss: 1.4633
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5039 | Val Loss: 1.5455
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5844 | Val Loss: 1.6386
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.6864 | Val Loss: 1.6901
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.6994 | Val Loss: 1.5887
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5370 | Val Loss: 1.5026
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4868 | Val Loss: 1.4835
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5129 | Val Loss: 1.4985
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4878 | Val Loss: 1.4755
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÖ‚ñà‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.46354
wandb:   val_loss 1.45092
wandb: 
wandb: üöÄ View run glamorous-sweep-791 at: https://wandb.ai/7shoe/domShift-extensive/runs/pqhc2g12
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190543-pqhc2g12/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4635 | Val Loss: 1.4509
2025-03-26 19:06:33,237 - wandb.wandb_agent - INFO - Cleaning up finished run: pqhc2g12
2025-03-26 19:06:33,751 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:06:33,751 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:06:33,754 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:06:38,767 - wandb.wandb_agent - INFO - Running runs: ['5uto0abu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190638-5uto0abu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-798
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5uto0abu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5uto0abu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 5.3973 | Val Loss: 4.8680
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 4.9157 | Val Loss: 4.5774
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 4.6742 | Val Loss: 4.3924
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 4.5816 | Val Loss: 4.3575
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 4.5211 | Val Loss: 4.2817
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 4.4901 | Val Loss: 4.2502
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 4.4593 | Val Loss: 4.2433
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 4.4368 | Val Loss: 4.2280
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 4.4158 | Val Loss: 4.1702
wandb: - 32.854 MB of 32.854 MB uploadedwandb: \ 32.854 MB of 32.854 MB uploadedwandb: | 32.947 MB of 32.947 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 4.38816
wandb:   val_loss 4.16121
wandb: 
wandb: üöÄ View run fine-sweep-798 at: https://wandb.ai/7shoe/domShift-extensive/runs/5uto0abu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190638-5uto0abu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 4.3882 | Val Loss: 4.1612
2025-03-26 19:07:24,439 - wandb.wandb_agent - INFO - Cleaning up finished run: 5uto0abu
2025-03-26 19:07:24,882 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:07:24,883 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:07:24,886 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:07:29,900 - wandb.wandb_agent - INFO - Running runs: ['bc5bcbqw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190730-bc5bcbqw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-803
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bc5bcbqw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bc5bcbqw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5188 | Val Loss: 1.2797
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1883 | Val Loss: 1.0478
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0531 | Val Loss: 1.0704
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0646 | Val Loss: 1.0632
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0584 | Val Loss: 1.0703
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1350 | Val Loss: 1.1970
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1178 | Val Loss: 1.0181
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0502 | Val Loss: 1.1638
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1598 | Val Loss: 1.1251
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÖ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12091
wandb:   val_loss 1.1268
wandb: 
wandb: üöÄ View run fresh-sweep-803 at: https://wandb.ai/7shoe/domShift-extensive/runs/bc5bcbqw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190730-bc5bcbqw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1209 | Val Loss: 1.1268
2025-03-26 19:08:25,708 - wandb.wandb_agent - INFO - Cleaning up finished run: bc5bcbqw
2025-03-26 19:08:26,281 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:08:26,281 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:08:26,284 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:08:31,296 - wandb.wandb_agent - INFO - Running runs: ['4bkdffeu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190832-4bkdffeu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-807
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4bkdffeu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4bkdffeu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9509 | Val Loss: 1.6594
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4229 | Val Loss: 1.2654
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2153 | Val Loss: 1.2268
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2941 | Val Loss: 1.2875
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3051 | Val Loss: 1.0693
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9372 | Val Loss: 0.9139
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9237 | Val Loss: 0.9320
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9435 | Val Loss: 0.9510
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9504 | Val Loss: 0.9490
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.991 MB uploadedwandb: | 32.971 MB of 32.991 MB uploadedwandb: / 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95216
wandb:   val_loss 0.9531
wandb: 
wandb: üöÄ View run earnest-sweep-807 at: https://wandb.ai/7shoe/domShift-extensive/runs/4bkdffeu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190832-4bkdffeu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9522 | Val Loss: 0.9531
2025-03-26 19:09:11,850 - wandb.wandb_agent - INFO - Cleaning up finished run: 4bkdffeu
2025-03-26 19:09:12,411 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:09:12,411 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:09:12,414 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:09:17,428 - wandb.wandb_agent - INFO - Running runs: ['w4butezq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190918-w4butezq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-814
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/w4butezq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: w4butezq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4384 | Val Loss: 1.3304
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3617 | Val Loss: 1.3635
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1923 | Val Loss: 1.1010
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0834 | Val Loss: 1.1842
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1304 | Val Loss: 1.0244
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0127 | Val Loss: 1.0085
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0209 | Val Loss: 1.0359
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0447 | Val Loss: 1.0460
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0395 | Val Loss: 1.0333
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.333 MB uploadedwandb: | 137.314 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03101
wandb:   val_loss 1.02797
wandb: 
wandb: üöÄ View run quiet-sweep-814 at: https://wandb.ai/7shoe/domShift-extensive/runs/w4butezq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190918-w4butezq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0310 | Val Loss: 1.0280
2025-03-26 19:10:13,195 - wandb.wandb_agent - INFO - Cleaning up finished run: w4butezq
2025-03-26 19:10:13,686 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:13,686 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:10:13,689 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:10:18,702 - wandb.wandb_agent - INFO - Running runs: ['7spozhgi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191020-7spozhgi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-821
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7spozhgi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7spozhgi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5665 | Val Loss: 1.3041
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2228 | Val Loss: 1.1561
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1219 | Val Loss: 1.1139
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1377 | Val Loss: 1.1554
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1753 | Val Loss: 1.1973
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2148 | Val Loss: 1.2215
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2235 | Val Loss: 1.2324
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2457 | Val Loss: 1.2439
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2141 | Val Loss: 1.2011
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.234 MB of 137.234 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20684
wandb:   val_loss 1.22031
wandb: 
wandb: üöÄ View run jumping-sweep-821 at: https://wandb.ai/7shoe/domShift-extensive/runs/7spozhgi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191020-7spozhgi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2068 | Val Loss: 1.2203
2025-03-26 19:11:24,672 - wandb.wandb_agent - INFO - Cleaning up finished run: 7spozhgi
2025-03-26 19:11:25,559 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:11:25,559 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:11:25,562 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 19:11:30,574 - wandb.wandb_agent - INFO - Running runs: ['goyrtp7q']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191130-goyrtp7q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-827
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/goyrtp7q
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: goyrtp7q
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9139 | Val Loss: 2.6296
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3994 | Val Loss: 2.2542
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2101 | Val Loss: 2.1730
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1574 | Val Loss: 2.1341
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1116 | Val Loss: 2.0827
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0577 | Val Loss: 2.0279
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.9984 | Val Loss: 1.9672
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.9455 | Val Loss: 1.9246
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9090 | Val Loss: 1.8927
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.88138
wandb:   val_loss 1.86998
wandb: 
wandb: üöÄ View run volcanic-sweep-827 at: https://wandb.ai/7shoe/domShift-extensive/runs/goyrtp7q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191130-goyrtp7q/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.8814 | Val Loss: 1.8700
2025-03-26 19:12:21,297 - wandb.wandb_agent - INFO - Cleaning up finished run: goyrtp7q
2025-03-26 19:12:21,858 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:12:21,858 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:12:21,861 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:12:26,873 - wandb.wandb_agent - INFO - Running runs: ['uffhdp6d']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191228-uffhdp6d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-835
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uffhdp6d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: uffhdp6d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9193 | Val Loss: 2.5207
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3409 | Val Loss: 2.2799
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2997 | Val Loss: 2.3255
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1605 | Val Loss: 2.1793
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1915 | Val Loss: 2.1670
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0297 | Val Loss: 1.8962
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0005 | Val Loss: 2.0509
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0284 | Val Loss: 1.9909
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9511 | Val Loss: 1.9089
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.031 MB of 137.031 MB uploadedwandb: | 137.190 MB of 137.190 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.8694
wandb:   val_loss 1.81104
wandb: 
wandb: üöÄ View run misunderstood-sweep-835 at: https://wandb.ai/7shoe/domShift-extensive/runs/uffhdp6d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191228-uffhdp6d/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.8694 | Val Loss: 1.8110
2025-03-26 19:13:42,952 - wandb.wandb_agent - INFO - Cleaning up finished run: uffhdp6d
2025-03-26 19:13:44,764 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:13:44,764 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:13:44,767 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 19:13:49,779 - wandb.wandb_agent - INFO - Running runs: ['4adk119c']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191350-4adk119c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-842
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4adk119c
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4adk119c
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4561 | Val Loss: 1.1533
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1334 | Val Loss: 1.1143
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1272 | Val Loss: 1.1041
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0850 | Val Loss: 1.0772
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0873 | Val Loss: 1.0987
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1069 | Val Loss: 1.1146
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1172 | Val Loss: 1.1167
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1137 | Val Loss: 1.1129
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1156 | Val Loss: 1.1189
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.473 MB uploadedwandb: | 137.314 MB of 137.473 MB uploadedwandb: / 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11909
wandb:   val_loss 1.11796
wandb: 
wandb: üöÄ View run laced-sweep-842 at: https://wandb.ai/7shoe/domShift-extensive/runs/4adk119c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191350-4adk119c/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1191 | Val Loss: 1.1180
2025-03-26 19:15:00,716 - wandb.wandb_agent - INFO - Cleaning up finished run: 4adk119c
2025-03-26 19:15:01,236 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:15:01,237 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:15:01,239 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:15:06,251 - wandb.wandb_agent - INFO - Running runs: ['94abt90m']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191506-94abt90m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-852
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/94abt90m
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 94abt90m
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5466 | Val Loss: 1.4146
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3630 | Val Loss: 1.2897
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2866 | Val Loss: 1.2965
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2972 | Val Loss: 1.3025
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3072 | Val Loss: 1.3103
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3137 | Val Loss: 1.3171
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3179 | Val Loss: 1.3185
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3188 | Val Loss: 1.3193
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3195 | Val Loss: 1.3198
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31979
wandb:   val_loss 1.32001
wandb: 
wandb: üöÄ View run twilight-sweep-852 at: https://wandb.ai/7shoe/domShift-extensive/runs/94abt90m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191506-94abt90m/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3198 | Val Loss: 1.3200
2025-03-26 19:16:22,291 - wandb.wandb_agent - INFO - Cleaning up finished run: 94abt90m
2025-03-26 19:16:22,790 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:16:22,790 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:16:22,794 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 19:16:27,806 - wandb.wandb_agent - INFO - Running runs: ['ucqesuo8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191629-ucqesuo8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-859
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ucqesuo8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ucqesuo8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9147 | Val Loss: 1.7687
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7587 | Val Loss: 1.7657
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7896 | Val Loss: 1.8350
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8562 | Val Loss: 1.8587
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8348 | Val Loss: 1.8081
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8073 | Val Loss: 1.8118
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8296 | Val Loss: 1.8656
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.8599 | Val Loss: 1.8230
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.7910 | Val Loss: 1.7412
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.095 MB uploadedwandb: | 137.075 MB of 137.095 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÉ‚ñá‚ñà‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.73787
wandb:   val_loss 1.72085
wandb: 
wandb: üöÄ View run lively-sweep-859 at: https://wandb.ai/7shoe/domShift-extensive/runs/ucqesuo8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191629-ucqesuo8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.7379 | Val Loss: 1.7209
2025-03-26 19:16:58,229 - wandb.wandb_agent - INFO - Cleaning up finished run: ucqesuo8
2025-03-26 19:16:59,005 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:16:59,006 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:16:59,008 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:17:04,019 - wandb.wandb_agent - INFO - Running runs: ['zh9a8ej8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191705-zh9a8ej8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-863
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/zh9a8ej8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: zh9a8ej8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2224 | Val Loss: 1.0236
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9712 | Val Loss: 0.9977
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1130 | Val Loss: 1.2401
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2666 | Val Loss: 1.2833
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2857 | Val Loss: 1.2939
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3068 | Val Loss: 1.3187
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3242 | Val Loss: 1.3277
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3324 | Val Loss: 1.3389
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3434 | Val Loss: 1.3432
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.095 MB uploadedwandb: | 137.075 MB of 137.095 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34082
wandb:   val_loss 1.3246
wandb: 
wandb: üöÄ View run kind-sweep-863 at: https://wandb.ai/7shoe/domShift-extensive/runs/zh9a8ej8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191705-zh9a8ej8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3408 | Val Loss: 1.3246
2025-03-26 19:17:49,619 - wandb.wandb_agent - INFO - Cleaning up finished run: zh9a8ej8
2025-03-26 19:17:50,312 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:17:50,312 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:17:50,315 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 19:17:55,327 - wandb.wandb_agent - INFO - Running runs: ['7s8dxx0e']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191756-7s8dxx0e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-868
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7s8dxx0e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7s8dxx0e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 3.0995 | Val Loss: 2.6118
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.4279 | Val Loss: 2.2348
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.0926 | Val Loss: 1.9677
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.9356 | Val Loss: 1.9225
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.9497 | Val Loss: 1.9861
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.0177 | Val Loss: 2.0502
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.0740 | Val Loss: 2.0982
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.1169 | Val Loss: 2.1382
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.1546 | Val Loss: 2.1709
wandb: - 137.025 MB of 137.025 MB uploadedwandb: \ 137.025 MB of 137.025 MB uploadedwandb: | 137.045 MB of 137.045 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.18279
wandb:   val_loss 2.19463
wandb: 
wandb: üöÄ View run fiery-sweep-868 at: https://wandb.ai/7shoe/domShift-extensive/runs/7s8dxx0e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191756-7s8dxx0e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.1828 | Val Loss: 2.1946
2025-03-26 19:18:35,885 - wandb.wandb_agent - INFO - Cleaning up finished run: 7s8dxx0e
2025-03-26 19:18:36,711 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:18:36,711 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 19:18:36,714 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:18:41,726 - wandb.wandb_agent - INFO - Running runs: ['qk0qeemu']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191841-qk0qeemu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-873
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qk0qeemu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qk0qeemu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7494 | Val Loss: 1.4873
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5499 | Val Loss: 1.4683
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3430 | Val Loss: 1.1945
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0810 | Val Loss: 1.0016
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9878 | Val Loss: 0.9438
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9841 | Val Loss: 1.0260
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0670 | Val Loss: 1.1055
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1210 | Val Loss: 1.1196
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0733 | Val Loss: 1.0292
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0152
wandb:   val_loss 1.00889
wandb: 
wandb: üöÄ View run laced-sweep-873 at: https://wandb.ai/7shoe/domShift-extensive/runs/qk0qeemu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191841-qk0qeemu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0152 | Val Loss: 1.0089
2025-03-26 19:19:22,307 - wandb.wandb_agent - INFO - Cleaning up finished run: qk0qeemu
2025-03-26 19:19:23,534 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:23,534 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.9
2025-03-26 19:19:23,537 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.9
2025-03-26 19:19:28,548 - wandb.wandb_agent - INFO - Running runs: ['dpgida20']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191929-dpgida20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-879
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dpgida20
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dpgida20
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.1732 | Val Loss: 2.7500
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.6203 | Val Loss: 2.4437
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.3676 | Val Loss: 2.3315
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.3429 | Val Loss: 2.3642
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.3788 | Val Loss: 2.4071
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.4407 | Val Loss: 2.4721
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.4978 | Val Loss: 2.5214
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.5392 | Val Loss: 2.5535
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.5608 | Val Loss: 2.5640
wandb: - 32.968 MB of 32.968 MB uploadedwandb: \ 32.968 MB of 32.968 MB uploadedwandb: | 32.987 MB of 32.987 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.55957
wandb:   val_loss 2.55051
wandb: 
wandb: üöÄ View run rose-sweep-879 at: https://wandb.ai/7shoe/domShift-extensive/runs/dpgida20
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191929-dpgida20/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.5596 | Val Loss: 2.5505
2025-03-26 19:20:09,128 - wandb.wandb_agent - INFO - Cleaning up finished run: dpgida20
2025-03-26 19:20:09,756 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:20:09,756 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:20:09,759 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:20:14,771 - wandb.wandb_agent - INFO - Running runs: ['24vz0mex']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_192015-24vz0mex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-884
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/24vz0mex
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 24vz0mex
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5078 | Val Loss: 1.2354
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2609 | Val Loss: 1.3230
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4201 | Val Loss: 1.5108
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4863 | Val Loss: 1.3941
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2875 | Val Loss: 1.2451
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2465 | Val Loss: 1.2501
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2549 | Val Loss: 1.2586
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2574 | Val Loss: 1.2556
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2545 | Val Loss: 1.2542
wandb: - 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÜ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25427
wandb:   val_loss 1.25395
wandb: 
wandb: üöÄ View run pretty-sweep-884 at: https://wandb.ai/7shoe/domShift-extensive/runs/24vz0mex
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_192015-24vz0mex/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2543 | Val Loss: 1.2540
