nohup: ignoring input
wandb: Starting wandb agent üïµÔ∏è
2025-03-26 17:04:23,135 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 17:04:23,396 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:04:23,397 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimCLR
	model_class: cnn
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 1
2025-03-26 17:04:23,399 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimCLR --model_class=cnn --model_type=basic --n_train=8000 --optim=AdamW --temperature=1
2025-03-26 17:04:28,412 - wandb.wandb_agent - INFO - Running runs: ['duayawzg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170429-duayawzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/duayawzg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: duayawzg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 5.5327 | Val Loss: 5.3613
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 5.3525 | Val Loss: 5.3311
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 5.3254 | Val Loss: 5.3131
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 5.3121 | Val Loss: 5.3038
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 5.3070 | Val Loss: 5.2894
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 5.2986 | Val Loss: 5.2878
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 5.2925 | Val Loss: 5.2814
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 5.2916 | Val Loss: 5.2837
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 5.2897 | Val Loss: 5.2809
wandb: - 10.260 MB of 10.260 MB uploadedwandb: \ 10.260 MB of 10.260 MB uploadedwandb: | 10.279 MB of 10.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 5.28869
wandb:   val_loss 5.27846
wandb: 
wandb: üöÄ View run likely-sweep-2 at: https://wandb.ai/7shoe/domShift-extensive/runs/duayawzg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170429-duayawzg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-cnn-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 5.2887 | Val Loss: 5.2785
2025-03-26 17:05:03,899 - wandb.wandb_agent - INFO - Cleaning up finished run: duayawzg
2025-03-26 17:05:04,421 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:04,421 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: cnn
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:05:04,423 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=cnn --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.9
2025-03-26 17:05:09,436 - wandb.wandb_agent - INFO - Running runs: ['v9pd5gra']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170510-v9pd5gra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/v9pd5gra
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: v9pd5gra
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.4581 | Val Loss: 1.2986
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.1133 | Val Loss: 1.0265
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.0270 | Val Loss: 1.0200
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.0211 | Val Loss: 1.0203
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.0194 | Val Loss: 1.0185
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0178 | Val Loss: 1.0171
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0166 | Val Loss: 1.0160
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0153 | Val Loss: 1.0147
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.0142 | Val Loss: 1.0136
wandb: - 10.260 MB of 10.260 MB uploadedwandb: \ 10.260 MB of 10.260 MB uploadedwandb: | 10.290 MB of 10.290 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.01315
wandb:   val_loss 1.01271
wandb: 
wandb: üöÄ View run fancy-sweep-8 at: https://wandb.ai/7shoe/domShift-extensive/runs/v9pd5gra
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170510-v9pd5gra/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.0131 | Val Loss: 1.0127
2025-03-26 17:05:55,044 - wandb.wandb_agent - INFO - Cleaning up finished run: v9pd5gra
2025-03-26 17:05:55,518 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:05:55,519 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:05:55,521 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=BYOL --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.9
2025-03-26 17:06:00,533 - wandb.wandb_agent - INFO - Running runs: ['vev2fhb3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170602-vev2fhb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vev2fhb3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: vev2fhb3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 3.4892 | Val Loss: 3.0119
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.9573 | Val Loss: 2.9146
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.9333 | Val Loss: 2.9140
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.8956 | Val Loss: 2.8362
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.8009 | Val Loss: 2.7354
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.6988 | Val Loss: 2.6395
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.6097 | Val Loss: 2.5592
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.5358 | Val Loss: 2.4967
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.4824 | Val Loss: 2.4552
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.873 MB uploadedwandb: | 32.853 MB of 32.873 MB uploadedwandb: / 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.45127
wandb:   val_loss 2.44214
wandb: 
wandb: üöÄ View run happy-sweep-16 at: https://wandb.ai/7shoe/domShift-extensive/runs/vev2fhb3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170602-vev2fhb3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.4513 | Val Loss: 2.4421
2025-03-26 17:06:30,941 - wandb.wandb_agent - INFO - Cleaning up finished run: vev2fhb3
2025-03-26 17:06:31,435 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:06:31,435 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.9
2025-03-26 17:06:31,439 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.9
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:06:36,453 - wandb.wandb_agent - INFO - Running runs: ['un5cepig']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170636-un5cepig
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/un5cepig
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: un5cepig
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2199 | Val Loss: 2.7908
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7536 | Val Loss: 2.8310
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.8831 | Val Loss: 2.9288
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.9391 | Val Loss: 2.9488
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.9393 | Val Loss: 2.9275
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.9199 | Val Loss: 2.9148
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.9130 | Val Loss: 2.9128
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.9133 | Val Loss: 2.9154
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.9180 | Val Loss: 2.9226
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.9274
wandb:   val_loss 2.93466
wandb: 
wandb: üöÄ View run fanciful-sweep-21 at: https://wandb.ai/7shoe/domShift-extensive/runs/un5cepig
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170636-un5cepig/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.9274 | Val Loss: 2.9347
2025-03-26 17:07:06,889 - wandb.wandb_agent - INFO - Cleaning up finished run: un5cepig
2025-03-26 17:07:07,676 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:07,676 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.8
2025-03-26 17:07:07,678 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.8
2025-03-26 17:07:12,691 - wandb.wandb_agent - INFO - Running runs: ['yxzy1y4x']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170713-yxzy1y4x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yxzy1y4x
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: yxzy1y4x
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 2.9161 | Val Loss: 2.8833
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 2.7736 | Val Loss: 2.6843
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.6674 | Val Loss: 2.6698
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.6800 | Val Loss: 2.6911
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.7037 | Val Loss: 2.7160
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 2.7272 | Val Loss: 2.7361
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.6504 | Val Loss: 2.5823
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.5892 | Val Loss: 2.6015
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.6135 | Val Loss: 2.6276
wandb: - 43.713 MB of 43.713 MB uploadedwandb: \ 43.713 MB of 43.713 MB uploadedwandb: | 43.757 MB of 43.757 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.63355
wandb:   val_loss 2.63732
wandb: 
wandb: üöÄ View run silvery-sweep-26 at: https://wandb.ai/7shoe/domShift-extensive/runs/yxzy1y4x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170713-yxzy1y4x/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.6336 | Val Loss: 2.6373
2025-03-26 17:07:58,337 - wandb.wandb_agent - INFO - Cleaning up finished run: yxzy1y4x
2025-03-26 17:07:58,915 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:07:58,915 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:07:58,917 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.4
2025-03-26 17:08:03,931 - wandb.wandb_agent - INFO - Running runs: ['25icc8w9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170805-25icc8w9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/25icc8w9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 25icc8w9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0151 | Val Loss: 2.7583
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6866 | Val Loss: 2.6069
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6085 | Val Loss: 2.6414
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.6888 | Val Loss: 2.7357
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7633 | Val Loss: 2.7744
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.7764 | Val Loss: 2.7654
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.7407 | Val Loss: 2.6978
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6393 | Val Loss: 2.5507
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4574 | Val Loss: 2.3724
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.31873
wandb:   val_loss 2.25337
wandb: 
wandb: üöÄ View run swept-sweep-35 at: https://wandb.ai/7shoe/domShift-extensive/runs/25icc8w9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170805-25icc8w9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3187 | Val Loss: 2.2534
2025-03-26 17:08:49,605 - wandb.wandb_agent - INFO - Cleaning up finished run: 25icc8w9
2025-03-26 17:09:08,268 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:08,269 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:09:08,271 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:09:13,284 - wandb.wandb_agent - INFO - Running runs: ['os9nid2y']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_170914-os9nid2y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/os9nid2y
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: os9nid2y
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0912 | Val Loss: 2.5638
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.4662 | Val Loss: 2.4796
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.5780 | Val Loss: 2.6673
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7132 | Val Loss: 2.7393
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7200 | Val Loss: 2.6835
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.6392 | Val Loss: 2.5797
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5242 | Val Loss: 2.4630
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4081 | Val Loss: 2.3434
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2850 | Val Loss: 2.2326
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.21466
wandb:   val_loss 2.20101
wandb: 
wandb: üöÄ View run efficient-sweep-40 at: https://wandb.ai/7shoe/domShift-extensive/runs/os9nid2y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_170914-os9nid2y/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2147 | Val Loss: 2.2010
2025-03-26 17:09:58,887 - wandb.wandb_agent - INFO - Cleaning up finished run: os9nid2y
2025-03-26 17:09:59,415 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:09:59,415 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:09:59,418 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:10:04,430 - wandb.wandb_agent - INFO - Running runs: ['az2s2qqf']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171004-az2s2qqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/az2s2qqf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: az2s2qqf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3232 | Val Loss: 1.2917
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2768 | Val Loss: 1.2769
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2810 | Val Loss: 1.3292
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3529 | Val Loss: 1.3780
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3782 | Val Loss: 1.3903
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3962 | Val Loss: 1.4069
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4086 | Val Loss: 1.4156
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4085 | Val Loss: 1.4005
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3815 | Val Loss: 1.3533
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÑ
wandb:   val_loss ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32455
wandb:   val_loss 1.29418
wandb: 
wandb: üöÄ View run expert-sweep-50 at: https://wandb.ai/7shoe/domShift-extensive/runs/az2s2qqf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171004-az2s2qqf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3246 | Val Loss: 1.2942
2025-03-26 17:10:34,845 - wandb.wandb_agent - INFO - Cleaning up finished run: az2s2qqf
2025-03-26 17:10:54,930 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 17:10:55,547 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:10:55,547 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:10:55,550 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
2025-03-26 17:11:00,562 - wandb.wandb_agent - INFO - Running runs: ['jo8mm72p']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171102-jo8mm72p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jo8mm72p
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: jo8mm72p
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9305 | Val Loss: 1.5295
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4867 | Val Loss: 1.4289
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4007 | Val Loss: 1.3488
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3279 | Val Loss: 1.2978
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2945 | Val Loss: 1.2962
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3097 | Val Loss: 1.3298
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3368 | Val Loss: 1.3440
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3419 | Val Loss: 1.3322
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3222 | Val Loss: 1.3037
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.29664
wandb:   val_loss 1.28827
wandb: 
wandb: üöÄ View run chocolate-sweep-56 at: https://wandb.ai/7shoe/domShift-extensive/runs/jo8mm72p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171102-jo8mm72p/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2966 | Val Loss: 1.2883
2025-03-26 17:11:30,932 - wandb.wandb_agent - INFO - Cleaning up finished run: jo8mm72p
2025-03-26 17:11:31,507 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:11:31,507 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:11:31,510 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:11:36,523 - wandb.wandb_agent - INFO - Running runs: ['e4vzzqjg']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171136-e4vzzqjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/e4vzzqjg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: e4vzzqjg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0633 | Val Loss: 1.9302
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2136 | Val Loss: 2.0010
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0433 | Val Loss: 1.9388
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.9317 | Val Loss: 1.8562
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8096 | Val Loss: 1.7264
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6920 | Val Loss: 1.6309
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6056 | Val Loss: 1.5697
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5543 | Val Loss: 1.5320
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5211 | Val Loss: 1.5050
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.49612
wandb:   val_loss 1.47961
wandb: 
wandb: üöÄ View run hearty-sweep-58 at: https://wandb.ai/7shoe/domShift-extensive/runs/e4vzzqjg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171136-e4vzzqjg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4961 | Val Loss: 1.4796
2025-03-26 17:12:01,863 - wandb.wandb_agent - INFO - Cleaning up finished run: e4vzzqjg
2025-03-26 17:12:03,874 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:03,874 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:12:03,877 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=cnn --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:12:08,889 - wandb.wandb_agent - INFO - Running runs: ['32o27ygj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171210-32o27ygj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-64
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/32o27ygj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 32o27ygj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5502 | Val Loss: 2.2719
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.2766 | Val Loss: 2.3087
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.3461 | Val Loss: 2.3761
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.4000 | Val Loss: 2.4193
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4313 | Val Loss: 2.4382
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.4412 | Val Loss: 2.4402
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.4372 | Val Loss: 2.4330
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.4290 | Val Loss: 2.4240
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.4197 | Val Loss: 2.4168
wandb: - 43.717 MB of 43.717 MB uploadedwandb: \ 43.717 MB of 43.717 MB uploadedwandb: | 43.736 MB of 43.736 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.41334
wandb:   val_loss 2.41242
wandb: 
wandb: üöÄ View run elated-sweep-64 at: https://wandb.ai/7shoe/domShift-extensive/runs/32o27ygj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171210-32o27ygj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.4133 | Val Loss: 2.4124
2025-03-26 17:12:39,325 - wandb.wandb_agent - INFO - Cleaning up finished run: 32o27ygj
2025-03-26 17:12:40,017 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:12:40,017 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:12:40,019 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:12:45,032 - wandb.wandb_agent - INFO - Running runs: ['rbfq0jzg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171245-rbfq0jzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-69
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rbfq0jzg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: rbfq0jzg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6341 | Val Loss: 1.4581
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4633 | Val Loss: 1.4358
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2482 | Val Loss: 1.1902
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2967 | Val Loss: 1.3948
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3885 | Val Loss: 1.3605
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3107 | Val Loss: 1.2609
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2653 | Val Loss: 1.2823
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2951 | Val Loss: 1.3020
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3055 | Val Loss: 1.3069
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31733
wandb:   val_loss 1.32517
wandb: 
wandb: üöÄ View run denim-sweep-69 at: https://wandb.ai/7shoe/domShift-extensive/runs/rbfq0jzg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171245-rbfq0jzg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3173 | Val Loss: 1.3252
2025-03-26 17:13:40,811 - wandb.wandb_agent - INFO - Cleaning up finished run: rbfq0jzg
2025-03-26 17:13:41,305 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:13:41,305 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:13:41,307 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:13:46,320 - wandb.wandb_agent - INFO - Running runs: ['b53mr5zp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171347-b53mr5zp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b53mr5zp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: b53mr5zp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.3579 | Val Loss: 2.8620
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6783 | Val Loss: 2.6201
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6600 | Val Loss: 2.7009
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7324 | Val Loss: 2.7509
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.7510 | Val Loss: 2.7359
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5847 | Val Loss: 2.4529
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.3740 | Val Loss: 2.2960
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2352 | Val Loss: 2.1788
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1314 | Val Loss: 2.0595
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.99196
wandb:   val_loss 1.95595
wandb: 
wandb: üöÄ View run revived-sweep-77 at: https://wandb.ai/7shoe/domShift-extensive/runs/b53mr5zp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171347-b53mr5zp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9920 | Val Loss: 1.9559
2025-03-26 17:14:47,176 - wandb.wandb_agent - INFO - Cleaning up finished run: b53mr5zp
2025-03-26 17:14:48,025 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:14:48,025 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:14:48,028 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:14:53,040 - wandb.wandb_agent - INFO - Running runs: ['b46gi1ov']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171453-b46gi1ov
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b46gi1ov
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: b46gi1ov
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.0032 | Val Loss: 1.6531
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4564 | Val Loss: 1.3092
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2418 | Val Loss: 1.1825
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1446 | Val Loss: 1.1022
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0573 | Val Loss: 1.0071
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9804 | Val Loss: 0.9513
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9385 | Val Loss: 0.9372
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9549 | Val Loss: 0.9575
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9508 | Val Loss: 0.9476
wandb: - 10.265 MB of 10.265 MB uploadedwandb: \ 10.265 MB of 10.265 MB uploadedwandb: | 10.285 MB of 10.285 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.94989
wandb:   val_loss 0.95131
wandb: 
wandb: üöÄ View run effortless-sweep-85 at: https://wandb.ai/7shoe/domShift-extensive/runs/b46gi1ov
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171453-b46gi1ov/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9499 | Val Loss: 0.9513
2025-03-26 17:15:23,430 - wandb.wandb_agent - INFO - Cleaning up finished run: b46gi1ov
2025-03-26 17:15:23,942 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:15:23,942 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:15:23,945 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.4
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:15:28,957 - wandb.wandb_agent - INFO - Running runs: ['f3fiy1pt']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171528-f3fiy1pt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/f3fiy1pt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: f3fiy1pt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6773 | Val Loss: 1.5501
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5169 | Val Loss: 1.4400
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4119 | Val Loss: 1.3790
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3851 | Val Loss: 1.4245
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4012 | Val Loss: 1.3130
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2120 | Val Loss: 1.1269
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0960 | Val Loss: 1.0675
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0660 | Val Loss: 1.0759
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0901 | Val Loss: 1.1026
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.102 MB uploadedwandb: | 137.082 MB of 137.102 MB uploadedwandb: / 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10744
wandb:   val_loss 1.1083
wandb: 
wandb: üöÄ View run young-sweep-90 at: https://wandb.ai/7shoe/domShift-extensive/runs/f3fiy1pt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171528-f3fiy1pt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1074 | Val Loss: 1.1083
2025-03-26 17:16:04,418 - wandb.wandb_agent - INFO - Cleaning up finished run: f3fiy1pt
2025-03-26 17:16:05,385 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:05,385 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:16:05,388 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:16:10,400 - wandb.wandb_agent - INFO - Running runs: ['wl50mss3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171611-wl50mss3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wl50mss3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: wl50mss3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6121 | Val Loss: 1.4570
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4399 | Val Loss: 1.4368
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4521 | Val Loss: 1.4647
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4743 | Val Loss: 1.4793
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4817 | Val Loss: 1.4781
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4684 | Val Loss: 1.4585
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4562 | Val Loss: 1.4552
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4571 | Val Loss: 1.4585
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4596 | Val Loss: 1.4606
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.095 MB uploadedwandb: | 137.075 MB of 137.095 MB uploadedwandb: / 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÑ‚ñÅ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.46306
wandb:   val_loss 1.46518
wandb: 
wandb: üöÄ View run ruby-sweep-95 at: https://wandb.ai/7shoe/domShift-extensive/runs/wl50mss3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171611-wl50mss3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4631 | Val Loss: 1.4652
2025-03-26 17:16:45,895 - wandb.wandb_agent - INFO - Cleaning up finished run: wl50mss3
2025-03-26 17:16:46,911 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:16:46,911 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:16:46,914 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:16:51,926 - wandb.wandb_agent - INFO - Running runs: ['84ivnriy']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171651-84ivnriy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/84ivnriy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 84ivnriy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2195 | Val Loss: 1.0199
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9917 | Val Loss: 0.9814
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9781 | Val Loss: 0.9787
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9778 | Val Loss: 0.9781
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9777 | Val Loss: 0.9781
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9777 | Val Loss: 0.9781
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9777 | Val Loss: 0.9781
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9777 | Val Loss: 0.9781
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9777 | Val Loss: 0.9781
wandb: - 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.97771
wandb:   val_loss 0.97806
wandb: 
wandb: üöÄ View run trim-sweep-100 at: https://wandb.ai/7shoe/domShift-extensive/runs/84ivnriy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171651-84ivnriy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9777 | Val Loss: 0.9781
2025-03-26 17:17:42,637 - wandb.wandb_agent - INFO - Cleaning up finished run: 84ivnriy
2025-03-26 17:17:43,178 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:17:43,178 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:17:43,181 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:17:48,193 - wandb.wandb_agent - INFO - Running runs: ['cggxdjuw']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171748-cggxdjuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-107
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cggxdjuw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cggxdjuw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6923 | Val Loss: 1.6668
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6117 | Val Loss: 1.5980
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4190 | Val Loss: 1.3890
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4208 | Val Loss: 1.4306
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4294 | Val Loss: 1.4166
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3591 | Val Loss: 1.2436
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2502 | Val Loss: 1.2472
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2488 | Val Loss: 1.2537
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2551 | Val Loss: 1.2545
wandb: - 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25755
wandb:   val_loss 1.25807
wandb: 
wandb: üöÄ View run resilient-sweep-107 at: https://wandb.ai/7shoe/domShift-extensive/runs/cggxdjuw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171748-cggxdjuw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2576 | Val Loss: 1.2581
2025-03-26 17:19:04,250 - wandb.wandb_agent - INFO - Cleaning up finished run: cggxdjuw
2025-03-26 17:19:04,956 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:19:04,957 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:19:04,965 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:19:09,978 - wandb.wandb_agent - INFO - Running runs: ['g4blowzt']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_171909-g4blowzt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-116
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/g4blowzt
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: g4blowzt
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5352 | Val Loss: 2.3104
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.0956 | Val Loss: 1.8857
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7995 | Val Loss: 1.7398
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.7034 | Val Loss: 1.6781
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6649 | Val Loss: 1.6544
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.6486 | Val Loss: 1.6440
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.6425 | Val Loss: 1.6422
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6425 | Val Loss: 1.6430
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6428 | Val Loss: 1.6418
wandb: - 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.63279
wandb:   val_loss 1.61611
wandb: 
wandb: üöÄ View run apricot-sweep-116 at: https://wandb.ai/7shoe/domShift-extensive/runs/g4blowzt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_171909-g4blowzt/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.6328 | Val Loss: 1.6161
2025-03-26 17:20:10,837 - wandb.wandb_agent - INFO - Cleaning up finished run: g4blowzt
2025-03-26 17:20:11,385 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:20:11,385 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:20:11,388 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:20:16,401 - wandb.wandb_agent - INFO - Running runs: ['vslbx176']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172017-vslbx176
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-125
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vslbx176
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: vslbx176
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8044 | Val Loss: 1.4912
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4281 | Val Loss: 1.3168
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2063 | Val Loss: 1.1329
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1429 | Val Loss: 1.1499
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1376 | Val Loss: 1.1300
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1444 | Val Loss: 1.1507
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1471 | Val Loss: 1.1504
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0344 | Val Loss: 0.9100
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8977 | Val Loss: 0.9018
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.89788
wandb:   val_loss 0.89075
wandb: 
wandb: üöÄ View run rural-sweep-125 at: https://wandb.ai/7shoe/domShift-extensive/runs/vslbx176
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172017-vslbx176/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8979 | Val Loss: 0.8908
2025-03-26 17:21:01,980 - wandb.wandb_agent - INFO - Cleaning up finished run: vslbx176
2025-03-26 17:21:02,486 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:21:02,486 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:21:02,489 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:21:07,501 - wandb.wandb_agent - INFO - Running runs: ['q1pfzdcv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172108-q1pfzdcv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-132
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/q1pfzdcv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: q1pfzdcv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4119 | Val Loss: 1.1059
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1116 | Val Loss: 1.1205
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1522 | Val Loss: 1.1788
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2111 | Val Loss: 1.2354
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2495 | Val Loss: 1.2321
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2007 | Val Loss: 1.2082
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2407 | Val Loss: 1.2737
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2785 | Val Loss: 1.2696
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2513 | Val Loss: 1.2235
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.22249
wandb:   val_loss 1.23851
wandb: 
wandb: üöÄ View run devoted-sweep-132 at: https://wandb.ai/7shoe/domShift-extensive/runs/q1pfzdcv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172108-q1pfzdcv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2225 | Val Loss: 1.2385
2025-03-26 17:21:53,117 - wandb.wandb_agent - INFO - Cleaning up finished run: q1pfzdcv
2025-03-26 17:21:53,958 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:21:53,958 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.8
2025-03-26 17:21:53,960 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.8
2025-03-26 17:21:58,973 - wandb.wandb_agent - INFO - Running runs: ['hupg1mzc']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172200-hupg1mzc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-137
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hupg1mzc
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: hupg1mzc
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.6893 | Val Loss: 2.5415
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.4635 | Val Loss: 2.5249
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.5935 | Val Loss: 2.6475
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.6770 | Val Loss: 2.6998
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.7083 | Val Loss: 2.7129
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.7130 | Val Loss: 2.7117
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.7090 | Val Loss: 2.7067
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.7075 | Val Loss: 2.7101
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.7165 | Val Loss: 2.7251
wandb: - 10.262 MB of 10.262 MB uploadedwandb: \ 10.262 MB of 10.262 MB uploadedwandb: | 10.291 MB of 10.291 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:   val_loss ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.73755
wandb:   val_loss 2.75233
wandb: 
wandb: üöÄ View run olive-sweep-137 at: https://wandb.ai/7shoe/domShift-extensive/runs/hupg1mzc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172200-hupg1mzc/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.7376 | Val Loss: 2.7523
2025-03-26 17:22:44,597 - wandb.wandb_agent - INFO - Cleaning up finished run: hupg1mzc
2025-03-26 17:22:45,178 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:22:45,178 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:22:45,181 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:22:50,193 - wandb.wandb_agent - INFO - Running runs: ['3thhs1ty']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172251-3thhs1ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-142
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3thhs1ty
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3thhs1ty
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4333 | Val Loss: 1.4298
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3588 | Val Loss: 1.2677
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2471 | Val Loss: 1.2009
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2154 | Val Loss: 1.2386
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2817 | Val Loss: 1.3141
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3114 | Val Loss: 1.3207
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3730 | Val Loss: 1.5354
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.5439 | Val Loss: 1.2469
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9665 | Val Loss: 0.7772
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÅ
wandb:   val_loss ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.77676
wandb:   val_loss 0.76832
wandb: 
wandb: üöÄ View run vital-sweep-142 at: https://wandb.ai/7shoe/domShift-extensive/runs/3thhs1ty
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172251-3thhs1ty/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.7768 | Val Loss: 0.7683
2025-03-26 17:24:01,239 - wandb.wandb_agent - INFO - Cleaning up finished run: 3thhs1ty
2025-03-26 17:24:01,921 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:01,921 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:24:01,924 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:24:06,936 - wandb.wandb_agent - INFO - Running runs: ['il7ztmdq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172407-il7ztmdq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-150
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/il7ztmdq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: il7ztmdq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5637 | Val Loss: 1.3077
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2089 | Val Loss: 1.1135
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0687 | Val Loss: 1.0457
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0531 | Val Loss: 1.0591
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0679 | Val Loss: 1.0756
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0883 | Val Loss: 1.0994
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1002 | Val Loss: 1.0548
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9970 | Val Loss: 0.9498
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9218 | Val Loss: 0.9001
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.91327
wandb:   val_loss 0.93692
wandb: 
wandb: üöÄ View run likely-sweep-150 at: https://wandb.ai/7shoe/domShift-extensive/runs/il7ztmdq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172407-il7ztmdq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9133 | Val Loss: 0.9369
2025-03-26 17:24:42,410 - wandb.wandb_agent - INFO - Cleaning up finished run: il7ztmdq
2025-03-26 17:24:43,438 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:24:43,438 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:24:43,441 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:24:48,454 - wandb.wandb_agent - INFO - Running runs: ['7wmcormp']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172448-7wmcormp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-156
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7wmcormp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7wmcormp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.8436 | Val Loss: 2.5888
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3899 | Val Loss: 2.1433
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.0117 | Val Loss: 1.9180
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.8887 | Val Loss: 1.8642
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8638 | Val Loss: 1.8648
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8748 | Val Loss: 1.8840
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.8949 | Val Loss: 1.9001
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.9094 | Val Loss: 1.9258
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9447 | Val Loss: 1.9632
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.97912
wandb:   val_loss 1.99254
wandb: 
wandb: üöÄ View run soft-sweep-156 at: https://wandb.ai/7shoe/domShift-extensive/runs/7wmcormp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172448-7wmcormp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9791 | Val Loss: 1.9925
2025-03-26 17:25:49,409 - wandb.wandb_agent - INFO - Cleaning up finished run: 7wmcormp
2025-03-26 17:25:50,074 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:25:50,074 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:25:50,077 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:25:55,089 - wandb.wandb_agent - INFO - Running runs: ['7df56fs0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172556-7df56fs0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-163
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7df56fs0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7df56fs0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9230 | Val Loss: 1.8248
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5903 | Val Loss: 1.3578
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2812 | Val Loss: 1.2134
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2005 | Val Loss: 1.1857
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1777 | Val Loss: 1.1717
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1808 | Val Loss: 1.1902
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2006 | Val Loss: 1.2082
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2104 | Val Loss: 1.2090
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2106 | Val Loss: 1.2116
wandb: - 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21475
wandb:   val_loss 1.21641
wandb: 
wandb: üöÄ View run true-sweep-163 at: https://wandb.ai/7shoe/domShift-extensive/runs/7df56fs0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172556-7df56fs0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2147 | Val Loss: 1.2164
2025-03-26 17:26:25,499 - wandb.wandb_agent - INFO - Cleaning up finished run: 7df56fs0
2025-03-26 17:26:26,095 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:26:26,095 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:26:26,097 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:26:31,110 - wandb.wandb_agent - INFO - Running runs: ['n82zl6rw']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172631-n82zl6rw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-167
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/n82zl6rw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: n82zl6rw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8020 | Val Loss: 1.5701
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4653 | Val Loss: 1.4356
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3950 | Val Loss: 1.2959
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2395 | Val Loss: 1.0419
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0047 | Val Loss: 1.0000
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9992 | Val Loss: 1.0000
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0004 | Val Loss: 1.0004
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0006 | Val Loss: 1.0004
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0006 | Val Loss: 1.0004
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.00056
wandb:   val_loss 1.00038
wandb: 
wandb: üöÄ View run fresh-sweep-167 at: https://wandb.ai/7shoe/domShift-extensive/runs/n82zl6rw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172631-n82zl6rw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0006 | Val Loss: 1.0004
2025-03-26 17:27:37,100 - wandb.wandb_agent - INFO - Cleaning up finished run: n82zl6rw
2025-03-26 17:27:37,822 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:27:37,822 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.5
2025-03-26 17:27:37,825 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.5
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:27:42,837 - wandb.wandb_agent - INFO - Running runs: ['iqqom51n']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172742-iqqom51n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-176
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/iqqom51n
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: iqqom51n
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0807 | Val Loss: 1.8370
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7994 | Val Loss: 1.7594
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.7000 | Val Loss: 1.6050
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5638 | Val Loss: 1.5116
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4841 | Val Loss: 1.4454
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4274 | Val Loss: 1.4029
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3969 | Val Loss: 1.3852
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3813 | Val Loss: 1.3721
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3678 | Val Loss: 1.3574
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.34909
wandb:   val_loss 1.3294
wandb: 
wandb: üöÄ View run fearless-sweep-176 at: https://wandb.ai/7shoe/domShift-extensive/runs/iqqom51n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172742-iqqom51n/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3491 | Val Loss: 1.3294
2025-03-26 17:28:13,260 - wandb.wandb_agent - INFO - Cleaning up finished run: iqqom51n
2025-03-26 17:28:13,951 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:28:13,951 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:28:13,954 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:28:18,966 - wandb.wandb_agent - INFO - Running runs: ['p1xrznef']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172820-p1xrznef
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-181
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/p1xrznef
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: p1xrznef
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4139 | Val Loss: 1.1596
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1516 | Val Loss: 1.1726
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1760 | Val Loss: 1.1799
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1565 | Val Loss: 1.1632
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2482 | Val Loss: 1.3072
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3420 | Val Loss: 1.3025
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1370 | Val Loss: 0.9486
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8922 | Val Loss: 0.8843
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9061 | Val Loss: 0.9215
wandb: - 137.075 MB of 137.075 MB uploadedwandb: \ 137.075 MB of 137.075 MB uploadedwandb: | 137.095 MB of 137.095 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.92089
wandb:   val_loss 0.91704
wandb: 
wandb: üöÄ View run robust-sweep-181 at: https://wandb.ai/7shoe/domShift-extensive/runs/p1xrznef
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172820-p1xrznef/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9209 | Val Loss: 0.9170
2025-03-26 17:29:04,607 - wandb.wandb_agent - INFO - Cleaning up finished run: p1xrznef
2025-03-26 17:29:05,649 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:29:05,649 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:29:05,652 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:29:10,665 - wandb.wandb_agent - INFO - Running runs: ['twefzryu']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_172910-twefzryu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-185
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/twefzryu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: twefzryu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5934 | Val Loss: 2.1636
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3032 | Val Loss: 2.4271
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4949 | Val Loss: 2.5395
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5656 | Val Loss: 2.5817
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5781 | Val Loss: 2.5666
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.2545 | Val Loss: 2.1068
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1256 | Val Loss: 2.1490
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1743 | Val Loss: 2.1993
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2247 | Val Loss: 2.2490
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.469 MB of 137.469 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñá‚ñà‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:   val_loss ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.27219
wandb:   val_loss 2.29435
wandb: 
wandb: üöÄ View run volcanic-sweep-185 at: https://wandb.ai/7shoe/domShift-extensive/runs/twefzryu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_172910-twefzryu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2722 | Val Loss: 2.2943
2025-03-26 17:30:31,878 - wandb.wandb_agent - INFO - Cleaning up finished run: twefzryu
2025-03-26 17:30:32,461 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:30:32,461 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:30:32,463 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:30:37,475 - wandb.wandb_agent - INFO - Running runs: ['dfydge8m']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173038-dfydge8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-195
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/dfydge8m
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: dfydge8m
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8386 | Val Loss: 1.5633
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4666 | Val Loss: 1.3394
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2707 | Val Loss: 1.2245
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1733 | Val Loss: 1.1410
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1395 | Val Loss: 1.1392
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1539 | Val Loss: 1.1738
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1895 | Val Loss: 1.2045
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2718 | Val Loss: 1.4104
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4678 | Val Loss: 1.4933
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.50939
wandb:   val_loss 1.44232
wandb: 
wandb: üöÄ View run visionary-sweep-195 at: https://wandb.ai/7shoe/domShift-extensive/runs/dfydge8m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173038-dfydge8m/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5094 | Val Loss: 1.4423
2025-03-26 17:31:33,225 - wandb.wandb_agent - INFO - Cleaning up finished run: dfydge8m
2025-03-26 17:31:34,647 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:31:34,648 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:31:34,651 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:31:39,665 - wandb.wandb_agent - INFO - Running runs: ['rfok9dml']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173139-rfok9dml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-202
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rfok9dml
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: rfok9dml
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3438 | Val Loss: 1.1933
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1868 | Val Loss: 1.3866
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.4210 | Val Loss: 1.4047
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3709 | Val Loss: 1.3352
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2378 | Val Loss: 1.1278
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1762 | Val Loss: 1.3262
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4285 | Val Loss: 1.4293
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4421 | Val Loss: 1.4906
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4908 | Val Loss: 1.4822
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÖ‚ñÅ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñá‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.48108
wandb:   val_loss 1.48528
wandb: 
wandb: üöÄ View run spring-sweep-202 at: https://wandb.ai/7shoe/domShift-extensive/runs/rfok9dml
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173139-rfok9dml/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4811 | Val Loss: 1.4853
2025-03-26 17:32:10,026 - wandb.wandb_agent - INFO - Cleaning up finished run: rfok9dml
2025-03-26 17:32:10,559 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:32:10,559 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:32:10,562 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:32:15,575 - wandb.wandb_agent - INFO - Running runs: ['kl73hbms']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173217-kl73hbms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-206
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kl73hbms
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: kl73hbms
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2527 | Val Loss: 1.1101
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0212 | Val Loss: 0.8913
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.8660 | Val Loss: 0.8524
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.8447 | Val Loss: 0.8385
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.8404 | Val Loss: 0.8407
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.8366 | Val Loss: 0.8359
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.8347 | Val Loss: 0.8341
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8335 | Val Loss: 0.8330
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8326 | Val Loss: 0.8324
wandb: - 32.875 MB of 32.875 MB uploadedwandb: \ 32.875 MB of 32.875 MB uploadedwandb: | 32.968 MB of 32.968 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.83204
wandb:   val_loss 0.83189
wandb: 
wandb: üöÄ View run stilted-sweep-206 at: https://wandb.ai/7shoe/domShift-extensive/runs/kl73hbms
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173217-kl73hbms/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8320 | Val Loss: 0.8319
2025-03-26 17:33:01,217 - wandb.wandb_agent - INFO - Cleaning up finished run: kl73hbms
2025-03-26 17:33:01,965 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:33:01,965 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:33:01,968 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:33:06,980 - wandb.wandb_agent - INFO - Running runs: ['2m37o7ra']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173306-2m37o7ra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-210
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2m37o7ra
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 2m37o7ra
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4911 | Val Loss: 1.3578
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2724 | Val Loss: 1.3229
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3343 | Val Loss: 1.3577
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3547 | Val Loss: 1.3604
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3028 | Val Loss: 1.2673
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2545 | Val Loss: 1.2527
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2747 | Val Loss: 1.2906
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3021 | Val Loss: 1.3115
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3162 | Val Loss: 1.3178
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÜ‚ñà‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32535
wandb:   val_loss 1.33476
wandb: 
wandb: üöÄ View run sweepy-sweep-210 at: https://wandb.ai/7shoe/domShift-extensive/runs/2m37o7ra
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173306-2m37o7ra/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3254 | Val Loss: 1.3348
2025-03-26 17:33:52,592 - wandb.wandb_agent - INFO - Cleaning up finished run: 2m37o7ra
2025-03-26 17:33:56,781 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:33:56,781 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:33:56,785 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:34:01,796 - wandb.wandb_agent - INFO - Running runs: ['u5ydq0kf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173401-u5ydq0kf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-215
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/u5ydq0kf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: u5ydq0kf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0294 | Val Loss: 2.7403
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6395 | Val Loss: 2.5477
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.4509 | Val Loss: 2.3506
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.2797 | Val Loss: 2.2178
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.1914 | Val Loss: 2.1715
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1719 | Val Loss: 2.1773
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1943 | Val Loss: 2.2142
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2385 | Val Loss: 2.2646
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.2918 | Val Loss: 2.3205
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.34834
wandb:   val_loss 2.37634
wandb: 
wandb: üöÄ View run sage-sweep-215 at: https://wandb.ai/7shoe/domShift-extensive/runs/u5ydq0kf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173401-u5ydq0kf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.3483 | Val Loss: 2.3763
2025-03-26 17:35:02,662 - wandb.wandb_agent - INFO - Cleaning up finished run: u5ydq0kf
2025-03-26 17:35:03,237 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:03,237 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:35:03,240 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:35:08,252 - wandb.wandb_agent - INFO - Running runs: ['9yeohzw0']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173508-9yeohzw0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-221
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9yeohzw0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9yeohzw0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9176 | Val Loss: 2.4135
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.1371 | Val Loss: 1.9194
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.8816 | Val Loss: 1.8947
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.9394 | Val Loss: 1.9783
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0037 | Val Loss: 2.0240
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0377 | Val Loss: 2.0473
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.0519 | Val Loss: 2.0482
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.0455 | Val Loss: 2.0452
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.0444 | Val Loss: 2.0394
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.99842
wandb:   val_loss 1.97957
wandb: 
wandb: üöÄ View run usual-sweep-221 at: https://wandb.ai/7shoe/domShift-extensive/runs/9yeohzw0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173508-9yeohzw0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9984 | Val Loss: 1.9796
2025-03-26 17:35:58,985 - wandb.wandb_agent - INFO - Cleaning up finished run: 9yeohzw0
2025-03-26 17:35:59,476 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:35:59,476 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:35:59,478 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:36:04,491 - wandb.wandb_agent - INFO - Running runs: ['ed88e7kq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173604-ed88e7kq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-231
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ed88e7kq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ed88e7kq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2026 | Val Loss: 0.9534
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0308 | Val Loss: 1.1056
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1396 | Val Loss: 1.1380
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1455 | Val Loss: 1.2058
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1880 | Val Loss: 1.1722
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1840 | Val Loss: 1.1902
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1815 | Val Loss: 1.1869
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1852 | Val Loss: 1.1619
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1444 | Val Loss: 1.1224
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13278
wandb:   val_loss 1.14418
wandb: 
wandb: üöÄ View run wobbly-sweep-231 at: https://wandb.ai/7shoe/domShift-extensive/runs/ed88e7kq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173604-ed88e7kq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1328 | Val Loss: 1.1442
2025-03-26 17:36:40,023 - wandb.wandb_agent - INFO - Cleaning up finished run: ed88e7kq
2025-03-26 17:36:40,676 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:36:40,676 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:36:40,679 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:36:45,691 - wandb.wandb_agent - INFO - Running runs: ['ex9xs4nq']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173647-ex9xs4nq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-235
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ex9xs4nq
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ex9xs4nq
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4910 | Val Loss: 1.2550
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2509 | Val Loss: 1.2107
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1536 | Val Loss: 1.1863
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2209 | Val Loss: 1.2835
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2622 | Val Loss: 1.2419
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2641 | Val Loss: 1.2999
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3451 | Val Loss: 1.3941
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4349 | Val Loss: 1.4658
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4627 | Val Loss: 1.4499
wandb: - 32.875 MB of 32.875 MB uploadedwandb: \ 32.875 MB of 32.875 MB uploadedwandb: | 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4231
wandb:   val_loss 1.38921
wandb: 
wandb: üöÄ View run resilient-sweep-235 at: https://wandb.ai/7shoe/domShift-extensive/runs/ex9xs4nq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173647-ex9xs4nq/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4231 | Val Loss: 1.3892
2025-03-26 17:37:21,139 - wandb.wandb_agent - INFO - Cleaning up finished run: ex9xs4nq
2025-03-26 17:37:21,683 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:37:21,683 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:37:21,686 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 17:37:26,699 - wandb.wandb_agent - INFO - Running runs: ['iszy1cel']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173726-iszy1cel
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-239
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/iszy1cel
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: iszy1cel
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.0396 | Val Loss: 1.9394
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.8631 | Val Loss: 1.7865
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.7569 | Val Loss: 1.7356
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.7421 | Val Loss: 1.7487
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.7694 | Val Loss: 1.7290
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.6897 | Val Loss: 1.6288
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5754 | Val Loss: 1.5285
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4987 | Val Loss: 1.3942
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3233 | Val Loss: 1.2678
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27398
wandb:   val_loss 1.27794
wandb: 
wandb: üöÄ View run copper-sweep-239 at: https://wandb.ai/7shoe/domShift-extensive/runs/iszy1cel
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173726-iszy1cel/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2740 | Val Loss: 1.2779
2025-03-26 17:37:57,171 - wandb.wandb_agent - INFO - Cleaning up finished run: iszy1cel
2025-03-26 17:37:58,097 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:37:58,097 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:37:58,100 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimCLR --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:38:03,112 - wandb.wandb_agent - INFO - Running runs: ['csfy36wg']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173803-csfy36wg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-243
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/csfy36wg
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: csfy36wg
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.1334 | Val Loss: 4.3215
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.2749 | Val Loss: 3.9053
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.9223 | Val Loss: 3.5547
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.6391 | Val Loss: 3.3723
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.4741 | Val Loss: 3.2032
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.3452 | Val Loss: 3.0636
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.2505 | Val Loss: 3.0080
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.1870 | Val Loss: 2.9371
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.1191 | Val Loss: 2.8843
wandb: - 137.081 MB of 137.081 MB uploadedwandb: \ 137.081 MB of 137.081 MB uploadedwandb: | 137.239 MB of 137.239 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.05332
wandb:   val_loss 2.81841
wandb: 
wandb: üöÄ View run fallen-sweep-243 at: https://wandb.ai/7shoe/domShift-extensive/runs/csfy36wg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173803-csfy36wg/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.0533 | Val Loss: 2.8184
2025-03-26 17:39:03,907 - wandb.wandb_agent - INFO - Cleaning up finished run: csfy36wg
2025-03-26 17:39:04,446 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:39:04,446 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:39:04,449 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:39:09,461 - wandb.wandb_agent - INFO - Running runs: ['bk3vw3al']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_173910-bk3vw3al
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-250
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bk3vw3al
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bk3vw3al
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7449 | Val Loss: 1.8200
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.8278 | Val Loss: 1.7948
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5671 | Val Loss: 1.3048
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3020 | Val Loss: 1.3124
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3419 | Val Loss: 1.3663
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3900 | Val Loss: 1.4171
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4329 | Val Loss: 1.4426
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4419 | Val Loss: 1.4412
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4460 | Val Loss: 1.4510
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.45517
wandb:   val_loss 1.4594
wandb: 
wandb: üöÄ View run classic-sweep-250 at: https://wandb.ai/7shoe/domShift-extensive/runs/bk3vw3al
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_173910-bk3vw3al/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4552 | Val Loss: 1.4594
2025-03-26 17:40:05,273 - wandb.wandb_agent - INFO - Cleaning up finished run: bk3vw3al
2025-03-26 17:40:05,965 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:40:05,965 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:40:05,968 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimCLR --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:40:10,980 - wandb.wandb_agent - INFO - Running runs: ['9v5zzzuy']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174012-9v5zzzuy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-258
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9v5zzzuy
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 9v5zzzuy
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.1985 | Val Loss: 4.5501
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.2387 | Val Loss: 3.8358
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.7467 | Val Loss: 3.3622
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.2851 | Val Loss: 2.8658
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.0300 | Val Loss: 2.7581
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8606 | Val Loss: 2.6748
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6830 | Val Loss: 2.4130
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5362 | Val Loss: 2.3028
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3670 | Val Loss: 2.1867
wandb: - 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.27791
wandb:   val_loss 2.06803
wandb: 
wandb: üöÄ View run glamorous-sweep-258 at: https://wandb.ai/7shoe/domShift-extensive/runs/9v5zzzuy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174012-9v5zzzuy/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2779 | Val Loss: 2.0680
2025-03-26 17:40:56,634 - wandb.wandb_agent - INFO - Cleaning up finished run: 9v5zzzuy
2025-03-26 17:40:57,366 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:40:57,366 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:40:57,368 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:41:02,380 - wandb.wandb_agent - INFO - Running runs: ['x3451rol']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174102-x3451rol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-264
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/x3451rol
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: x3451rol
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.0673 | Val Loss: 2.7253
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.6629 | Val Loss: 2.6227
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6085 | Val Loss: 2.5865
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5547 | Val Loss: 2.5054
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.4689 | Val Loss: 2.4157
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.3703 | Val Loss: 2.3126
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.2772 | Val Loss: 2.2344
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.2128 | Val Loss: 2.1856
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1691 | Val Loss: 2.1480
wandb: - 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.13764
wandb:   val_loss 2.12546
wandb: 
wandb: üöÄ View run stellar-sweep-264 at: https://wandb.ai/7shoe/domShift-extensive/runs/x3451rol
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174102-x3451rol/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1376 | Val Loss: 2.1255
2025-03-26 17:41:32,806 - wandb.wandb_agent - INFO - Cleaning up finished run: x3451rol
2025-03-26 17:41:33,303 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:41:33,304 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:41:33,306 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:41:38,319 - wandb.wandb_agent - INFO - Running runs: ['hn4xm6vd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174138-hn4xm6vd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-268
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hn4xm6vd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: hn4xm6vd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.8463 | Val Loss: 1.7481
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7845 | Val Loss: 1.7533
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6858 | Val Loss: 1.5861
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5420 | Val Loss: 1.5010
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4952 | Val Loss: 1.4871
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4798 | Val Loss: 1.4667
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4641 | Val Loss: 1.4490
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4387 | Val Loss: 1.4161
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4058 | Val Loss: 1.3866
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37713
wandb:   val_loss 1.36575
wandb: 
wandb: üöÄ View run lyric-sweep-268 at: https://wandb.ai/7shoe/domShift-extensive/runs/hn4xm6vd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174138-hn4xm6vd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3771 | Val Loss: 1.3658
2025-03-26 17:42:08,697 - wandb.wandb_agent - INFO - Cleaning up finished run: hn4xm6vd
2025-03-26 17:42:09,364 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:42:09,365 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:42:09,367 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:42:14,380 - wandb.wandb_agent - INFO - Running runs: ['uxbwi9nz']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174214-uxbwi9nz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-273
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uxbwi9nz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: uxbwi9nz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0285 | Val Loss: 0.8815
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.8582 | Val Loss: 0.8346
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.8406 | Val Loss: 0.8409
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8440 | Val Loss: 0.8439
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8440 | Val Loss: 0.8444
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8441 | Val Loss: 0.8443
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8440 | Val Loss: 0.8441
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8440 | Val Loss: 0.8440
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8440 | Val Loss: 0.8440
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.84392
wandb:   val_loss 0.84402
wandb: 
wandb: üöÄ View run curious-sweep-273 at: https://wandb.ai/7shoe/domShift-extensive/runs/uxbwi9nz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174214-uxbwi9nz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8439 | Val Loss: 0.8440
2025-03-26 17:43:25,335 - wandb.wandb_agent - INFO - Cleaning up finished run: uxbwi9nz
2025-03-26 17:43:25,876 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:43:25,876 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:43:25,880 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:43:30,894 - wandb.wandb_agent - INFO - Running runs: ['7egvg3t6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174330-7egvg3t6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-280
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7egvg3t6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 7egvg3t6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5349 | Val Loss: 1.5461
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5544 | Val Loss: 1.5601
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5443 | Val Loss: 1.5226
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5203 | Val Loss: 1.5649
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5629 | Val Loss: 1.5249
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5107 | Val Loss: 1.5448
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5364 | Val Loss: 1.4691
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4224 | Val Loss: 1.3743
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3536 | Val Loss: 1.3318
wandb: - 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÑ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33866
wandb:   val_loss 1.33958
wandb: 
wandb: üöÄ View run quiet-sweep-280 at: https://wandb.ai/7shoe/domShift-extensive/runs/7egvg3t6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174330-7egvg3t6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3387 | Val Loss: 1.3396
2025-03-26 17:43:56,236 - wandb.wandb_agent - INFO - Cleaning up finished run: 7egvg3t6
2025-03-26 17:43:56,774 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:43:56,774 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:43:56,777 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 17:44:01,791 - wandb.wandb_agent - INFO - Running runs: ['cqn4dvqi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174403-cqn4dvqi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-285
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/cqn4dvqi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: cqn4dvqi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1962 | Val Loss: 1.1704
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1779 | Val Loss: 1.1459
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1303 | Val Loss: 1.1126
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1330 | Val Loss: 1.1442
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1410 | Val Loss: 1.1340
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1426 | Val Loss: 1.1427
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1330 | Val Loss: 1.1379
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1361 | Val Loss: 1.1407
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1319 | Val Loss: 1.1278
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14061
wandb:   val_loss 1.15927
wandb: 
wandb: üöÄ View run pious-sweep-285 at: https://wandb.ai/7shoe/domShift-extensive/runs/cqn4dvqi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174403-cqn4dvqi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1406 | Val Loss: 1.1593
2025-03-26 17:44:47,404 - wandb.wandb_agent - INFO - Cleaning up finished run: cqn4dvqi
2025-03-26 17:44:48,072 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:44:48,072 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:44:48,075 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 17:44:53,087 - wandb.wandb_agent - INFO - Running runs: ['uewfnnuz']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174453-uewfnnuz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-291
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uewfnnuz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: uewfnnuz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3616 | Val Loss: 1.1906
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1782 | Val Loss: 1.1710
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2079 | Val Loss: 1.2365
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2685 | Val Loss: 1.3007
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2930 | Val Loss: 1.2489
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0816 | Val Loss: 1.0054
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0590 | Val Loss: 1.0791
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0586 | Val Loss: 1.0577
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0733 | Val Loss: 1.0705
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.333 MB uploadedwandb: | 137.313 MB of 137.333 MB uploadedwandb: / 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06479
wandb:   val_loss 1.07179
wandb: 
wandb: üöÄ View run stoic-sweep-291 at: https://wandb.ai/7shoe/domShift-extensive/runs/uewfnnuz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174453-uewfnnuz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0648 | Val Loss: 1.0718
2025-03-26 17:45:43,829 - wandb.wandb_agent - INFO - Cleaning up finished run: uewfnnuz
2025-03-26 17:45:44,876 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:45:44,876 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:45:44,879 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:45:49,891 - wandb.wandb_agent - INFO - Running runs: ['fcwnx58d']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174551-fcwnx58d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-297
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/fcwnx58d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: fcwnx58d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6109 | Val Loss: 1.2661
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2335 | Val Loss: 1.2241
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2395 | Val Loss: 1.2532
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2549 | Val Loss: 1.2490
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2403 | Val Loss: 1.2187
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2288 | Val Loss: 1.2780
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3268 | Val Loss: 1.3719
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3860 | Val Loss: 1.3949
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4055 | Val Loss: 1.4117
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñá‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40101
wandb:   val_loss 1.37943
wandb: 
wandb: üöÄ View run expert-sweep-297 at: https://wandb.ai/7shoe/domShift-extensive/runs/fcwnx58d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174551-fcwnx58d/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4010 | Val Loss: 1.3794
2025-03-26 17:46:45,697 - wandb.wandb_agent - INFO - Cleaning up finished run: fcwnx58d
2025-03-26 17:46:46,636 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:46:46,636 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:46:46,639 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:46:51,652 - wandb.wandb_agent - INFO - Running runs: ['8woj86o4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174651-8woj86o4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-303
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8woj86o4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 8woj86o4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.0747 | Val Loss: 1.9192
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.8091 | Val Loss: 1.6666
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.6222 | Val Loss: 1.5787
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5700 | Val Loss: 1.5574
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.5636 | Val Loss: 1.5654
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5723 | Val Loss: 1.5765
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5817 | Val Loss: 1.5813
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.5814 | Val Loss: 1.5769
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.5761 | Val Loss: 1.5733
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.56948
wandb:   val_loss 1.56201
wandb: 
wandb: üöÄ View run graceful-sweep-303 at: https://wandb.ai/7shoe/domShift-extensive/runs/8woj86o4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174651-8woj86o4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.5695 | Val Loss: 1.5620
2025-03-26 17:47:17,124 - wandb.wandb_agent - INFO - Cleaning up finished run: 8woj86o4
2025-03-26 17:47:17,828 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:47:17,828 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:47:17,831 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:47:22,840 - wandb.wandb_agent - INFO - Running runs: ['wd2jass1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174722-wd2jass1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-309
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wd2jass1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wd2jass1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4061 | Val Loss: 1.2650
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2369 | Val Loss: 1.2190
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2203 | Val Loss: 1.2248
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2327 | Val Loss: 1.2395
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2469 | Val Loss: 1.2527
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2588 | Val Loss: 1.2640
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2691 | Val Loss: 1.2735
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2777 | Val Loss: 1.2806
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2848 | Val Loss: 1.2888
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.29279
wandb:   val_loss 1.29611
wandb: 
wandb: üöÄ View run crimson-sweep-309 at: https://wandb.ai/7shoe/domShift-extensive/runs/wd2jass1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174722-wd2jass1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2928 | Val Loss: 1.2961
2025-03-26 17:48:33,811 - wandb.wandb_agent - INFO - Cleaning up finished run: wd2jass1
2025-03-26 17:48:35,414 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:48:35,414 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:48:35,418 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimCLR --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 17:48:40,430 - wandb.wandb_agent - INFO - Running runs: ['2wozn83k']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174841-2wozn83k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-316
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2wozn83k
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2wozn83k
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 4.9932 | Val Loss: 4.3629
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.2288 | Val Loss: 4.0027
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.8709 | Val Loss: 3.6060
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.6178 | Val Loss: 3.4620
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.3905 | Val Loss: 3.2629
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.2002 | Val Loss: 3.1375
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.0756 | Val Loss: 2.9836
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.9508 | Val Loss: 2.8879
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.8775 | Val Loss: 2.7862
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.330 MB uploadedwandb: | 137.311 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.79171
wandb:   val_loss 2.7427
wandb: 
wandb: üöÄ View run fanciful-sweep-316 at: https://wandb.ai/7shoe/domShift-extensive/runs/2wozn83k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174841-2wozn83k/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7917 | Val Loss: 2.7427
2025-03-26 17:49:15,898 - wandb.wandb_agent - INFO - Cleaning up finished run: 2wozn83k
2025-03-26 17:49:16,726 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:16,726 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:49:16,729 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:49:21,740 - wandb.wandb_agent - INFO - Running runs: ['j4j282n1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174923-j4j282n1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-320
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/j4j282n1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: j4j282n1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.7470 | Val Loss: 1.5134
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.3587 | Val Loss: 1.2465
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.1997 | Val Loss: 1.1465
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.1126 | Val Loss: 1.0747
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.0490 | Val Loss: 1.0146
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0104 | Val Loss: 1.0174
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0400 | Val Loss: 1.0674
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0940 | Val Loss: 1.1182
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.1305 | Val Loss: 1.1360
wandb: - 32.850 MB of 32.850 MB uploadedwandb: \ 32.850 MB of 32.850 MB uploadedwandb: | 32.869 MB of 32.869 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.12568
wandb:   val_loss 1.10374
wandb: 
wandb: üöÄ View run honest-sweep-320 at: https://wandb.ai/7shoe/domShift-extensive/runs/j4j282n1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174923-j4j282n1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.1257 | Val Loss: 1.1037
2025-03-26 17:49:52,148 - wandb.wandb_agent - INFO - Cleaning up finished run: j4j282n1
2025-03-26 17:49:52,906 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:49:52,906 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:49:52,909 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 17:49:57,922 - wandb.wandb_agent - INFO - Running runs: ['tb8ebc98']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_174959-tb8ebc98
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-325
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tb8ebc98
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: tb8ebc98
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9016 | Val Loss: 1.6829
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4483 | Val Loss: 1.2797
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2169 | Val Loss: 1.1729
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1333 | Val Loss: 1.1081
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1092 | Val Loss: 1.1074
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0996 | Val Loss: 1.0863
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0648 | Val Loss: 1.0511
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0480 | Val Loss: 1.0470
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0690 | Val Loss: 1.0872
wandb: - 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08919
wandb:   val_loss 1.08286
wandb: 
wandb: üöÄ View run giddy-sweep-325 at: https://wandb.ai/7shoe/domShift-extensive/runs/tb8ebc98
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_174959-tb8ebc98/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0892 | Val Loss: 1.0829
2025-03-26 17:50:33,440 - wandb.wandb_agent - INFO - Cleaning up finished run: tb8ebc98
2025-03-26 17:50:34,148 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:50:34,149 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:50:34,151 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 17:50:39,163 - wandb.wandb_agent - INFO - Running runs: ['hrsdf85e']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175039-hrsdf85e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-329
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/hrsdf85e
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: hrsdf85e
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5719 | Val Loss: 1.5910
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.5808 | Val Loss: 1.5361
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5050 | Val Loss: 1.4821
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4605 | Val Loss: 1.4448
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4373 | Val Loss: 1.4284
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4148 | Val Loss: 1.3908
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3916 | Val Loss: 1.3907
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3875 | Val Loss: 1.3801
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3747 | Val Loss: 1.3693
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36773
wandb:   val_loss 1.36754
wandb: 
wandb: üöÄ View run lemon-sweep-329 at: https://wandb.ai/7shoe/domShift-extensive/runs/hrsdf85e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175039-hrsdf85e/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3677 | Val Loss: 1.3675
2025-03-26 17:51:24,794 - wandb.wandb_agent - INFO - Cleaning up finished run: hrsdf85e
2025-03-26 17:51:25,535 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:51:25,535 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 17:51:25,538 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 17:51:30,551 - wandb.wandb_agent - INFO - Running runs: ['10ey457r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175131-10ey457r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-334
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/10ey457r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 10ey457r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 2.4771 | Val Loss: 2.1306
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.9775 | Val Loss: 1.8990
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.8934 | Val Loss: 1.8973
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.9139 | Val Loss: 1.9326
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.9523 | Val Loss: 1.9713
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.9885 | Val Loss: 2.0047
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 2.0185 | Val Loss: 2.0313
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 2.0430 | Val Loss: 2.0543
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 2.0648 | Val Loss: 2.0750
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.415 MB uploadedwandb: | 137.256 MB of 137.415 MB uploadedwandb: / 137.415 MB of 137.415 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.08448
wandb:   val_loss 2.09373
wandb: 
wandb: üöÄ View run ancient-sweep-334 at: https://wandb.ai/7shoe/domShift-extensive/runs/10ey457r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175131-10ey457r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 2.0845 | Val Loss: 2.0937
2025-03-26 17:52:56,883 - wandb.wandb_agent - INFO - Cleaning up finished run: 10ey457r
2025-03-26 17:52:58,130 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:52:58,130 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:52:58,133 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.1
2025-03-26 17:53:03,145 - wandb.wandb_agent - INFO - Running runs: ['9h9r5vv1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175304-9h9r5vv1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-345
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9h9r5vv1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9h9r5vv1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.9724 | Val Loss: 1.5500
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.4134 | Val Loss: 1.2308
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.1914 | Val Loss: 1.1208
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.1056 | Val Loss: 1.0757
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.0713 | Val Loss: 1.0589
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0570 | Val Loss: 1.0457
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.0429 | Val Loss: 1.0279
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.0217 | Val Loss: 1.0049
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 0.9992 | Val Loss: 0.9861
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.98194
wandb:   val_loss 0.9723
wandb: 
wandb: üöÄ View run resilient-sweep-345 at: https://wandb.ai/7shoe/domShift-extensive/runs/9h9r5vv1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175304-9h9r5vv1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 0.9819 | Val Loss: 0.9723
2025-03-26 17:53:33,534 - wandb.wandb_agent - INFO - Cleaning up finished run: 9h9r5vv1
2025-03-26 17:53:34,114 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:53:34,115 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:53:34,117 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:53:39,130 - wandb.wandb_agent - INFO - Running runs: ['8lb4ptpi']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175339-8lb4ptpi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-347
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8lb4ptpi
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 8lb4ptpi
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4663 | Val Loss: 1.4748
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2446 | Val Loss: 1.1360
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2462 | Val Loss: 1.4224
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3509 | Val Loss: 1.3376
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3296 | Val Loss: 1.3025
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2545 | Val Loss: 1.1873
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1128 | Val Loss: 1.0659
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0533 | Val Loss: 1.0505
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0487 | Val Loss: 1.0453
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04268
wandb:   val_loss 1.04016
wandb: 
wandb: üöÄ View run radiant-sweep-347 at: https://wandb.ai/7shoe/domShift-extensive/runs/8lb4ptpi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175339-8lb4ptpi/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0427 | Val Loss: 1.0402
2025-03-26 17:54:45,049 - wandb.wandb_agent - INFO - Cleaning up finished run: 8lb4ptpi
2025-03-26 17:54:49,835 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:54:49,836 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:54:49,838 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 17:54:54,851 - wandb.wandb_agent - INFO - Running runs: ['97xkstsm']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175456-97xkstsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-356
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/97xkstsm
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 97xkstsm
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5448 | Val Loss: 1.2752
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2773 | Val Loss: 1.2680
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2032 | Val Loss: 1.1420
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1119 | Val Loss: 1.0812
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0642 | Val Loss: 1.0517
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0499 | Val Loss: 1.0478
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0438 | Val Loss: 1.0387
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0365 | Val Loss: 1.0329
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0187 | Val Loss: 0.9677
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95716
wandb:   val_loss 0.96194
wandb: 
wandb: üöÄ View run lunar-sweep-356 at: https://wandb.ai/7shoe/domShift-extensive/runs/97xkstsm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175456-97xkstsm/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9572 | Val Loss: 0.9619
2025-03-26 17:55:35,398 - wandb.wandb_agent - INFO - Cleaning up finished run: 97xkstsm
2025-03-26 17:55:36,193 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:55:36,194 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:55:36,197 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:55:41,210 - wandb.wandb_agent - INFO - Running runs: ['2krp3roz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175542-2krp3roz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-361
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2krp3roz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2krp3roz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.8695 | Val Loss: 1.6775
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.6288 | Val Loss: 1.5665
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.5838 | Val Loss: 1.6192
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.5557 | Val Loss: 1.5437
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.4965 | Val Loss: 1.3933
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.2539 | Val Loss: 1.1519
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.1413 | Val Loss: 1.1408
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.1561 | Val Loss: 1.1745
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.1828 | Val Loss: 1.1892
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.279 MB of 137.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19039
wandb:   val_loss 1.19022
wandb: 
wandb: üöÄ View run autumn-sweep-361 at: https://wandb.ai/7shoe/domShift-extensive/runs/2krp3roz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175542-2krp3roz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.1904 | Val Loss: 1.1902
2025-03-26 17:56:21,787 - wandb.wandb_agent - INFO - Cleaning up finished run: 2krp3roz
2025-03-26 17:56:22,480 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:56:22,480 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 17:56:22,483 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 17:56:27,496 - wandb.wandb_agent - INFO - Running runs: ['gnf96vdh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175627-gnf96vdh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-367
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gnf96vdh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gnf96vdh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3414 | Val Loss: 1.2107
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2117 | Val Loss: 1.2215
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2439 | Val Loss: 1.2532
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2594 | Val Loss: 1.2670
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2736 | Val Loss: 1.2777
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2845 | Val Loss: 1.2913
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2979 | Val Loss: 1.3078
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3111 | Val Loss: 1.3177
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3163 | Val Loss: 1.2861
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.29251
wandb:   val_loss 1.30663
wandb: 
wandb: üöÄ View run fresh-sweep-367 at: https://wandb.ai/7shoe/domShift-extensive/runs/gnf96vdh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175627-gnf96vdh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2925 | Val Loss: 1.3066
2025-03-26 17:57:43,594 - wandb.wandb_agent - INFO - Cleaning up finished run: gnf96vdh
2025-03-26 17:57:44,616 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:57:44,616 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 17:57:44,619 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 17:57:49,632 - wandb.wandb_agent - INFO - Running runs: ['1nsd2iqw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175749-1nsd2iqw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-376
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1nsd2iqw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 1nsd2iqw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3167 | Val Loss: 1.1012
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1308 | Val Loss: 1.1201
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1505 | Val Loss: 1.1650
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1900 | Val Loss: 1.1701
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1030 | Val Loss: 1.0354
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0118 | Val Loss: 1.0936
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0905 | Val Loss: 1.0837
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1195 | Val Loss: 1.1262
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1232 | Val Loss: 1.1029
wandb: - 34.351 MB of 34.351 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ
wandb:   val_loss ‚ñÑ‚ñÖ‚ñà‚ñà‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11355
wandb:   val_loss 1.17279
wandb: 
wandb: üöÄ View run vital-sweep-376 at: https://wandb.ai/7shoe/domShift-extensive/runs/1nsd2iqw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175749-1nsd2iqw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1135 | Val Loss: 1.1728
2025-03-26 17:58:25,137 - wandb.wandb_agent - INFO - Cleaning up finished run: 1nsd2iqw
2025-03-26 17:58:25,749 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:58:25,750 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 17:58:25,753 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 17:58:30,765 - wandb.wandb_agent - INFO - Running runs: ['3ayygyn3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175831-3ayygyn3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-381
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3ayygyn3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3ayygyn3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2091 | Val Loss: 1.0985
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1439 | Val Loss: 1.1901
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1841 | Val Loss: 1.1350
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0358 | Val Loss: 1.0406
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0192 | Val Loss: 1.0348
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0571 | Val Loss: 1.0609
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0655 | Val Loss: 1.0755
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0699 | Val Loss: 1.0664
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0695 | Val Loss: 1.0636
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:   val_loss ‚ñÑ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.05351
wandb:   val_loss 1.04907
wandb: 
wandb: üöÄ View run comic-sweep-381 at: https://wandb.ai/7shoe/domShift-extensive/runs/3ayygyn3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175831-3ayygyn3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0535 | Val Loss: 1.0491
2025-03-26 17:59:26,535 - wandb.wandb_agent - INFO - Cleaning up finished run: 3ayygyn3
2025-03-26 17:59:27,130 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 17:59:27,130 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 17:59:27,134 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 17:59:32,149 - wandb.wandb_agent - INFO - Running runs: ['6dnexqrf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_175933-6dnexqrf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-387
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6dnexqrf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 6dnexqrf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6894 | Val Loss: 1.3181
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2054 | Val Loss: 1.1226
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1212 | Val Loss: 1.1299
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1553 | Val Loss: 1.1892
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2216 | Val Loss: 1.2510
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2598 | Val Loss: 1.2649
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2761 | Val Loss: 1.2886
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3090 | Val Loss: 1.3345
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3568 | Val Loss: 1.3806
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.873 MB of 32.873 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.38861
wandb:   val_loss 1.39255
wandb: 
wandb: üöÄ View run glorious-sweep-387 at: https://wandb.ai/7shoe/domShift-extensive/runs/6dnexqrf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_175933-6dnexqrf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3886 | Val Loss: 1.3925
2025-03-26 18:00:02,572 - wandb.wandb_agent - INFO - Cleaning up finished run: 6dnexqrf
2025-03-26 18:00:03,225 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:03,226 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:00:03,228 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:00:08,240 - wandb.wandb_agent - INFO - Running runs: ['ioaid3x2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180008-ioaid3x2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-390
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ioaid3x2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ioaid3x2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5175 | Val Loss: 1.0427
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 0.9524 | Val Loss: 0.9247
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9526 | Val Loss: 1.0057
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0469 | Val Loss: 1.0778
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0139 | Val Loss: 0.9963
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9644 | Val Loss: 0.9429
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9326 | Val Loss: 0.9275
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9294 | Val Loss: 0.9317
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9353 | Val Loss: 0.9401
wandb: - 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.9462
wandb:   val_loss 0.93499
wandb: 
wandb: üöÄ View run glad-sweep-390 at: https://wandb.ai/7shoe/domShift-extensive/runs/ioaid3x2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180008-ioaid3x2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9462 | Val Loss: 0.9350
2025-03-26 18:00:53,832 - wandb.wandb_agent - INFO - Cleaning up finished run: ioaid3x2
2025-03-26 18:00:54,324 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:00:54,324 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:00:54,326 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:00:59,339 - wandb.wandb_agent - INFO - Running runs: ['o07h77q6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180100-o07h77q6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-397
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o07h77q6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o07h77q6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3059 | Val Loss: 0.9493
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9517 | Val Loss: 0.9565
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9534 | Val Loss: 0.9525
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9528 | Val Loss: 0.9528
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9527 | Val Loss: 0.9528
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9527 | Val Loss: 0.9528
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9527 | Val Loss: 0.9528
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9527 | Val Loss: 0.9528
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9527 | Val Loss: 0.9528
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.95272
wandb:   val_loss 0.95278
wandb: 
wandb: üöÄ View run hardy-sweep-397 at: https://wandb.ai/7shoe/domShift-extensive/runs/o07h77q6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180100-o07h77q6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9527 | Val Loss: 0.9528
2025-03-26 18:02:10,298 - wandb.wandb_agent - INFO - Cleaning up finished run: o07h77q6
2025-03-26 18:02:11,027 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:02:11,027 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:02:11,030 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:02:16,040 - wandb.wandb_agent - INFO - Running runs: ['775wupx3']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180216-775wupx3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-404
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/775wupx3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 775wupx3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6904 | Val Loss: 1.5670
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5452 | Val Loss: 1.4659
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4337 | Val Loss: 1.4108
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4207 | Val Loss: 1.4374
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4526 | Val Loss: 1.4709
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4819 | Val Loss: 1.4972
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5026 | Val Loss: 1.4900
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4790 | Val Loss: 1.4693
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4727 | Val Loss: 1.4730
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.47304
wandb:   val_loss 1.47043
wandb: 
wandb: üöÄ View run quiet-sweep-404 at: https://wandb.ai/7shoe/domShift-extensive/runs/775wupx3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180216-775wupx3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4730 | Val Loss: 1.4704
2025-03-26 18:02:46,423 - wandb.wandb_agent - INFO - Cleaning up finished run: 775wupx3
2025-03-26 18:02:47,136 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:02:47,136 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:02:47,139 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:02:52,152 - wandb.wandb_agent - INFO - Running runs: ['s0760tuz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180253-s0760tuz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-407
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s0760tuz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s0760tuz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1670 | Val Loss: 0.9910
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0132 | Val Loss: 1.0046
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0442 | Val Loss: 1.0709
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0731 | Val Loss: 1.0739
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0758 | Val Loss: 1.0779
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0808 | Val Loss: 1.0831
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0847 | Val Loss: 1.0868
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0887 | Val Loss: 1.0904
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0923 | Val Loss: 1.0934
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.473 MB uploadedwandb: | 137.314 MB of 137.473 MB uploadedwandb: / 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.09472
wandb:   val_loss 1.09572
wandb: 
wandb: üöÄ View run proud-sweep-407 at: https://wandb.ai/7shoe/domShift-extensive/runs/s0760tuz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180253-s0760tuz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0947 | Val Loss: 1.0957
2025-03-26 18:04:08,190 - wandb.wandb_agent - INFO - Cleaning up finished run: s0760tuz
2025-03-26 18:04:08,822 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:04:08,822 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:04:08,825 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:04:13,837 - wandb.wandb_agent - INFO - Running runs: ['2ql1271r']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180413-2ql1271r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-416
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2ql1271r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2ql1271r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5129 | Val Loss: 1.4880
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4249 | Val Loss: 1.4409
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4395 | Val Loss: 1.4649
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4793 | Val Loss: 1.5115
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.5291 | Val Loss: 1.5591
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5683 | Val Loss: 1.5860
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5900 | Val Loss: 1.6022
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6066 | Val Loss: 1.6204
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.6263 | Val Loss: 1.6379
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.63968
wandb:   val_loss 1.64599
wandb: 
wandb: üöÄ View run lilac-sweep-416 at: https://wandb.ai/7shoe/domShift-extensive/runs/2ql1271r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180413-2ql1271r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.6397 | Val Loss: 1.6460
2025-03-26 18:04:44,313 - wandb.wandb_agent - INFO - Cleaning up finished run: 2ql1271r
2025-03-26 18:04:44,847 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:04:44,847 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:04:44,850 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:04:49,862 - wandb.wandb_agent - INFO - Running runs: ['bocsd8ae']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180449-bocsd8ae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-420
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/bocsd8ae
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: bocsd8ae
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4157 | Val Loss: 1.3188
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2174 | Val Loss: 1.1831
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1774 | Val Loss: 1.1765
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1945 | Val Loss: 1.1992
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1844 | Val Loss: 1.1778
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1852 | Val Loss: 1.1850
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.1829 | Val Loss: 1.1778
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1836 | Val Loss: 1.1654
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1636 | Val Loss: 1.1574
wandb: - 10.265 MB of 10.265 MB uploadedwandb: \ 10.265 MB of 10.265 MB uploadedwandb: | 10.285 MB of 10.285 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.1533
wandb:   val_loss 1.18398
wandb: 
wandb: üöÄ View run summer-sweep-420 at: https://wandb.ai/7shoe/domShift-extensive/runs/bocsd8ae
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180449-bocsd8ae/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1533 | Val Loss: 1.1840
2025-03-26 18:05:25,394 - wandb.wandb_agent - INFO - Cleaning up finished run: bocsd8ae
2025-03-26 18:05:25,872 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:05:25,872 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:05:25,875 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 18:05:30,888 - wandb.wandb_agent - INFO - Running runs: ['qdrb58ub']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180531-qdrb58ub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-425
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/qdrb58ub
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: qdrb58ub
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0163 | Val Loss: 1.6905
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6535 | Val Loss: 1.6461
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6593 | Val Loss: 1.6907
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.6929 | Val Loss: 1.6648
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.6315 | Val Loss: 1.5692
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5413 | Val Loss: 1.5211
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.5032 | Val Loss: 1.4617
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4478 | Val Loss: 1.4236
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4021 | Val Loss: 1.3778
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.3655
wandb:   val_loss 1.35037
wandb: 
wandb: üöÄ View run dandy-sweep-425 at: https://wandb.ai/7shoe/domShift-extensive/runs/qdrb58ub
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180531-qdrb58ub/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3655 | Val Loss: 1.3504
2025-03-26 18:06:06,370 - wandb.wandb_agent - INFO - Cleaning up finished run: qdrb58ub
2025-03-26 18:06:07,883 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:07,884 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:06:07,886 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=BYOL --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:06:12,899 - wandb.wandb_agent - INFO - Running runs: ['mqa37xig']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180612-mqa37xig
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-431
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/mqa37xig
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: mqa37xig
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 2.9212 | Val Loss: 2.5883
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 2.6070 | Val Loss: 2.5984
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 2.5051 | Val Loss: 2.4137
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 2.3365 | Val Loss: 2.2812
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 2.3136 | Val Loss: 2.3521
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 2.3848 | Val Loss: 2.4183
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 2.4465 | Val Loss: 2.4742
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 2.4974 | Val Loss: 2.5205
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 2.5386 | Val Loss: 2.5560
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.872 MB of 32.872 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.56892
wandb:   val_loss 2.58111
wandb: 
wandb: üöÄ View run astral-sweep-431 at: https://wandb.ai/7shoe/domShift-extensive/runs/mqa37xig
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180612-mqa37xig/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 2.5689 | Val Loss: 2.5811
2025-03-26 18:06:48,345 - wandb.wandb_agent - INFO - Cleaning up finished run: mqa37xig
2025-03-26 18:06:48,894 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:06:48,894 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: cnn
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.8
2025-03-26 18:06:48,897 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=cnn --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.8
2025-03-26 18:06:53,910 - wandb.wandb_agent - INFO - Running runs: ['l1d7rqgr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180654-l1d7rqgr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-436
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l1d7rqgr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: l1d7rqgr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 2.8536 | Val Loss: 2.3690
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.2383 | Val Loss: 2.1277
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.0711 | Val Loss: 2.0092
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.9801 | Val Loss: 1.9564
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.9442 | Val Loss: 1.9347
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.9308 | Val Loss: 1.9285
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.9265 | Val Loss: 1.9044
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.8955 | Val Loss: 1.8855
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.8795 | Val Loss: 1.8743
wandb: - 10.260 MB of 10.260 MB uploadedwandb: \ 10.260 MB of 10.279 MB uploadedwandb: | 10.260 MB of 10.279 MB uploadedwandb: / 10.279 MB of 10.279 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.87118
wandb:   val_loss 1.86889
wandb: 
wandb: üöÄ View run lemon-sweep-436 at: https://wandb.ai/7shoe/domShift-extensive/runs/l1d7rqgr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180654-l1d7rqgr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.8712 | Val Loss: 1.8689
2025-03-26 18:07:29,385 - wandb.wandb_agent - INFO - Cleaning up finished run: l1d7rqgr
2025-03-26 18:07:29,921 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:07:29,921 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:07:29,924 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:07:34,936 - wandb.wandb_agent - INFO - Running runs: ['s5kzmjn8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180736-s5kzmjn8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-441
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s5kzmjn8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s5kzmjn8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9805 | Val Loss: 1.9153
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.6176 | Val Loss: 1.2369
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2306 | Val Loss: 1.1743
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1421 | Val Loss: 1.1169
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0916 | Val Loss: 1.1230
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1594 | Val Loss: 1.1894
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2131 | Val Loss: 1.2315
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2419 | Val Loss: 1.2485
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2515 | Val Loss: 1.2514
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25073
wandb:   val_loss 1.24939
wandb: 
wandb: üöÄ View run denim-sweep-441 at: https://wandb.ai/7shoe/domShift-extensive/runs/s5kzmjn8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180736-s5kzmjn8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2507 | Val Loss: 1.2494
2025-03-26 18:08:25,639 - wandb.wandb_agent - INFO - Cleaning up finished run: s5kzmjn8
2025-03-26 18:08:26,133 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:08:26,133 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:08:26,136 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:08:31,148 - wandb.wandb_agent - INFO - Running runs: ['5q948nx0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180832-5q948nx0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-447
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5q948nx0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5q948nx0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5379 | Val Loss: 1.4057
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1332 | Val Loss: 1.0179
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9196 | Val Loss: 0.8249
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.8143 | Val Loss: 0.8076
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.8061 | Val Loss: 0.8050
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.8046 | Val Loss: 0.8043
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.8056 | Val Loss: 0.8076
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.8109 | Val Loss: 0.8137
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.8179 | Val Loss: 0.8218
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.82654
wandb:   val_loss 0.83111
wandb: 
wandb: üöÄ View run splendid-sweep-447 at: https://wandb.ai/7shoe/domShift-extensive/runs/5q948nx0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180832-5q948nx0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.8265 | Val Loss: 0.8311
2025-03-26 18:09:37,107 - wandb.wandb_agent - INFO - Cleaning up finished run: 5q948nx0
2025-03-26 18:09:37,879 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:09:37,879 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:09:37,882 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:09:42,894 - wandb.wandb_agent - INFO - Running runs: ['m0c8quwf']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_180942-m0c8quwf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-454
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m0c8quwf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: m0c8quwf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3106 | Val Loss: 1.1308
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1107 | Val Loss: 1.1172
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1275 | Val Loss: 1.1291
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1343 | Val Loss: 1.1370
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1368 | Val Loss: 1.0630
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0159 | Val Loss: 1.0140
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0450 | Val Loss: 1.0523
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0486 | Val Loss: 1.0493
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0488 | Val Loss: 1.0490
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñá‚ñà‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.04889
wandb:   val_loss 1.04896
wandb: 
wandb: üöÄ View run dainty-sweep-454 at: https://wandb.ai/7shoe/domShift-extensive/runs/m0c8quwf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_180942-m0c8quwf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0489 | Val Loss: 1.0490
2025-03-26 18:10:58,968 - wandb.wandb_agent - INFO - Cleaning up finished run: m0c8quwf
2025-03-26 18:10:59,404 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:10:59,404 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:10:59,407 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:11:04,420 - wandb.wandb_agent - INFO - Running runs: ['8qlnb5is']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181105-8qlnb5is
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-463
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8qlnb5is
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 8qlnb5is
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2640 | Val Loss: 1.2591
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2274 | Val Loss: 1.1001
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0823 | Val Loss: 1.0800
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0703 | Val Loss: 1.0547
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0606 | Val Loss: 1.0728
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0680 | Val Loss: 1.0566
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0595 | Val Loss: 1.0708
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0631 | Val Loss: 1.0527
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0595 | Val Loss: 1.0699
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.06027
wandb:   val_loss 1.04949
wandb: 
wandb: üöÄ View run earnest-sweep-463 at: https://wandb.ai/7shoe/domShift-extensive/runs/8qlnb5is
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181105-8qlnb5is/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0603 | Val Loss: 1.0495
2025-03-26 18:12:15,415 - wandb.wandb_agent - INFO - Cleaning up finished run: 8qlnb5is
2025-03-26 18:12:15,870 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:12:15,870 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:12:15,873 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:12:20,888 - wandb.wandb_agent - INFO - Running runs: ['uiedsbkr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181222-uiedsbkr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-473
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/uiedsbkr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: uiedsbkr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.0667 | Val Loss: 0.8149
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.8619 | Val Loss: 0.9074
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9146 | Val Loss: 0.9231
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9245 | Val Loss: 0.9229
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9199 | Val Loss: 0.9170
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9166 | Val Loss: 0.9124
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9128 | Val Loss: 0.9091
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9095 | Val Loss: 0.9066
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9070 | Val Loss: 0.9044
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.90496
wandb:   val_loss 0.90285
wandb: 
wandb: üöÄ View run fluent-sweep-473 at: https://wandb.ai/7shoe/domShift-extensive/runs/uiedsbkr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181222-uiedsbkr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9050 | Val Loss: 0.9028
2025-03-26 18:13:01,433 - wandb.wandb_agent - INFO - Cleaning up finished run: uiedsbkr
2025-03-26 18:13:02,062 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:13:02,062 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:13:02,065 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:13:07,077 - wandb.wandb_agent - INFO - Running runs: ['d5usla7o']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181308-d5usla7o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-476
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/d5usla7o
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: d5usla7o
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5201 | Val Loss: 1.6452
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5106 | Val Loss: 1.4286
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4102 | Val Loss: 1.3907
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3814 | Val Loss: 1.3703
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3539 | Val Loss: 1.3396
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3317 | Val Loss: 1.3253
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3229 | Val Loss: 1.3217
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3224 | Val Loss: 1.3228
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3226 | Val Loss: 1.3239
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32542
wandb:   val_loss 1.32609
wandb: 
wandb: üöÄ View run fine-sweep-476 at: https://wandb.ai/7shoe/domShift-extensive/runs/d5usla7o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181308-d5usla7o/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3254 | Val Loss: 1.3261
2025-03-26 18:14:18,119 - wandb.wandb_agent - INFO - Cleaning up finished run: d5usla7o
2025-03-26 18:14:18,775 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:14:18,775 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:14:18,778 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:14:23,790 - wandb.wandb_agent - INFO - Running runs: ['o0d9z0sa']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181424-o0d9z0sa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-483
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o0d9z0sa
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o0d9z0sa
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.6986 | Val Loss: 1.5856
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.5112 | Val Loss: 1.3583
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.1149 | Val Loss: 0.8660
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 0.9113 | Val Loss: 0.9182
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 0.8801 | Val Loss: 0.8622
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 0.8608 | Val Loss: 0.8582
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 0.8600 | Val Loss: 0.8621
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 0.8653 | Val Loss: 0.8676
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 0.8700 | Val Loss: 0.8720
wandb: - 137.417 MB of 137.417 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.87422
wandb:   val_loss 0.87609
wandb: 
wandb: üöÄ View run light-sweep-483 at: https://wandb.ai/7shoe/domShift-extensive/runs/o0d9z0sa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181424-o0d9z0sa/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 0.8742 | Val Loss: 0.8761
2025-03-26 18:15:39,868 - wandb.wandb_agent - INFO - Cleaning up finished run: o0d9z0sa
2025-03-26 18:15:40,462 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:15:40,462 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:15:40,466 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:15:45,479 - wandb.wandb_agent - INFO - Running runs: ['9f0zxdpr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181546-9f0zxdpr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-490
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9f0zxdpr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9f0zxdpr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5096 | Val Loss: 1.3437
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3395 | Val Loss: 1.3200
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3848 | Val Loss: 1.4327
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.4394 | Val Loss: 1.4517
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4561 | Val Loss: 1.4671
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.4544 | Val Loss: 1.4520
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.4501 | Val Loss: 1.4557
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4493 | Val Loss: 1.4423
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4439 | Val Loss: 1.4470
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 33.064 MB of 33.064 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.44278
wandb:   val_loss 1.4328
wandb: 
wandb: üöÄ View run volcanic-sweep-490 at: https://wandb.ai/7shoe/domShift-extensive/runs/9f0zxdpr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181546-9f0zxdpr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4428 | Val Loss: 1.4328
2025-03-26 18:16:36,169 - wandb.wandb_agent - INFO - Cleaning up finished run: 9f0zxdpr
2025-03-26 18:16:36,936 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:16:36,936 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:16:36,942 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:16:41,954 - wandb.wandb_agent - INFO - Running runs: ['b9gf9g4r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181643-b9gf9g4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-495
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/b9gf9g4r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: b9gf9g4r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3936 | Val Loss: 1.2063
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1464 | Val Loss: 1.1053
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0821 | Val Loss: 1.0642
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0495 | Val Loss: 1.0285
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0171 | Val Loss: 1.0058
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9966 | Val Loss: 0.9874
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9827 | Val Loss: 0.9768
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9731 | Val Loss: 0.9701
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9704 | Val Loss: 0.9698
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.96834
wandb:   val_loss 0.96658
wandb: 
wandb: üöÄ View run glorious-sweep-495 at: https://wandb.ai/7shoe/domShift-extensive/runs/b9gf9g4r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181643-b9gf9g4r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9683 | Val Loss: 0.9666
2025-03-26 18:17:37,790 - wandb.wandb_agent - INFO - Cleaning up finished run: b9gf9g4r
2025-03-26 18:17:38,605 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:17:38,605 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:17:38,608 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:17:43,621 - wandb.wandb_agent - INFO - Running runs: ['h2iev9y1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181744-h2iev9y1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-499
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/h2iev9y1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: h2iev9y1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2330 | Val Loss: 1.0765
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0617 | Val Loss: 1.0629
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0618 | Val Loss: 1.0691
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0697 | Val Loss: 1.0742
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0732 | Val Loss: 1.0779
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0777 | Val Loss: 1.0829
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0833 | Val Loss: 1.0892
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0905 | Val Loss: 1.0978
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1007 | Val Loss: 1.1089
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.773 MB uploadedwandb: | 43.729 MB of 43.773 MB uploadedwandb: / 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.10831
wandb:   val_loss 1.0966
wandb: 
wandb: üöÄ View run flowing-sweep-499 at: https://wandb.ai/7shoe/domShift-extensive/runs/h2iev9y1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181744-h2iev9y1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1083 | Val Loss: 1.0966
2025-03-26 18:18:34,313 - wandb.wandb_agent - INFO - Cleaning up finished run: h2iev9y1
2025-03-26 18:18:35,165 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:18:35,165 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:18:35,168 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:18:40,181 - wandb.wandb_agent - INFO - Running runs: ['ywn6ixby']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_181840-ywn6ixby
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-505
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ywn6ixby
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ywn6ixby
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.6173 | Val Loss: 2.3955
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.5429 | Val Loss: 2.6361
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6883 | Val Loss: 2.7311
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7698 | Val Loss: 2.8061
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8420 | Val Loss: 2.8767
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.9103 | Val Loss: 2.9429
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.9732 | Val Loss: 3.0018
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.0276 | Val Loss: 3.0500
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.0727 | Val Loss: 3.0927
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.469 MB uploadedwandb: | 137.311 MB of 137.469 MB uploadedwandb: / 137.469 MB of 137.469 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 3.10603
wandb:   val_loss 3.11247
wandb: 
wandb: üöÄ View run fiery-sweep-505 at: https://wandb.ai/7shoe/domShift-extensive/runs/ywn6ixby
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_181840-ywn6ixby/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 3.1060 | Val Loss: 3.1125
2025-03-26 18:20:06,527 - wandb.wandb_agent - INFO - Cleaning up finished run: ywn6ixby
2025-03-26 18:20:07,216 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:20:07,216 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:20:07,219 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:20:12,232 - wandb.wandb_agent - INFO - Running runs: ['34sp8xd1']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182013-34sp8xd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-513
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/34sp8xd1
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 34sp8xd1
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1760 | Val Loss: 0.9858
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0391 | Val Loss: 1.0690
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0713 | Val Loss: 1.0737
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0757 | Val Loss: 1.0762
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0772 | Val Loss: 1.0771
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0775 | Val Loss: 1.0773
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0777 | Val Loss: 1.0774
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0778 | Val Loss: 1.0774
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0778 | Val Loss: 1.0775
wandb: - 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.07788
wandb:   val_loss 1.07755
wandb: 
wandb: üöÄ View run chocolate-sweep-513 at: https://wandb.ai/7shoe/domShift-extensive/runs/34sp8xd1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182013-34sp8xd1/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0779 | Val Loss: 1.0776
2025-03-26 18:21:28,391 - wandb.wandb_agent - INFO - Cleaning up finished run: 34sp8xd1
2025-03-26 18:21:28,950 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:21:28,950 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:21:28,953 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:21:33,965 - wandb.wandb_agent - INFO - Running runs: ['a76oxzlv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182134-a76oxzlv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-523
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/a76oxzlv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: a76oxzlv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.2227 | Val Loss: 3.0268
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.9920 | Val Loss: 2.9870
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.9773 | Val Loss: 2.9529
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.9295 | Val Loss: 2.9057
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.8868 | Val Loss: 2.8699
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.8223 | Val Loss: 2.7953
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.7872 | Val Loss: 2.7833
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.7824 | Val Loss: 2.7821
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.7802 | Val Loss: 2.7782
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.77614
wandb:   val_loss 2.774
wandb: 
wandb: üöÄ View run graceful-sweep-523 at: https://wandb.ai/7shoe/domShift-extensive/runs/a76oxzlv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182134-a76oxzlv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7761 | Val Loss: 2.7740
2025-03-26 18:23:00,252 - wandb.wandb_agent - INFO - Cleaning up finished run: a76oxzlv
2025-03-26 18:23:00,871 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:23:00,871 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 18:23:00,875 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 18:23:05,888 - wandb.wandb_agent - INFO - Running runs: ['o84aqo5s']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182305-o84aqo5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-530
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o84aqo5s
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: o84aqo5s
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7833 | Val Loss: 1.4285
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2693 | Val Loss: 0.9532
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.9008 | Val Loss: 0.8761
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9121 | Val Loss: 0.9264
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.9261 | Val Loss: 0.9278
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9280 | Val Loss: 0.9285
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9282 | Val Loss: 0.9286
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9283 | Val Loss: 0.9287
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9283 | Val Loss: 0.9288
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.949 MB of 32.949 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.9284
wandb:   val_loss 0.92888
wandb: 
wandb: üöÄ View run silvery-sweep-530 at: https://wandb.ai/7shoe/domShift-extensive/runs/o84aqo5s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182305-o84aqo5s/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.9284 | Val Loss: 0.9289
2025-03-26 18:23:51,534 - wandb.wandb_agent - INFO - Cleaning up finished run: o84aqo5s
2025-03-26 18:23:52,008 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:23:52,008 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:23:52,011 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 18:23:57,024 - wandb.wandb_agent - INFO - Running runs: ['lpkvtbg3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182358-lpkvtbg3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-535
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/lpkvtbg3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: lpkvtbg3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 3.6405 | Val Loss: 3.0751
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 2.8438 | Val Loss: 2.6671
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 2.6950 | Val Loss: 2.7263
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 2.7479 | Val Loss: 2.7539
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 2.7594 | Val Loss: 2.7622
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 2.7665 | Val Loss: 2.7698
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 2.7744 | Val Loss: 2.7785
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 2.7836 | Val Loss: 2.7871
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 2.7898 | Val Loss: 2.7900
wandb: - 32.964 MB of 32.964 MB uploadedwandb: \ 32.964 MB of 32.964 MB uploadedwandb: | 32.984 MB of 32.984 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.79016
wandb:   val_loss 2.78838
wandb: 
wandb: üöÄ View run summer-sweep-535 at: https://wandb.ai/7shoe/domShift-extensive/runs/lpkvtbg3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182358-lpkvtbg3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 2.7902 | Val Loss: 2.7884
2025-03-26 18:24:32,508 - wandb.wandb_agent - INFO - Cleaning up finished run: lpkvtbg3
2025-03-26 18:24:33,159 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:24:33,160 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimCLR
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:24:33,163 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimCLR --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:24:38,175 - wandb.wandb_agent - INFO - Running runs: ['o2k2h1g2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182439-o2k2h1g2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-539
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o2k2h1g2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: o2k2h1g2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 5.0647 | Val Loss: 4.1854
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 4.0489 | Val Loss: 3.5644
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.6774 | Val Loss: 3.2132
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.2531 | Val Loss: 2.8345
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.9777 | Val Loss: 2.6396
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.7494 | Val Loss: 2.4292
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.5321 | Val Loss: 2.2237
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.3965 | Val Loss: 2.1554
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.3048 | Val Loss: 2.0610
wandb: - 137.037 MB of 137.037 MB uploadedwandb: \ 137.037 MB of 137.037 MB uploadedwandb: | 137.056 MB of 137.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.20294
wandb:   val_loss 1.98739
wandb: 
wandb: üöÄ View run stellar-sweep-539 at: https://wandb.ai/7shoe/domShift-extensive/runs/o2k2h1g2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182439-o2k2h1g2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimCLR-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.2029 | Val Loss: 1.9874
2025-03-26 18:25:23,819 - wandb.wandb_agent - INFO - Cleaning up finished run: o2k2h1g2
2025-03-26 18:25:24,331 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:25:24,331 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:25:24,334 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:25:29,349 - wandb.wandb_agent - INFO - Running runs: ['k7zf8fsr']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182529-k7zf8fsr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-544
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/k7zf8fsr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: k7zf8fsr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5828 | Val Loss: 1.6163
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5960 | Val Loss: 1.5331
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5500 | Val Loss: 1.5381
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3225 | Val Loss: 1.2767
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2860 | Val Loss: 1.2903
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2893 | Val Loss: 1.2890
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2893 | Val Loss: 1.2890
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2893 | Val Loss: 1.2890
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2893 | Val Loss: 1.2890
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.2893
wandb:   val_loss 1.28901
wandb: 
wandb: üöÄ View run honest-sweep-544 at: https://wandb.ai/7shoe/domShift-extensive/runs/k7zf8fsr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182529-k7zf8fsr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2893 | Val Loss: 1.2890
2025-03-26 18:26:45,508 - wandb.wandb_agent - INFO - Cleaning up finished run: k7zf8fsr
2025-03-26 18:26:46,168 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:26:46,168 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:26:46,171 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:26:51,183 - wandb.wandb_agent - INFO - Running runs: ['pbgve0k4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182652-pbgve0k4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-552
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/pbgve0k4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: pbgve0k4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5665 | Val Loss: 1.4909
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4553 | Val Loss: 1.4125
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3960 | Val Loss: 1.3810
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3962 | Val Loss: 1.4241
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.4539 | Val Loss: 1.4770
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.5074 | Val Loss: 1.5291
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.5179 | Val Loss: 1.4966
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.4866 | Val Loss: 1.4812
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.4838 | Val Loss: 1.4823
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.991 MB uploadedwandb: | 32.971 MB of 32.991 MB uploadedwandb: / 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÜ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.48257
wandb:   val_loss 1.48451
wandb: 
wandb: üöÄ View run likely-sweep-552 at: https://wandb.ai/7shoe/domShift-extensive/runs/pbgve0k4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182652-pbgve0k4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.4826 | Val Loss: 1.4845
2025-03-26 18:27:31,773 - wandb.wandb_agent - INFO - Cleaning up finished run: pbgve0k4
2025-03-26 18:27:32,270 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:27:32,270 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:27:32,273 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:27:37,284 - wandb.wandb_agent - INFO - Running runs: ['z68kaydw']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182737-z68kaydw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-557
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/z68kaydw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: z68kaydw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6764 | Val Loss: 1.7346
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7896 | Val Loss: 1.8588
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.8003 | Val Loss: 1.6935
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4899 | Val Loss: 1.3832
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4793 | Val Loss: 1.4070
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3431 | Val Loss: 1.3238
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3270 | Val Loss: 1.3096
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3005 | Val Loss: 1.2944
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2896 | Val Loss: 1.2838
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.471 MB uploadedwandb: | 137.313 MB of 137.471 MB uploadedwandb: / 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñá‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.27377
wandb:   val_loss 1.26523
wandb: 
wandb: üöÄ View run clear-sweep-557 at: https://wandb.ai/7shoe/domShift-extensive/runs/z68kaydw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182737-z68kaydw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2738 | Val Loss: 1.2652
2025-03-26 18:28:48,274 - wandb.wandb_agent - INFO - Cleaning up finished run: z68kaydw
2025-03-26 18:28:48,778 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:28:48,779 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.5
2025-03-26 18:28:48,781 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.5
2025-03-26 18:28:53,794 - wandb.wandb_agent - INFO - Running runs: ['ylvmr3cu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182853-ylvmr3cu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-563
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ylvmr3cu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: ylvmr3cu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3258 | Val Loss: 1.3853
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4431 | Val Loss: 1.4283
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4694 | Val Loss: 1.5122
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5073 | Val Loss: 1.4919
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4720 | Val Loss: 1.4447
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4156 | Val Loss: 1.3912
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3868 | Val Loss: 1.3874
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3841 | Val Loss: 1.3781
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3651 | Val Loss: 1.3518
wandb: - 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.33773
wandb:   val_loss 1.32105
wandb: 
wandb: üöÄ View run rich-sweep-563 at: https://wandb.ai/7shoe/domShift-extensive/runs/ylvmr3cu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182853-ylvmr3cu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3377 | Val Loss: 1.3210
2025-03-26 18:29:29,285 - wandb.wandb_agent - INFO - Cleaning up finished run: ylvmr3cu
2025-03-26 18:29:29,813 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:29:29,813 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:29:29,816 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:29:34,828 - wandb.wandb_agent - INFO - Running runs: ['nkw4gvc9']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_182936-nkw4gvc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-568
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/nkw4gvc9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: nkw4gvc9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5553 | Val Loss: 1.5099
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3469 | Val Loss: 1.1997
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3176 | Val Loss: 1.2432
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2021 | Val Loss: 1.1254
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0751 | Val Loss: 1.0407
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0460 | Val Loss: 1.0524
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0564 | Val Loss: 1.0564
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0546 | Val Loss: 1.0499
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0483 | Val Loss: 1.0458
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.046
wandb:   val_loss 1.05074
wandb: 
wandb: üöÄ View run different-sweep-568 at: https://wandb.ai/7shoe/domShift-extensive/runs/nkw4gvc9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_182936-nkw4gvc9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0460 | Val Loss: 1.0507
2025-03-26 18:30:51,222 - wandb.wandb_agent - INFO - Cleaning up finished run: nkw4gvc9
2025-03-26 18:30:51,703 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:30:51,704 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:30:51,707 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 18:30:56,719 - wandb.wandb_agent - INFO - Running runs: ['4rxmj4to']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183057-4rxmj4to
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-576
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4rxmj4to
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4rxmj4to
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5762 | Val Loss: 1.5754
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4524 | Val Loss: 1.4391
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4425 | Val Loss: 1.3310
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0658 | Val Loss: 1.0086
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0024 | Val Loss: 1.0022
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0041 | Val Loss: 1.0025
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0042 | Val Loss: 1.0029
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0042 | Val Loss: 1.0029
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0042 | Val Loss: 1.0029
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.00427
wandb:   val_loss 1.003
wandb: 
wandb: üöÄ View run ruby-sweep-576 at: https://wandb.ai/7shoe/domShift-extensive/runs/4rxmj4to
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183057-4rxmj4to/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0043 | Val Loss: 1.0030
2025-03-26 18:32:12,808 - wandb.wandb_agent - INFO - Cleaning up finished run: 4rxmj4to
2025-03-26 18:32:13,352 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:32:13,352 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:32:13,355 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:32:18,368 - wandb.wandb_agent - INFO - Running runs: ['6t3dxkmx']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183218-6t3dxkmx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-582
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6t3dxkmx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6t3dxkmx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3251 | Val Loss: 1.2142
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2384 | Val Loss: 1.2396
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2374 | Val Loss: 1.2353
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2369 | Val Loss: 1.2339
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2362 | Val Loss: 1.2341
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2358 | Val Loss: 1.2332
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2353 | Val Loss: 1.2329
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2349 | Val Loss: 1.2352
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2342 | Val Loss: 1.2303
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.23204
wandb:   val_loss 1.23122
wandb: 
wandb: üöÄ View run smart-sweep-582 at: https://wandb.ai/7shoe/domShift-extensive/runs/6t3dxkmx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183218-6t3dxkmx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2320 | Val Loss: 1.2312
2025-03-26 18:33:29,365 - wandb.wandb_agent - INFO - Cleaning up finished run: 6t3dxkmx
2025-03-26 18:33:29,884 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:33:29,884 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:33:29,887 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:33:34,900 - wandb.wandb_agent - INFO - Running runs: ['o6y7248x']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183336-o6y7248x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-591
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/o6y7248x
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: o6y7248x
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5659 | Val Loss: 1.5030
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4646 | Val Loss: 1.4274
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4685 | Val Loss: 1.5257
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4516 | Val Loss: 1.1628
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2812 | Val Loss: 1.4364
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.5046 | Val Loss: 1.4784
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4194 | Val Loss: 1.3580
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3712 | Val Loss: 1.3804
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3280 | Val Loss: 1.2409
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.472 MB uploadedwandb: | 137.314 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.29874
wandb:   val_loss 1.3175
wandb: 
wandb: üöÄ View run expert-sweep-591 at: https://wandb.ai/7shoe/domShift-extensive/runs/o6y7248x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183336-o6y7248x/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2987 | Val Loss: 1.3175
2025-03-26 18:34:50,947 - wandb.wandb_agent - INFO - Cleaning up finished run: o6y7248x
2025-03-26 18:34:51,992 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:34:51,992 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:34:51,995 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.6000000000000001
2025-03-26 18:34:57,007 - wandb.wandb_agent - INFO - Running runs: ['rkwf39pf']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183458-rkwf39pf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-598
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rkwf39pf
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: rkwf39pf
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.0942 | Val Loss: 0.8538
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 0.8533 | Val Loss: 0.8902
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 0.8919 | Val Loss: 0.8890
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 0.8898 | Val Loss: 0.8899
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 0.8898 | Val Loss: 0.8902
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 0.8898 | Val Loss: 0.8897
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 0.8896 | Val Loss: 0.8899
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 0.8895 | Val Loss: 0.8897
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 0.8895 | Val Loss: 0.8896
wandb: - 137.259 MB of 137.259 MB uploadedwandb: \ 137.259 MB of 137.259 MB uploadedwandb: | 137.418 MB of 137.418 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.88938
wandb:   val_loss 0.8896
wandb: 
wandb: üöÄ View run devout-sweep-598 at: https://wandb.ai/7shoe/domShift-extensive/runs/rkwf39pf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183458-rkwf39pf/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 0.8894 | Val Loss: 0.8896
2025-03-26 18:36:13,089 - wandb.wandb_agent - INFO - Cleaning up finished run: rkwf39pf
2025-03-26 18:36:14,825 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:36:14,825 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:36:14,828 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:36:19,840 - wandb.wandb_agent - INFO - Running runs: ['1ldpgfg7']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183620-1ldpgfg7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-608
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/1ldpgfg7
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 1ldpgfg7
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.5271 | Val Loss: 2.2647
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3316 | Val Loss: 2.4163
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2842 | Val Loss: 2.1648
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1239 | Val Loss: 2.0986
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0967 | Val Loss: 2.0929
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1223 | Val Loss: 2.1601
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1395 | Val Loss: 2.1033
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1028 | Val Loss: 2.1076
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1259 | Val Loss: 2.1495
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.286 MB of 137.286 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.18073
wandb:   val_loss 2.2155
wandb: 
wandb: üöÄ View run twilight-sweep-608 at: https://wandb.ai/7shoe/domShift-extensive/runs/1ldpgfg7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183620-1ldpgfg7/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1807 | Val Loss: 2.2155
2025-03-26 18:37:20,740 - wandb.wandb_agent - INFO - Cleaning up finished run: 1ldpgfg7
2025-03-26 18:37:21,256 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:37:21,256 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.6000000000000001
2025-03-26 18:37:21,259 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.6000000000000001
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:37:26,272 - wandb.wandb_agent - INFO - Running runs: ['ax84ls0d']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183726-ax84ls0d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-616
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/ax84ls0d
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: ax84ls0d
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5309 | Val Loss: 1.4754
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5354 | Val Loss: 1.5890
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.6350 | Val Loss: 1.6886
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.7604 | Val Loss: 1.7997
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.8281 | Val Loss: 1.8585
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.8628 | Val Loss: 1.8405
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.7872 | Val Loss: 1.7342
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.6799 | Val Loss: 1.6149
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.5676 | Val Loss: 1.5343
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.53581
wandb:   val_loss 1.54571
wandb: 
wandb: üöÄ View run wandering-sweep-616 at: https://wandb.ai/7shoe/domShift-extensive/runs/ax84ls0d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183726-ax84ls0d/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.5358 | Val Loss: 1.5457
2025-03-26 18:38:17,010 - wandb.wandb_agent - INFO - Cleaning up finished run: ax84ls0d
2025-03-26 18:38:17,847 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:38:17,847 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:38:17,850 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:38:22,863 - wandb.wandb_agent - INFO - Running runs: ['sh5dzzyp']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183823-sh5dzzyp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-620
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/sh5dzzyp
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: sh5dzzyp
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3078 | Val Loss: 0.8453
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9768 | Val Loss: 1.1290
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2138 | Val Loss: 1.2004
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1378 | Val Loss: 1.0992
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0948 | Val Loss: 1.0961
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1080 | Val Loss: 1.1193
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1237 | Val Loss: 1.1250
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1252 | Val Loss: 1.1252
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1302 | Val Loss: 1.1362
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13988
wandb:   val_loss 1.14151
wandb: 
wandb: üöÄ View run quiet-sweep-620 at: https://wandb.ai/7shoe/domShift-extensive/runs/sh5dzzyp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183823-sh5dzzyp/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1399 | Val Loss: 1.1415
2025-03-26 18:39:18,641 - wandb.wandb_agent - INFO - Cleaning up finished run: sh5dzzyp
2025-03-26 18:39:19,266 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:39:19,266 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:39:19,268 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 18:39:24,281 - wandb.wandb_agent - INFO - Running runs: ['jao7701r']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_183925-jao7701r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-628
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jao7701r
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jao7701r
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5139 | Val Loss: 1.4772
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5294 | Val Loss: 1.5721
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5210 | Val Loss: 1.5003
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5168 | Val Loss: 1.5076
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4694 | Val Loss: 1.4366
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4237 | Val Loss: 1.3741
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3187 | Val Loss: 1.2748
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2554 | Val Loss: 1.2520
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2624 | Val Loss: 1.2786
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÉ
wandb:   val_loss ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.32434
wandb:   val_loss 1.37451
wandb: 
wandb: üöÄ View run visionary-sweep-628 at: https://wandb.ai/7shoe/domShift-extensive/runs/jao7701r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_183925-jao7701r/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3243 | Val Loss: 1.3745
2025-03-26 18:40:20,089 - wandb.wandb_agent - INFO - Cleaning up finished run: jao7701r
2025-03-26 18:40:20,649 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:40:20,650 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:40:20,652 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.4
2025-03-26 18:40:25,665 - wandb.wandb_agent - INFO - Running runs: ['57q89gjo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184026-57q89gjo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-633
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/57q89gjo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 57q89gjo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4211 | Val Loss: 1.2696
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3213 | Val Loss: 1.4045
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4234 | Val Loss: 1.3731
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3518 | Val Loss: 1.3367
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3450 | Val Loss: 1.3527
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3497 | Val Loss: 1.3459
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3456 | Val Loss: 1.3465
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3506 | Val Loss: 1.3528
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3559 | Val Loss: 1.3575
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.35838
wandb:   val_loss 1.3571
wandb: 
wandb: üöÄ View run stoic-sweep-633 at: https://wandb.ai/7shoe/domShift-extensive/runs/57q89gjo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184026-57q89gjo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3584 | Val Loss: 1.3571
2025-03-26 18:41:21,480 - wandb.wandb_agent - INFO - Cleaning up finished run: 57q89gjo
2025-03-26 18:41:22,405 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:41:22,405 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:41:22,408 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:41:27,420 - wandb.wandb_agent - INFO - Running runs: ['7u236jaj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184127-7u236jaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-640
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7u236jaj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7u236jaj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4027 | Val Loss: 1.2954
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3354 | Val Loss: 1.4457
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5139 | Val Loss: 1.3716
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1972 | Val Loss: 1.1375
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1044 | Val Loss: 1.0035
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.9052 | Val Loss: 0.8371
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.8280 | Val Loss: 0.8345
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8490 | Val Loss: 0.8597
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8644 | Val Loss: 0.8662
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÜ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.86658
wandb:   val_loss 0.86688
wandb: 
wandb: üöÄ View run comfy-sweep-640 at: https://wandb.ai/7shoe/domShift-extensive/runs/7u236jaj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184127-7u236jaj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8666 | Val Loss: 0.8669
2025-03-26 18:42:13,097 - wandb.wandb_agent - INFO - Cleaning up finished run: 7u236jaj
2025-03-26 18:42:13,610 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:42:13,610 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:42:13,614 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:42:18,627 - wandb.wandb_agent - INFO - Running runs: ['2tnoetgv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184219-2tnoetgv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-645
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2tnoetgv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2tnoetgv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.1881 | Val Loss: 1.2235
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0603 | Val Loss: 1.0093
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0344 | Val Loss: 1.0653
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1143 | Val Loss: 1.1462
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1496 | Val Loss: 1.1460
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1356 | Val Loss: 1.1306
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1402 | Val Loss: 1.1542
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1627 | Val Loss: 1.1626
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1599 | Val Loss: 1.1610
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.16812
wandb:   val_loss 1.17189
wandb: 
wandb: üöÄ View run royal-sweep-645 at: https://wandb.ai/7shoe/domShift-extensive/runs/2tnoetgv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184219-2tnoetgv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1681 | Val Loss: 1.1719
2025-03-26 18:43:14,436 - wandb.wandb_agent - INFO - Cleaning up finished run: 2tnoetgv
2025-03-26 18:43:15,001 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:43:15,001 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:43:15,004 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 18:43:20,016 - wandb.wandb_agent - INFO - Running runs: ['umuf9dfe']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184321-umuf9dfe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-651
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/umuf9dfe
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: umuf9dfe
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.4586 | Val Loss: 1.2451
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.1914 | Val Loss: 1.1370
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.0982 | Val Loss: 1.0611
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.0739 | Val Loss: 1.0922
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.0928 | Val Loss: 1.0913
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.0990 | Val Loss: 1.1112
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.1249 | Val Loss: 1.1405
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.1622 | Val Loss: 1.1829
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.2008 | Val Loss: 1.2232
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.24018
wandb:   val_loss 1.25254
wandb: 
wandb: üöÄ View run revived-sweep-651 at: https://wandb.ai/7shoe/domShift-extensive/runs/umuf9dfe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184321-umuf9dfe/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.2402 | Val Loss: 1.2525
2025-03-26 18:43:50,440 - wandb.wandb_agent - INFO - Cleaning up finished run: umuf9dfe
2025-03-26 18:43:51,323 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:43:51,323 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:43:51,326 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:43:56,339 - wandb.wandb_agent - INFO - Running runs: ['axft7xva']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184356-axft7xva
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-654
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/axft7xva
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: axft7xva
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7471 | Val Loss: 1.6427
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5978 | Val Loss: 1.6039
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5839 | Val Loss: 1.5691
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.5261 | Val Loss: 1.4145
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3656 | Val Loss: 1.3390
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3466 | Val Loss: 1.3488
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3323 | Val Loss: 1.3110
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2880 | Val Loss: 1.3003
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2994 | Val Loss: 1.2997
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30361
wandb:   val_loss 1.31164
wandb: 
wandb: üöÄ View run usual-sweep-654 at: https://wandb.ai/7shoe/domShift-extensive/runs/axft7xva
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184356-axft7xva/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3036 | Val Loss: 1.3116
2025-03-26 18:44:36,898 - wandb.wandb_agent - INFO - Cleaning up finished run: axft7xva
2025-03-26 18:44:37,557 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:44:37,557 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:44:37,560 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:44:42,573 - wandb.wandb_agent - INFO - Running runs: ['gs1marnr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184443-gs1marnr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-659
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gs1marnr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gs1marnr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2852 | Val Loss: 1.0399
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0782 | Val Loss: 1.1277
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1800 | Val Loss: 1.2257
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2573 | Val Loss: 1.3386
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3587 | Val Loss: 1.3288
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2945 | Val Loss: 1.2960
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3220 | Val Loss: 1.3261
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3168 | Val Loss: 1.3132
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3159 | Val Loss: 1.3159
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.31315
wandb:   val_loss 1.31409
wandb: 
wandb: üöÄ View run spring-sweep-659 at: https://wandb.ai/7shoe/domShift-extensive/runs/gs1marnr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184443-gs1marnr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3132 | Val Loss: 1.3141
2025-03-26 18:45:24,301 - wandb.wandb_agent - INFO - Cleaning up finished run: gs1marnr
2025-03-26 18:45:24,903 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:45:24,903 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:45:24,906 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 18:45:29,918 - wandb.wandb_agent - INFO - Running runs: ['rwu3cq4b']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184530-rwu3cq4b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-663
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rwu3cq4b
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: rwu3cq4b
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5484 | Val Loss: 1.5888
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.6009 | Val Loss: 1.5762
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.5588 | Val Loss: 1.5249
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4872 | Val Loss: 1.4355
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3805 | Val Loss: 1.3043
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2544 | Val Loss: 1.1921
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1770 | Val Loss: 1.1777
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1815 | Val Loss: 1.2026
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2277 | Val Loss: 1.2271
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:   val_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19482
wandb:   val_loss 1.17756
wandb: 
wandb: üöÄ View run golden-sweep-663 at: https://wandb.ai/7shoe/domShift-extensive/runs/rwu3cq4b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184530-rwu3cq4b/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1948 | Val Loss: 1.1776
2025-03-26 18:46:05,414 - wandb.wandb_agent - INFO - Cleaning up finished run: rwu3cq4b
2025-03-26 18:46:05,942 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:46:05,942 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:46:05,945 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 18:46:10,957 - wandb.wandb_agent - INFO - Running runs: ['w55wasmd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184611-w55wasmd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-668
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/w55wasmd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: w55wasmd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.9616 | Val Loss: 2.9960
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.9067 | Val Loss: 2.8039
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.7459 | Val Loss: 2.6934
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5460 | Val Loss: 2.5063
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5188 | Val Loss: 2.5401
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5665 | Val Loss: 2.5904
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6113 | Val Loss: 2.6290
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6409 | Val Loss: 2.6487
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6544 | Val Loss: 2.6570
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.748 MB of 43.748 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.6587
wandb:   val_loss 2.65835
wandb: 
wandb: üöÄ View run dry-sweep-668 at: https://wandb.ai/7shoe/domShift-extensive/runs/w55wasmd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184611-w55wasmd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.6587 | Val Loss: 2.6584
2025-03-26 18:46:51,515 - wandb.wandb_agent - INFO - Cleaning up finished run: w55wasmd
2025-03-26 18:46:52,052 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:46:52,052 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:46:52,056 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:46:57,068 - wandb.wandb_agent - INFO - Running runs: ['6ygmk0j3']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184657-6ygmk0j3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-671
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6ygmk0j3
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6ygmk0j3
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.8187 | Val Loss: 3.5971
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.4660 | Val Loss: 3.3596
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 3.3539 | Val Loss: 3.3432
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 3.3397 | Val Loss: 3.3096
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 3.2633 | Val Loss: 3.1979
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 3.1642 | Val Loss: 3.1317
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 3.1088 | Val Loss: 3.0768
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 3.0549 | Val Loss: 3.0318
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 3.0149 | Val Loss: 2.9885
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.330 MB uploadedwandb: | 137.311 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.96037
wandb:   val_loss 2.92442
wandb: 
wandb: üöÄ View run rare-sweep-671 at: https://wandb.ai/7shoe/domShift-extensive/runs/6ygmk0j3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184657-6ygmk0j3/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.9604 | Val Loss: 2.9244
2025-03-26 18:47:42,771 - wandb.wandb_agent - INFO - Cleaning up finished run: 6ygmk0j3
2025-03-26 18:47:43,389 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:47:43,389 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:47:43,392 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:47:48,405 - wandb.wandb_agent - INFO - Running runs: ['gmmv5yle']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184749-gmmv5yle
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-677
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gmmv5yle
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gmmv5yle
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.2021 | Val Loss: 1.1257
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.0860 | Val Loss: 1.0125
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 0.8646 | Val Loss: 0.7727
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.7754 | Val Loss: 0.7706
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.7757 | Val Loss: 0.7857
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.7920 | Val Loss: 0.7965
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.7981 | Val Loss: 0.7989
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.7995 | Val Loss: 0.7994
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.7997 | Val Loss: 0.7995
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.089 MB of 33.089 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.79979
wandb:   val_loss 0.79949
wandb: 
wandb: üöÄ View run twilight-sweep-677 at: https://wandb.ai/7shoe/domShift-extensive/runs/gmmv5yle
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184749-gmmv5yle/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.7998 | Val Loss: 0.7995
2025-03-26 18:48:34,092 - wandb.wandb_agent - INFO - Cleaning up finished run: gmmv5yle
2025-03-26 18:48:34,871 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:48:34,871 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:48:34,874 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 18:48:39,887 - wandb.wandb_agent - INFO - Running runs: ['3e3kvpko']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184841-3e3kvpko
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-683
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3e3kvpko
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 3e3kvpko
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4923 | Val Loss: 1.4168
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4656 | Val Loss: 1.5025
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.5231 | Val Loss: 1.5425
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.5649 | Val Loss: 1.6021
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.6782 | Val Loss: 1.7754
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.7634 | Val Loss: 1.7029
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.6407 | Val Loss: 1.6318
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.6768 | Val Loss: 1.7249
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.7491 | Val Loss: 1.7663
wandb: - 32.894 MB of 32.894 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÖ‚ñá‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.75955
wandb:   val_loss 1.74442
wandb: 
wandb: üöÄ View run peachy-sweep-683 at: https://wandb.ai/7shoe/domShift-extensive/runs/3e3kvpko
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184841-3e3kvpko/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.7596 | Val Loss: 1.7444
2025-03-26 18:49:10,308 - wandb.wandb_agent - INFO - Cleaning up finished run: 3e3kvpko
2025-03-26 18:49:10,860 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:49:10,860 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:49:10,864 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:49:15,876 - wandb.wandb_agent - INFO - Running runs: ['l29ej5wh']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_184916-l29ej5wh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-687
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l29ej5wh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: l29ej5wh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5551 | Val Loss: 1.4259
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4304 | Val Loss: 1.3178
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2020 | Val Loss: 1.1704
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2527 | Val Loss: 1.2599
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2272 | Val Loss: 1.2143
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2120 | Val Loss: 1.2079
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2040 | Val Loss: 1.2008
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.1973 | Val Loss: 1.1943
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.1911 | Val Loss: 1.1873
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19528
wandb:   val_loss 1.20591
wandb: 
wandb: üöÄ View run dry-sweep-687 at: https://wandb.ai/7shoe/domShift-extensive/runs/l29ej5wh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_184916-l29ej5wh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.1953 | Val Loss: 1.2059
2025-03-26 18:50:06,693 - wandb.wandb_agent - INFO - Cleaning up finished run: l29ej5wh
2025-03-26 18:50:07,373 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:50:07,373 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:50:07,377 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 18:50:12,390 - wandb.wandb_agent - INFO - Running runs: ['t6lnlxvr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185012-t6lnlxvr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-694
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/t6lnlxvr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: t6lnlxvr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5235 | Val Loss: 1.3394
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2919 | Val Loss: 1.2680
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2441 | Val Loss: 1.2194
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2062 | Val Loss: 1.1964
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1988 | Val Loss: 1.2011
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2017 | Val Loss: 1.1980
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.2205 | Val Loss: 1.2416
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.2529 | Val Loss: 1.2618
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2639 | Val Loss: 1.2650
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.241 MB of 137.241 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.26744
wandb:   val_loss 1.26957
wandb: 
wandb: üöÄ View run easy-sweep-694 at: https://wandb.ai/7shoe/domShift-extensive/runs/t6lnlxvr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185012-t6lnlxvr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2674 | Val Loss: 1.2696
2025-03-26 18:51:18,386 - wandb.wandb_agent - INFO - Cleaning up finished run: t6lnlxvr
2025-03-26 18:51:18,829 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:51:18,830 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:51:18,832 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:51:23,844 - wandb.wandb_agent - INFO - Running runs: ['i2d2h7ce']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185123-i2d2h7ce
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-700
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/i2d2h7ce
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: i2d2h7ce
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7523 | Val Loss: 1.3361
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4533 | Val Loss: 1.9013
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3790 | Val Loss: 1.2950
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3351 | Val Loss: 1.3755
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4090 | Val Loss: 1.4530
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4939 | Val Loss: 1.5149
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4496 | Val Loss: 1.2828
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1744 | Val Loss: 1.1211
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1294 | Val Loss: 1.1413
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15012
wandb:   val_loss 1.15239
wandb: 
wandb: üöÄ View run fast-sweep-700 at: https://wandb.ai/7shoe/domShift-extensive/runs/i2d2h7ce
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185123-i2d2h7ce/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1501 | Val Loss: 1.1524
2025-03-26 18:52:39,928 - wandb.wandb_agent - INFO - Cleaning up finished run: i2d2h7ce
2025-03-26 18:52:40,504 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:52:40,505 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:52:40,507 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:52:45,520 - wandb.wandb_agent - INFO - Running runs: ['tvhbhg41']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185245-tvhbhg41
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-709
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/tvhbhg41
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: tvhbhg41
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5322 | Val Loss: 1.4477
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3935 | Val Loss: 1.3585
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3288 | Val Loss: 1.3035
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2838 | Val Loss: 1.2741
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2720 | Val Loss: 1.2911
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3086 | Val Loss: 1.3321
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3265 | Val Loss: 1.3161
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3027 | Val Loss: 1.2886
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.2778 | Val Loss: 1.2664
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.749 MB uploadedwandb: | 43.730 MB of 43.749 MB uploadedwandb: / 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.26254
wandb:   val_loss 1.25994
wandb: 
wandb: üöÄ View run serene-sweep-709 at: https://wandb.ai/7shoe/domShift-extensive/runs/tvhbhg41
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185245-tvhbhg41/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.2625 | Val Loss: 1.2599
2025-03-26 18:53:15,989 - wandb.wandb_agent - INFO - Cleaning up finished run: tvhbhg41
2025-03-26 18:53:16,591 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:53:16,591 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 18:53:16,594 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 18:53:21,605 - wandb.wandb_agent - INFO - Running runs: ['za2dcwwu']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185321-za2dcwwu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-713
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/za2dcwwu
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: za2dcwwu
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.9144 | Val Loss: 1.5618
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4559 | Val Loss: 1.3740
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3667 | Val Loss: 1.3585
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.3652 | Val Loss: 1.3682
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.3757 | Val Loss: 1.3791
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.3852 | Val Loss: 1.3884
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.3918 | Val Loss: 1.3908
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.3878 | Val Loss: 1.3818
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.3795 | Val Loss: 1.3772
wandb: - 32.856 MB of 32.856 MB uploadedwandb: \ 32.856 MB of 32.856 MB uploadedwandb: | 32.876 MB of 32.876 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.37909
wandb:   val_loss 1.37962
wandb: 
wandb: üöÄ View run hearty-sweep-713 at: https://wandb.ai/7shoe/domShift-extensive/runs/za2dcwwu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185321-za2dcwwu/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.3791 | Val Loss: 1.3796
2025-03-26 18:53:52,011 - wandb.wandb_agent - INFO - Cleaning up finished run: za2dcwwu
2025-03-26 18:53:52,590 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:53:52,590 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 18:53:52,593 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:53:57,605 - wandb.wandb_agent - INFO - Running runs: ['omyrggxh']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185357-omyrggxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-717
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/omyrggxh
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: omyrggxh
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6973 | Val Loss: 1.4984
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4809 | Val Loss: 1.3970
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3063 | Val Loss: 1.2402
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2725 | Val Loss: 1.3117
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.2783 | Val Loss: 1.2399
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.2030 | Val Loss: 1.1697
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1652 | Val Loss: 1.1678
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1751 | Val Loss: 1.1817
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1882 | Val Loss: 1.1939
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.313 MB uploadedwandb: | 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.19857
wandb:   val_loss 1.20227
wandb: 
wandb: üöÄ View run ancient-sweep-717 at: https://wandb.ai/7shoe/domShift-extensive/runs/omyrggxh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185357-omyrggxh/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1986 | Val Loss: 1.2023
2025-03-26 18:55:08,530 - wandb.wandb_agent - INFO - Cleaning up finished run: omyrggxh
2025-03-26 18:55:09,174 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:55:09,174 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:55:09,177 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 18:55:14,189 - wandb.wandb_agent - INFO - Running runs: ['2h50ykx4']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185514-2h50ykx4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-726
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/2h50ykx4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 2h50ykx4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2045 | Val Loss: 1.1044
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0506 | Val Loss: 0.9787
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0021 | Val Loss: 0.9950
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0085 | Val Loss: 1.0182
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0112 | Val Loss: 1.0133
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0128 | Val Loss: 1.0114
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0120 | Val Loss: 1.0121
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0118 | Val Loss: 1.0123
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0118 | Val Loss: 1.0121
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.749 MB of 43.749 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.01177
wandb:   val_loss 1.01182
wandb: 
wandb: üöÄ View run amber-sweep-726 at: https://wandb.ai/7shoe/domShift-extensive/runs/2h50ykx4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185514-2h50ykx4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0118 | Val Loss: 1.0118
2025-03-26 18:55:54,756 - wandb.wandb_agent - INFO - Cleaning up finished run: 2h50ykx4
2025-03-26 18:56:14,832 - wandb.wandb_agent - INFO - Running runs: []
2025-03-26 18:56:17,874 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:56:17,874 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:56:17,877 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 18:56:22,890 - wandb.wandb_agent - INFO - Running runs: ['35vb72uo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185624-35vb72uo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-730
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/35vb72uo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 35vb72uo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 4.0684 | Val Loss: 3.5445
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 3.2252 | Val Loss: 2.8036
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 2.6102 | Val Loss: 2.3628
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 2.2271 | Val Loss: 2.0786
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 2.0161 | Val Loss: 1.9328
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.9138 | Val Loss: 1.8858
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.8838 | Val Loss: 1.8705
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.8693 | Val Loss: 1.8583
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.8606 | Val Loss: 1.8555
wandb: - 137.256 MB of 137.256 MB uploadedwandb: \ 137.256 MB of 137.256 MB uploadedwandb: | 137.276 MB of 137.276 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.85744
wandb:   val_loss 1.85175
wandb: 
wandb: üöÄ View run lively-sweep-730 at: https://wandb.ai/7shoe/domShift-extensive/runs/35vb72uo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185624-35vb72uo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.8574 | Val Loss: 1.8517
2025-03-26 18:56:58,362 - wandb.wandb_agent - INFO - Cleaning up finished run: 35vb72uo
2025-03-26 18:56:58,829 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:56:58,829 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:56:58,832 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:57:03,840 - wandb.wandb_agent - INFO - Running runs: ['56stjepd']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185704-56stjepd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-735
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/56stjepd
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 56stjepd
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.4208 | Val Loss: 1.2550
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1424 | Val Loss: 1.0355
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0078 | Val Loss: 1.0227
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.0722 | Val Loss: 1.1121
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.1255 | Val Loss: 1.1183
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.1026 | Val Loss: 1.0908
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.0886 | Val Loss: 1.0876
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.0862 | Val Loss: 1.0843
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.0832 | Val Loss: 1.0823
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.088 MB of 33.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08148
wandb:   val_loss 1.08066
wandb: 
wandb: üöÄ View run happy-sweep-735 at: https://wandb.ai/7shoe/domShift-extensive/runs/56stjepd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185704-56stjepd/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0815 | Val Loss: 1.0807
2025-03-26 18:57:49,528 - wandb.wandb_agent - INFO - Cleaning up finished run: 56stjepd
2025-03-26 18:57:50,214 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:57:50,215 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 18:57:50,218 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 18:57:55,231 - wandb.wandb_agent - INFO - Running runs: ['6qwmav03']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185756-6qwmav03
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-742
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6qwmav03
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6qwmav03
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.3903 | Val Loss: 1.0782
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.1096 | Val Loss: 1.1241
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1337 | Val Loss: 1.1429
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1440 | Val Loss: 1.1449
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1469 | Val Loss: 1.1474
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1483 | Val Loss: 1.1485
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1491 | Val Loss: 1.1489
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1495 | Val Loss: 1.1494
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1496 | Val Loss: 1.1495
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.14967
wandb:   val_loss 1.14939
wandb: 
wandb: üöÄ View run jolly-sweep-742 at: https://wandb.ai/7shoe/domShift-extensive/runs/6qwmav03
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185756-6qwmav03/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1497 | Val Loss: 1.1494
2025-03-26 18:59:06,230 - wandb.wandb_agent - INFO - Cleaning up finished run: 6qwmav03
2025-03-26 18:59:06,835 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 18:59:06,835 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 18:59:06,837 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 18:59:11,850 - wandb.wandb_agent - INFO - Running runs: ['k614r8n0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_185911-k614r8n0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-751
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/k614r8n0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: k614r8n0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6166 | Val Loss: 1.1798
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2573 | Val Loss: 1.3400
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3270 | Val Loss: 1.2842
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.2220 | Val Loss: 1.1633
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1279 | Val Loss: 1.0998
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1071 | Val Loss: 1.0808
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0569 | Val Loss: 1.0338
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0242 | Val Loss: 1.0192
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0148 | Val Loss: 1.0181
wandb: - 137.082 MB of 137.082 MB uploadedwandb: \ 137.082 MB of 137.082 MB uploadedwandb: | 137.240 MB of 137.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.02379
wandb:   val_loss 1.02776
wandb: 
wandb: üöÄ View run whole-sweep-751 at: https://wandb.ai/7shoe/domShift-extensive/runs/k614r8n0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_185911-k614r8n0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0238 | Val Loss: 1.0278
2025-03-26 19:00:12,715 - wandb.wandb_agent - INFO - Cleaning up finished run: k614r8n0
2025-03-26 19:00:13,172 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:13,172 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 4000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:00:13,175 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=4000 --optim=AdamW --temperature=0.2
2025-03-26 19:00:18,188 - wandb.wandb_agent - INFO - Running runs: ['7kqtwct0']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190019-7kqtwct0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-757
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/7kqtwct0
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 7kqtwct0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.4711 | Val Loss: 1.3585
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2732 | Val Loss: 1.2395
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2139 | Val Loss: 1.1777
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1437 | Val Loss: 1.1125
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1097 | Val Loss: 1.1183
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1361 | Val Loss: 1.1289
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1109 | Val Loss: 1.1031
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1068 | Val Loss: 1.1127
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1120 | Val Loss: 1.1135
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.331 MB uploadedwandb: | 137.311 MB of 137.331 MB uploadedwandb: / 137.331 MB of 137.331 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.11344
wandb:   val_loss 1.11387
wandb: 
wandb: üöÄ View run sleek-sweep-757 at: https://wandb.ai/7shoe/domShift-extensive/runs/7kqtwct0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190019-7kqtwct0/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1134 | Val Loss: 1.1139
2025-03-26 19:00:58,754 - wandb.wandb_agent - INFO - Cleaning up finished run: 7kqtwct0
2025-03-26 19:00:59,475 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:00:59,475 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:00:59,478 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.4
2025-03-26 19:01:04,490 - wandb.wandb_agent - INFO - Running runs: ['m71iy4e2']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190104-m71iy4e2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-762
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/m71iy4e2
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: m71iy4e2
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7659 | Val Loss: 1.5976
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3666 | Val Loss: 1.2747
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.2996 | Val Loss: 1.3421
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3462 | Val Loss: 1.3598
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3642 | Val Loss: 1.3673
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3680 | Val Loss: 1.3691
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3687 | Val Loss: 1.3693
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3687 | Val Loss: 1.3692
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3685 | Val Loss: 1.3689
wandb: - 137.313 MB of 137.313 MB uploadedwandb: \ 137.313 MB of 137.472 MB uploadedwandb: | 137.313 MB of 137.472 MB uploadedwandb: / 137.472 MB of 137.472 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.36835
wandb:   val_loss 1.3688
wandb: 
wandb: üöÄ View run restful-sweep-762 at: https://wandb.ai/7shoe/domShift-extensive/runs/m71iy4e2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190104-m71iy4e2/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3684 | Val Loss: 1.3688
2025-03-26 19:02:15,588 - wandb.wandb_agent - INFO - Cleaning up finished run: m71iy4e2
2025-03-26 19:02:16,208 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:02:16,208 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: BYOL
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.7000000000000001
2025-03-26 19:02:16,211 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=BYOL --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.7000000000000001
2025-03-26 19:02:21,224 - wandb.wandb_agent - INFO - Running runs: ['3ul3vuo8']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190222-3ul3vuo8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-771
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3ul3vuo8
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3ul3vuo8
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.4498 | Val Loss: 2.7044
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.7800 | Val Loss: 2.7745
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.6997 | Val Loss: 2.6296
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.5994 | Val Loss: 2.5818
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.5834 | Val Loss: 2.5875
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.5934 | Val Loss: 2.5994
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6030 | Val Loss: 2.6108
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.6219 | Val Loss: 2.6363
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.6467 | Val Loss: 2.6568
wandb: - 43.729 MB of 43.729 MB uploadedwandb: \ 43.729 MB of 43.729 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.65953
wandb:   val_loss 2.65926
wandb: 
wandb: üöÄ View run toasty-sweep-771 at: https://wandb.ai/7shoe/domShift-extensive/runs/3ul3vuo8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190222-3ul3vuo8/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-cnn-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.6595 | Val Loss: 2.6593
2025-03-26 19:03:06,841 - wandb.wandb_agent - INFO - Cleaning up finished run: 3ul3vuo8
2025-03-26 19:03:07,887 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:03:07,888 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:03:07,891 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=uniform --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:03:12,904 - wandb.wandb_agent - INFO - Running runs: ['gvumktpo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190314-gvumktpo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-777
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/gvumktpo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: gvumktpo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 1/10 Train Loss: 1.5260 | Val Loss: 1.2209
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 2/10 Train Loss: 1.2236 | Val Loss: 1.2236
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 3/10 Train Loss: 1.2515 | Val Loss: 1.2988
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 4/10 Train Loss: 1.2859 | Val Loss: 1.2649
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 5/10 Train Loss: 1.2618 | Val Loss: 1.2548
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 6/10 Train Loss: 1.2475 | Val Loss: 1.2449
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 7/10 Train Loss: 1.2443 | Val Loss: 1.2387
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 8/10 Train Loss: 1.2322 | Val Loss: 1.2267
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 9/10 Train Loss: 1.2248 | Val Loss: 1.2202
wandb: - 32.965 MB of 32.965 MB uploadedwandb: \ 32.965 MB of 32.965 MB uploadedwandb: | 32.985 MB of 32.985 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñÇ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.21173
wandb:   val_loss 1.20187
wandb: 
wandb: üöÄ View run wobbly-sweep-777 at: https://wandb.ai/7shoe/domShift-extensive/runs/gvumktpo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190314-gvumktpo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-uniform-basic-AdamW] Epoch 10/10 Train Loss: 1.2117 | Val Loss: 1.2019
2025-03-26 19:03:53,478 - wandb.wandb_agent - INFO - Cleaning up finished run: gvumktpo
2025-03-26 19:03:53,982 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:03:53,982 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:03:53,985 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.1
2025-03-26 19:03:58,998 - wandb.wandb_agent - INFO - Running runs: ['5of5cemv']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190400-5of5cemv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-782
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/5of5cemv
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: 5of5cemv
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.5171 | Val Loss: 1.3987
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.3473 | Val Loss: 1.2416
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1959 | Val Loss: 1.1423
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.1329 | Val Loss: 1.1306
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.1306 | Val Loss: 1.1271
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1303 | Val Loss: 1.1352
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1418 | Val Loss: 1.1469
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1525 | Val Loss: 1.1582
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1649 | Val Loss: 1.1709
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.083 MB uploadedwandb: | 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.17652
wandb:   val_loss 1.18199
wandb: 
wandb: üöÄ View run wise-sweep-782 at: https://wandb.ai/7shoe/domShift-extensive/runs/5of5cemv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190400-5of5cemv/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1765 | Val Loss: 1.1820
2025-03-26 19:04:44,634 - wandb.wandb_agent - INFO - Cleaning up finished run: 5of5cemv
2025-03-26 19:04:45,190 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:04:45,190 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.8
2025-03-26 19:04:45,194 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.8
2025-03-26 19:04:50,207 - wandb.wandb_agent - INFO - Running runs: ['vpab45vr']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190451-vpab45vr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-789
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vpab45vr
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vpab45vr
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9591 | Val Loss: 1.8592
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.7634 | Val Loss: 1.6169
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3520 | Val Loss: 1.0692
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9715 | Val Loss: 0.9448
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9531 | Val Loss: 0.9631
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 0.9722 | Val Loss: 0.9788
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9857 | Val Loss: 0.9914
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9981 | Val Loss: 1.0038
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0100 | Val Loss: 1.0154
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.470 MB uploadedwandb: | 137.311 MB of 137.470 MB uploadedwandb: / 137.470 MB of 137.470 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.02149
wandb:   val_loss 1.02697
wandb: 
wandb: üöÄ View run vocal-sweep-789 at: https://wandb.ai/7shoe/domShift-extensive/runs/vpab45vr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190451-vpab45vr/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0215 | Val Loss: 1.0270
2025-03-26 19:06:01,207 - wandb.wandb_agent - INFO - Cleaning up finished run: vpab45vr
2025-03-26 19:06:01,707 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:06:01,708 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 1000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:06:01,710 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=1000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:06:06,722 - wandb.wandb_agent - INFO - Running runs: ['6iui35ce']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190607-6iui35ce
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-796
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/6iui35ce
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 6iui35ce
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.6980 | Val Loss: 1.3384
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.2482 | Val Loss: 1.1573
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1555 | Val Loss: 1.1587
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1371 | Val Loss: 1.0962
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0727 | Val Loss: 1.0437
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0268 | Val Loss: 1.0004
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9861 | Val Loss: 0.9691
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9642 | Val Loss: 0.9682
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.9751 | Val Loss: 0.9935
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.0051
wandb:   val_loss 1.03524
wandb: 
wandb: üöÄ View run generous-sweep-796 at: https://wandb.ai/7shoe/domShift-extensive/runs/6iui35ce
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190607-6iui35ce/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.0051 | Val Loss: 1.0352
2025-03-26 19:06:37,164 - wandb.wandb_agent - INFO - Cleaning up finished run: 6iui35ce
2025-03-26 19:06:37,808 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:06:37,808 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: cnn
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:06:37,811 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=cnn --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:06:42,823 - wandb.wandb_agent - INFO - Running runs: ['3i6zfbj9']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190642-3i6zfbj9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-799
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/3i6zfbj9
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 3i6zfbj9
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2801 | Val Loss: 1.0216
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 0.9832 | Val Loss: 0.9769
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 0.9133 | Val Loss: 0.8906
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 0.9705 | Val Loss: 0.9944
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 0.9825 | Val Loss: 1.0013
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0073 | Val Loss: 1.0013
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 0.9973 | Val Loss: 0.9795
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 0.9781 | Val Loss: 0.9871
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 0.9907 | Val Loss: 0.9897
wandb: - 43.730 MB of 43.730 MB uploadedwandb: \ 43.730 MB of 43.730 MB uploadedwandb: | 43.773 MB of 43.773 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÜ‚ñÅ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.98829
wandb:   val_loss 0.99028
wandb: 
wandb: üöÄ View run sparkling-sweep-799 at: https://wandb.ai/7shoe/domShift-extensive/runs/3i6zfbj9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190642-3i6zfbj9/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-cnn-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 0.9883 | Val Loss: 0.9903
2025-03-26 19:07:28,458 - wandb.wandb_agent - INFO - Cleaning up finished run: 3i6zfbj9
2025-03-26 19:07:29,005 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:07:29,005 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:07:29,008 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
2025-03-26 19:07:34,021 - wandb.wandb_agent - INFO - Running runs: ['9q3g68kl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190735-9q3g68kl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-804
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/9q3g68kl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 9q3g68kl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2511 | Val Loss: 1.0825
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.0802 | Val Loss: 1.0759
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.0784 | Val Loss: 1.0822
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0867 | Val Loss: 1.0917
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0977 | Val Loss: 1.1038
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1098 | Val Loss: 1.1160
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1205 | Val Loss: 1.1252
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1298 | Val Loss: 1.1349
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1397 | Val Loss: 1.1451
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:   val_loss ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.15086
wandb:   val_loss 1.15694
wandb: 
wandb: üöÄ View run chocolate-sweep-804 at: https://wandb.ai/7shoe/domShift-extensive/runs/9q3g68kl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190735-9q3g68kl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.1509 | Val Loss: 1.1569
2025-03-26 19:08:50,093 - wandb.wandb_agent - INFO - Cleaning up finished run: 9q3g68kl
2025-03-26 19:08:50,832 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:08:50,833 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:08:50,836 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:08:55,847 - wandb.wandb_agent - INFO - Running runs: ['vtm4mpyo']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_190856-vtm4mpyo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-811
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/vtm4mpyo
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: vtm4mpyo
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.2955 | Val Loss: 1.2616
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2972 | Val Loss: 1.3062
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3028 | Val Loss: 1.3016
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3002 | Val Loss: 1.3004
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3003 | Val Loss: 1.3014
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3003 | Val Loss: 1.3008
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3003 | Val Loss: 1.3012
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3003 | Val Loss: 1.3011
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.3003 | Val Loss: 1.3010
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.311 MB uploadedwandb: | 137.471 MB of 137.471 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:   val_loss ‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.30029
wandb:   val_loss 1.30102
wandb: 
wandb: üöÄ View run major-sweep-811 at: https://wandb.ai/7shoe/domShift-extensive/runs/vtm4mpyo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_190856-vtm4mpyo/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.3003 | Val Loss: 1.3010
2025-03-26 19:10:06,798 - wandb.wandb_agent - INFO - Cleaning up finished run: vtm4mpyo
2025-03-26 19:10:07,351 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:07,351 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:10:07,354 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=BYOL --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:10:12,367 - wandb.wandb_agent - INFO - Running runs: ['jopwrbn6']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191012-jopwrbn6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-819
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/jopwrbn6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: jopwrbn6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.6426 | Val Loss: 2.3934
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 2.3423 | Val Loss: 2.2564
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.2190 | Val Loss: 2.1617
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.1297 | Val Loss: 2.0771
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0489 | Val Loss: 2.0124
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.0049 | Val Loss: 1.9913
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.9877 | Val Loss: 1.9798
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.9794 | Val Loss: 1.9785
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.9797 | Val Loss: 1.9801
wandb: - 137.311 MB of 137.311 MB uploadedwandb: \ 137.311 MB of 137.330 MB uploadedwandb: | 137.311 MB of 137.330 MB uploadedwandb: / 137.330 MB of 137.330 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.98362
wandb:   val_loss 1.98794
wandb: 
wandb: üöÄ View run fearless-sweep-819 at: https://wandb.ai/7shoe/domShift-extensive/runs/jopwrbn6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191012-jopwrbn6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-extremely_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.9836 | Val Loss: 1.9879
2025-03-26 19:10:47,829 - wandb.wandb_agent - INFO - Cleaning up finished run: jopwrbn6
2025-03-26 19:10:48,478 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:10:48,479 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.4
2025-03-26 19:10:48,482 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.4
2025-03-26 19:10:53,495 - wandb.wandb_agent - INFO - Running runs: ['i6xr82u6']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191054-i6xr82u6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-823
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/i6xr82u6
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: i6xr82u6
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6792 | Val Loss: 1.6088
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.5607 | Val Loss: 1.5006
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.4526 | Val Loss: 1.4174
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.4211 | Val Loss: 1.4227
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.4264 | Val Loss: 1.4285
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.4310 | Val Loss: 1.4312
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.4324 | Val Loss: 1.4318
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.4312 | Val Loss: 1.4282
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4258 | Val Loss: 1.4217
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.4189
wandb:   val_loss 1.41417
wandb: 
wandb: üöÄ View run rich-sweep-823 at: https://wandb.ai/7shoe/domShift-extensive/runs/i6xr82u6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191054-i6xr82u6/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4189 | Val Loss: 1.4142
2025-03-26 19:11:28,957 - wandb.wandb_agent - INFO - Cleaning up finished run: i6xr82u6
2025-03-26 19:11:29,454 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:11:29,454 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:11:29,458 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:11:34,471 - wandb.wandb_agent - INFO - Running runs: ['kai4etlj']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191135-kai4etlj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-829
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/kai4etlj
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: kai4etlj
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.3344 | Val Loss: 1.2068
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1891 | Val Loss: 1.2691
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.2566 | Val Loss: 1.2388
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2571 | Val Loss: 1.2496
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2552 | Val Loss: 1.2543
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2554 | Val Loss: 1.2561
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2553 | Val Loss: 1.2552
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2550 | Val Loss: 1.2546
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2551 | Val Loss: 1.2551
wandb: - 32.853 MB of 32.853 MB uploadedwandb: \ 32.853 MB of 32.853 MB uploadedwandb: | 32.947 MB of 32.947 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   val_loss ‚ñÅ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.25509
wandb:   val_loss 1.25481
wandb: 
wandb: üöÄ View run whole-sweep-829 at: https://wandb.ai/7shoe/domShift-extensive/runs/kai4etlj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191135-kai4etlj/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2551 | Val Loss: 1.2548
2025-03-26 19:12:20,115 - wandb.wandb_agent - INFO - Cleaning up finished run: kai4etlj
2025-03-26 19:12:20,655 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:12:20,656 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 8000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:12:20,659 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=basic --n_train=8000 --optim=AdamW --temperature=0.2
2025-03-26 19:12:25,671 - wandb.wandb_agent - INFO - Running runs: ['j10h00xx']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191225-j10h00xx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-834
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/j10h00xx
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: j10h00xx
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.5994 | Val Loss: 1.2048
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.1618 | Val Loss: 1.1188
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.0530 | Val Loss: 0.9499
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 4/10 Train Loss: 0.9175 | Val Loss: 0.8988
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 5/10 Train Loss: 0.8853 | Val Loss: 0.8769
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 6/10 Train Loss: 0.8750 | Val Loss: 0.8732
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.8738 | Val Loss: 0.8734
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.8730 | Val Loss: 0.8722
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8725 | Val Loss: 0.8727
wandb: - 33.014 MB of 33.014 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.87348
wandb:   val_loss 0.87394
wandb: 
wandb: üöÄ View run wandering-sweep-834 at: https://wandb.ai/7shoe/domShift-extensive/runs/j10h00xx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191225-j10h00xx/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-extremely_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8735 | Val Loss: 0.8739
2025-03-26 19:13:06,235 - wandb.wandb_agent - INFO - Cleaning up finished run: j10h00xx
2025-03-26 19:13:06,889 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:13:06,890 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:13:06,893 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:13:11,906 - wandb.wandb_agent - INFO - Running runs: ['s3bpgbso']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191312-s3bpgbso
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-839
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/s3bpgbso
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: s3bpgbso
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.6432 | Val Loss: 1.5848
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.2937 | Val Loss: 1.1725
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1312 | Val Loss: 1.0960
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0897 | Val Loss: 1.0801
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0731 | Val Loss: 1.0669
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.0624 | Val Loss: 1.0579
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.0545 | Val Loss: 1.0510
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.0482 | Val Loss: 1.0453
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.0430 | Val Loss: 1.0405
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.473 MB of 137.473 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.03856
wandb:   val_loss 1.03659
wandb: 
wandb: üöÄ View run stilted-sweep-839 at: https://wandb.ai/7shoe/domShift-extensive/runs/s3bpgbso
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191312-s3bpgbso/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0386 | Val Loss: 1.0366
2025-03-26 19:14:23,389 - wandb.wandb_agent - INFO - Cleaning up finished run: s3bpgbso
2025-03-26 19:14:24,055 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:14:24,055 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 1000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:14:24,058 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=1000 --optim=AdamW --temperature=0.1
2025-03-26 19:14:29,071 - wandb.wandb_agent - INFO - Running runs: ['wg2ju3l4']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191430-wg2ju3l4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-847
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wg2ju3l4
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: wg2ju3l4
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 2.0358 | Val Loss: 1.6375
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4876 | Val Loss: 1.2588
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.1860 | Val Loss: 1.0984
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.0912 | Val Loss: 1.0818
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.0867 | Val Loss: 1.0913
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.1001 | Val Loss: 1.1087
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.1136 | Val Loss: 1.1134
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.1128 | Val Loss: 1.1071
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.1043 | Val Loss: 1.0946
wandb: - 137.083 MB of 137.083 MB uploadedwandb: \ 137.083 MB of 137.102 MB uploadedwandb: | 137.083 MB of 137.102 MB uploadedwandb: / 137.102 MB of 137.102 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.08995
wandb:   val_loss 1.07538
wandb: 
wandb: üöÄ View run robust-sweep-847 at: https://wandb.ai/7shoe/domShift-extensive/runs/wg2ju3l4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191430-wg2ju3l4/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.0900 | Val Loss: 1.0754
2025-03-26 19:14:59,441 - wandb.wandb_agent - INFO - Cleaning up finished run: wg2ju3l4
2025-03-26 19:15:00,049 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:15:00,049 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: uniform
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:15:00,051 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=uniform --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:15:05,064 - wandb.wandb_agent - INFO - Running runs: ['yopd7ajw']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191506-yopd7ajw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-851
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/yopd7ajw
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: yopd7ajw
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 1/10 Train Loss: 1.4495 | Val Loss: 1.2697
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 2/10 Train Loss: 1.2465 | Val Loss: 1.2269
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 3/10 Train Loss: 1.2099 | Val Loss: 1.1944
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 4/10 Train Loss: 1.1855 | Val Loss: 1.1785
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 5/10 Train Loss: 1.1757 | Val Loss: 1.1724
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 6/10 Train Loss: 1.1703 | Val Loss: 1.1677
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 7/10 Train Loss: 1.1653 | Val Loss: 1.1617
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 8/10 Train Loss: 1.1557 | Val Loss: 1.1336
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 9/10 Train Loss: 1.1303 | Val Loss: 1.1327
wandb: - 137.027 MB of 137.027 MB uploadedwandb: \ 137.027 MB of 137.027 MB uploadedwandb: | 137.186 MB of 137.186 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.13482
wandb:   val_loss 1.13598
wandb: 
wandb: üöÄ View run copper-sweep-851 at: https://wandb.ai/7shoe/domShift-extensive/runs/yopd7ajw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191506-yopd7ajw/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-MNIST-uniform-advanced-AdamW] Epoch 10/10 Train Loss: 1.1348 | Val Loss: 1.1360
2025-03-26 19:16:11,004 - wandb.wandb_agent - INFO - Cleaning up finished run: yopd7ajw
2025-03-26 19:16:11,486 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:16:11,486 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 4000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:16:11,490 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=SimSiam --model_class=vit --model_type=basic --n_train=4000 --optim=AdamW --temperature=0.1
2025-03-26 19:16:16,503 - wandb.wandb_agent - INFO - Running runs: ['l7tyxm73']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191617-l7tyxm73
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-857
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/l7tyxm73
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: l7tyxm73
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7817 | Val Loss: 1.4850
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.3808 | Val Loss: 1.2852
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.1917 | Val Loss: 1.1157
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.1065 | Val Loss: 1.0836
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.0609 | Val Loss: 1.0281
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.0032 | Val Loss: 0.9748
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 7/10 Train Loss: 0.9547 | Val Loss: 0.9288
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 8/10 Train Loss: 0.9032 | Val Loss: 0.8718
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 9/10 Train Loss: 0.8491 | Val Loss: 0.8255
wandb: - 32.971 MB of 32.971 MB uploadedwandb: \ 32.971 MB of 32.971 MB uploadedwandb: | 32.991 MB of 32.991 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 0.81269
wandb:   val_loss 0.79944
wandb: 
wandb: üöÄ View run cosmic-sweep-857 at: https://wandb.ai/7shoe/domShift-extensive/runs/l7tyxm73
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191617-l7tyxm73/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-heavily_skewed-basic-AdamW] Epoch 10/10 Train Loss: 0.8127 | Val Loss: 0.7994
2025-03-26 19:16:51,996 - wandb.wandb_agent - INFO - Cleaning up finished run: l7tyxm73
2025-03-26 19:16:52,649 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:16:52,650 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 2000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:16:52,653 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=advanced --n_train=2000 --optim=AdamW --temperature=0.2
2025-03-26 19:16:57,665 - wandb.wandb_agent - INFO - Running runs: ['rgwacduz']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191658-rgwacduz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-862
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/rgwacduz
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: rgwacduz
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.7854 | Val Loss: 1.5103
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.4637 | Val Loss: 1.3993
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.3879 | Val Loss: 1.3658
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 1.3573 | Val Loss: 1.3536
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 1.3577 | Val Loss: 1.3553
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 1.3656 | Val Loss: 1.3691
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 1.3766 | Val Loss: 1.3840
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 1.3925 | Val Loss: 1.3995
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 1.4040 | Val Loss: 1.4078
wandb: - 137.314 MB of 137.314 MB uploadedwandb: \ 137.314 MB of 137.314 MB uploadedwandb: | 137.333 MB of 137.333 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:   val_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.40936
wandb:   val_loss 1.41083
wandb: 
wandb: üöÄ View run zesty-sweep-862 at: https://wandb.ai/7shoe/domShift-extensive/runs/rgwacduz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191658-rgwacduz/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 1.4094 | Val Loss: 1.4108
2025-03-26 19:17:33,136 - wandb.wandb_agent - INFO - Cleaning up finished run: rgwacduz
2025-03-26 19:17:33,699 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:17:33,699 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:17:33,702 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.1
2025-03-26 19:17:38,715 - wandb.wandb_agent - INFO - Running runs: ['4hnq72id']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191739-4hnq72id
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-866
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/4hnq72id
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 4hnq72id
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 1.9829 | Val Loss: 1.7810
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 1.8411 | Val Loss: 1.8873
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 1.9294 | Val Loss: 1.9715
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.0053 | Val Loss: 2.0377
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.0628 | Val Loss: 2.0864
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.1054 | Val Loss: 2.1236
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.1386 | Val Loss: 2.1529
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.1646 | Val Loss: 2.1757
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.1847 | Val Loss: 2.1932
wandb: - 137.266 MB of 137.266 MB uploadedwandb: \ 137.266 MB of 137.266 MB uploadedwandb: | 137.425 MB of 137.425 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.1997
wandb:   val_loss 2.20558
wandb: 
wandb: üöÄ View run mild-sweep-866 at: https://wandb.ai/7shoe/domShift-extensive/runs/4hnq72id
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191739-4hnq72id/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-CIFAR10-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.1997 | Val Loss: 2.2056
2025-03-26 19:19:04,908 - wandb.wandb_agent - INFO - Cleaning up finished run: 4hnq72id
2025-03-26 19:19:05,456 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:05,456 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: moderate
	model: SimSiam
	model_class: vit
	model_type: basic
	n_train: 2000
	optim: AdamW
	temperature: 0.1
2025-03-26 19:19:05,458 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=moderate --model=SimSiam --model_class=vit --model_type=basic --n_train=2000 --optim=AdamW --temperature=0.1
2025-03-26 19:19:10,471 - wandb.wandb_agent - INFO - Running runs: ['8zki2nxl']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191911-8zki2nxl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-876
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/8zki2nxl
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: 8zki2nxl
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 1/10 Train Loss: 1.7926 | Val Loss: 1.5369
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 2/10 Train Loss: 1.4497 | Val Loss: 1.3924
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 3/10 Train Loss: 1.3627 | Val Loss: 1.3146
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 4/10 Train Loss: 1.2602 | Val Loss: 1.2123
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 5/10 Train Loss: 1.2198 | Val Loss: 1.2369
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 6/10 Train Loss: 1.2513 | Val Loss: 1.2626
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 7/10 Train Loss: 1.2664 | Val Loss: 1.2649
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 8/10 Train Loss: 1.2559 | Val Loss: 1.2482
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 9/10 Train Loss: 1.2408 | Val Loss: 1.2228
wandb: - 32.995 MB of 32.995 MB uploadedwandb: \ 32.995 MB of 32.995 MB uploadedwandb: | 33.015 MB of 33.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 1.20482
wandb:   val_loss 1.18701
wandb: 
wandb: üöÄ View run usual-sweep-876 at: https://wandb.ai/7shoe/domShift-extensive/runs/8zki2nxl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191911-8zki2nxl/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[SimSiam-vit-CIFAR10-moderately_skewed-basic-AdamW] Epoch 10/10 Train Loss: 1.2048 | Val Loss: 1.1870
2025-03-26 19:19:40,893 - wandb.wandb_agent - INFO - Cleaning up finished run: 8zki2nxl
2025-03-26 19:19:41,771 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:19:41,771 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: MNIST
	dataset: heavy
	model: BYOL
	model_class: vit
	model_type: advanced
	n_train: 8000
	optim: AdamW
	temperature: 0.30000000000000004
2025-03-26 19:19:41,774 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=MNIST --dataset=heavy --model=BYOL --model_class=vit --model_type=advanced --n_train=8000 --optim=AdamW --temperature=0.30000000000000004
2025-03-26 19:19:46,787 - wandb.wandb_agent - INFO - Running runs: ['y7bp6cyb']
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_191948-y7bp6cyb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-880
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/y7bp6cyb
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: MNIST with input shape (1, 28, 28)
Run ID: y7bp6cyb
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 1/10 Train Loss: 3.8634 | Val Loss: 3.3746
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 2/10 Train Loss: 3.0839 | Val Loss: 2.9042
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 3/10 Train Loss: 2.8411 | Val Loss: 2.7830
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 4/10 Train Loss: 2.7481 | Val Loss: 2.7129
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 5/10 Train Loss: 2.6885 | Val Loss: 2.6637
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 6/10 Train Loss: 2.6479 | Val Loss: 2.6318
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 7/10 Train Loss: 2.6267 | Val Loss: 2.5977
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 8/10 Train Loss: 2.5427 | Val Loss: 2.5286
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 9/10 Train Loss: 2.5818 | Val Loss: 2.6524
wandb: - 137.031 MB of 137.031 MB uploadedwandb: \ 137.031 MB of 137.031 MB uploadedwandb: | 137.051 MB of 137.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:   val_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb: checkpoint /eagle/projects/argo...
wandb:      epoch 10
wandb: train_loss 2.71117
wandb:   val_loss 2.75054
wandb: 
wandb: üöÄ View run worldly-sweep-880 at: https://wandb.ai/7shoe/domShift-extensive/runs/y7bp6cyb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/7shoe/domShift-extensive
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)
wandb: Find logs at: ./wandb/run-20250326_191948-y7bp6cyb/logs
wandb: WARNING wandb version 0.19.8 is available!  To upgrade, please run:
wandb: WARNING  $ pip install wandb --upgrade
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[BYOL-vit-MNIST-heavily_skewed-advanced-AdamW] Epoch 10/10 Train Loss: 2.7112 | Val Loss: 2.7505
2025-03-26 19:20:42,524 - wandb.wandb_agent - INFO - Cleaning up finished run: y7bp6cyb
2025-03-26 19:20:43,608 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-26 19:20:43,608 - wandb.wandb_agent - INFO - Agent starting run with config:
	data_source: CIFAR10
	dataset: extreme
	model: SimSiam
	model_class: vit
	model_type: advanced
	n_train: 14000
	optim: AdamW
	temperature: 0.2
2025-03-26 19:20:43,611 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_ssl.py --data_source=CIFAR10 --dataset=extreme --model=SimSiam --model_class=vit --model_type=advanced --n_train=14000 --optim=AdamW --temperature=0.2
wandb: Currently logged in as: 7shoe. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
2025-03-26 19:20:48,624 - wandb.wandb_agent - INFO - Running runs: ['wjtu66ea']
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/siebenschuh/Projects/domShift/src/wandb/run-20250326_192048-wjtu66ea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-888
wandb: ‚≠êÔ∏è View project at https://wandb.ai/7shoe/domShift-extensive
wandb: üßπ View sweep at https://wandb.ai/7shoe/domShift-extensive/sweeps/jj1sqf9o
wandb: üöÄ View run at https://wandb.ai/7shoe/domShift-extensive/runs/wjtu66ea
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dataset' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temperature' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data_source' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'n_train' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optim' was locked by 'sweep' (ignored update).
/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Training dataset: CIFAR10 with input shape (3, 32, 32)
Run ID: wjtu66ea
